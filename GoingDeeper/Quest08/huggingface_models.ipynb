{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a7d0015",
   "metadata": {},
   "source": [
    "# HuggingFace 커스텀 프로젝트 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5d3e30",
   "metadata": {},
   "source": [
    "## 프로젝트 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfd928c",
   "metadata": {},
   "source": [
    "1. 모델과 데이터를 정상적으로 불러오고, 작동하는 것을 확인하였다.\t\n",
    "    - klue/bert-base를 NSMC 데이터셋으로 fine-tuning 하여, 모델이 정상적으로 작동하는 것을 확인하였다.\n",
    "2. Preprocessing을 개선하고, fine-tuning을 통해 모델의 성능을 개선시켰다.\t\n",
    "    - Validation accuracy를 90% 이상으로 개선하였다.\n",
    "3. 모델 학습에 Bucketing을 성공적으로 적용하고, 그 결과를 비교분석하였다.\t\n",
    "    - Bucketing task을 수행하여 fine-tuning 시 연산 속도와 모델 성능 간의 trade-off 관계가 발생하는지 여부를 확인하고, 분석한 결과를 제시하였다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd28b40e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f4f6867",
   "metadata": {},
   "source": [
    "## 코드구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6765ed30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n",
    "\n",
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1cd4fd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Blpeng/nsmc 데이터 셋 사용\n",
    "# 토크나이저 진행 중 에러발생\n",
    "# 일부 데이터의 document가 다른 형식으로 입력되어 있는거 같음\n",
    "\n",
    "# e9t/nsmc로 수정 후 토크나이저\n",
    "# 정상작동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8b9d2e6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset nsmc (/aiffel/.cache/huggingface/datasets/e9t___nsmc)/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a7c8f6a439c4e87834c574897a3f525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 150000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'document', 'label'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "loaded_nsme_dataset = load_dataset(\"e9t/nsmc\")\n",
    "print(loaded_nsme_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71cab5a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'document', 'label']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = loaded_nsme_dataset['train']\n",
    "cols = train.column_names\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "132dbc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0edaa19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 속도 이슈로 15만개에서 5만개 추출해서 학습에 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83aaee14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /aiffel/.cache/huggingface/datasets/e9t___nsmc)/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-a54e64762cbfb223.arrow and /aiffel/.cache/huggingface/datasets/e9t___nsmc)/default/1.1.0/bfd4729bf1a67114e5267e6916b9e4807010aeb238e4a3c2b95fbfa3a014b5f3/cache-bd2febced1d21d3a.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_1M = loaded_nsme_dataset['train'].train_test_split(train_size=100000, seed=42)['train']\n",
    "len(train_dataset_1M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4f460b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id : 9976970\n",
      "document : 아 더빙.. 진짜 짜증나네요 목소리\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 3819312\n",
      "document : 흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나\n",
      "label : 1\n",
      "\n",
      "\n",
      "id : 10265843\n",
      "document : 너무재밓었다그래서보는것을추천한다\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 9045019\n",
      "document : 교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정\n",
      "label : 0\n",
      "\n",
      "\n",
      "id : 6483659\n",
      "document : 사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 던스트가 너무나도 이뻐보였다\n",
      "label : 1\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    for col in cols:\n",
    "        print(col, \":\", train[col][i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a1870214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처음은 train / temp(test + val)\n",
    "dataset_split = train.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# temp를 다시 validation / test로 분리\n",
    "test_val_split = dataset_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# 최종 dataset dict\n",
    "loaded_nsme_dataset = DatasetDict({\n",
    "    'train': dataset_split['train'],\n",
    "    'validation': test_val_split['train'],\n",
    "    'test': test_val_split['test']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f4cc258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 15000, 15000)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(loaded_nsme_dataset['train']), len(loaded_nsme_dataset['validation']), len(loaded_nsme_dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e78d1d0",
   "metadata": {},
   "source": [
    "###  klue/bert-base model, tokenizer 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3a6345b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': '이정도로 지루함... 몹시도 굼벵이 같은 주인공..빨리감기 추천', 'id': '5050479', 'label': 0}\n"
     ]
    }
   ],
   "source": [
    "print(loaded_nsme_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dff51d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/vocab.txt from cache at /aiffel/.cache/huggingface/transformers/1a36e69d48a008e522b75e43693002ffc8b6e6df72de7c53412c23466ec165eb.085110015ec67fc02ad067f712a7c83aafefaf31586a3361dd800bcac635b456\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer.json from cache at /aiffel/.cache/huggingface/transformers/310a974e892b181d75eed58b545cc0592d066ae4ef35cc760ea92e9b0bf65b3b.74f7933572f937b11a02b2cfb4e88a024059be36c84f53241b85b1fec49e21f7\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/special_tokens_map.json from cache at /aiffel/.cache/huggingface/transformers/aeaaa3afd086a040be912f92ffe7b5f85008b744624f4517c4216bcc32b51cf0.054ece8d16bd524c8a00f0e8a976c00d5de22a755ffb79e353ee2954d9289e26\n",
      "loading file https://huggingface.co/klue/bert-base/resolve/main/tokenizer_config.json from cache at /aiffel/.cache/huggingface/transformers/f8f71eb411bb03f57b455cfb1b4e04ae124201312e67a3ad66e0a92d0c228325.78871951edcb66032caa0a9628d77b3557c23616c653dacdb7a1a8f33011a843\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/klue/bert-base/resolve/main/config.json from cache at /aiffel/.cache/huggingface/transformers/fbd0b2ef898c4653902683fea8cc0dd99bf43f0e082645b913cda3b92429d1bb.99b3298ed554f2ad731c27cdb11a6215f39b90bc845ff5ce709bb4e74ba45621\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/klue/bert-base/resolve/main/pytorch_model.bin from cache at /aiffel/.cache/huggingface/transformers/05b36ee62545d769939a7746eca739b844a40a7a7553700f110b58b28ed6a949.7cb231256a5dbe886e12b902d05cb1241f330d8c19428508f91b2b28c1cfe0b6\n",
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "huggingface_tokenizer = AutoTokenizer.from_pretrained('klue/bert-base')\n",
    "huggingface_model = AutoModelForSequenceClassification.from_pretrained('klue/bert-base', num_labels = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32bd64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토크나이저 해주는 transform 함수 정의\n",
    "def transform(data):\n",
    "    return huggingface_tokenizer(\n",
    "        data['document'],\n",
    "        truncation = True,\n",
    "#         padding = 'max_length',\n",
    "        return_token_type_ids = False,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "96a6dcf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bfa4e5197c4aa9a645276675f97817",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/120 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08756016fa124aacbe238f0e2f5145eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0f979658e25414dbfbbcc17dbf9a89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nsme_dataset = loaded_nsme_dataset.map(\n",
    "    transform,\n",
    "    batched=True\n",
    ")\n",
    "\n",
    "# train & validation & test split\n",
    "nsme_train_dataset = nsme_dataset['train']\n",
    "nsme_val_dataset = nsme_dataset['validation']\n",
    "nsme_test_dataset = nsme_dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d974b5d5",
   "metadata": {},
   "source": [
    "### model 학습 진행해 보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "63ad3cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MRPC 불러옴(Acc, F1)\n",
    "metric = load_metric('glue', 'mrpc')\n",
    "\n",
    "def compute_metrics(eval_pred):    \n",
    "    predictions,labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references = labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4f353e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getenv('HOME')+'/aiffel/transformers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "692715ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "36f6959a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 15000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15000' max='15000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15000/15000 3:18:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.310000</td>\n",
       "      <td>0.331196</td>\n",
       "      <td>0.885600</td>\n",
       "      <td>0.884491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.249600</td>\n",
       "      <td>0.402897</td>\n",
       "      <td>0.886600</td>\n",
       "      <td>0.885570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.149900</td>\n",
       "      <td>0.513854</td>\n",
       "      <td>0.890600</td>\n",
       "      <td>0.889873</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=15000, training_loss=0.24337798716227213, metrics={'train_runtime': 11882.6268, 'train_samples_per_second': 10.099, 'train_steps_per_second': 1.262, 'total_flos': 3.15733266432e+16, 'train_loss': 0.24337798716227213, 'epoch': 3.0})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=nsme_train_dataset,    # training dataset\n",
    "    eval_dataset=nsme_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad707f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: id, document.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 5000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='529' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [529/625 02:24 < 00:26, 3.65 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.evaluate(nsme_test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883d4523",
   "metadata": {},
   "source": [
    "### Bucketing을 적용하여 학습시키고, STEP 4의 결과와의 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e056feeb",
   "metadata": {},
   "source": [
    "1. Dynamic Padding\n",
    "    - 문장 길이에 맞춰서 패딩\n",
    "    - 기존 : 모든 배치가 최대길이로 패딩\n",
    "    - Dynamic Padding : 배치 단위로 가장 긴 문장 기준으로 패딩\n",
    "\n",
    "2. Bucketing\n",
    "    - 비슷한 길이끼리 그룹으로 묶어서 배치\n",
    "    - 길이가 비슷한 문장끼리 묶어서 패딩 최소화\n",
    "\n",
    "- Dynamic Padding, Bucketing은 메모리 효율 + 훈련 속도 향상을 기대할수있음\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "daceee02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# Bucketing 옵션 True 설정\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir,                                         # output이 저장될 경로\n",
    "    evaluation_strategy=\"epoch\",           #evaluation하는 빈도\n",
    "    learning_rate = 2e-5,                         #learning_rate\n",
    "    per_device_train_batch_size = 8,   # 각 device 당 batch size\n",
    "    per_device_eval_batch_size = 8,    # evaluation 시에 batch size\n",
    "    num_train_epochs = 3,                     # train 시킬 총 epochs\n",
    "    weight_decay = 0.01,                        # weight decay\n",
    "    group_by_length = True,       # Bucketing 옵션\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ad30a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 45000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45000' max='45000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45000/45000 1:29:24, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.295500</td>\n",
       "      <td>0.298654</td>\n",
       "      <td>0.894800</td>\n",
       "      <td>0.895330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.238400</td>\n",
       "      <td>0.366269</td>\n",
       "      <td>0.902133</td>\n",
       "      <td>0.902381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.167300</td>\n",
       "      <td>0.476985</td>\n",
       "      <td>0.902600</td>\n",
       "      <td>0.901900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-1500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-1500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-1500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-2500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-2500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-2500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-3500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-3500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-3500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-4500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-4500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-4500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-5500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-5500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-5500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-6500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-6500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-6500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-7500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-7500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-7500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-8500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-8500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-8500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-9500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-9500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-9500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-10500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-10500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-10500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-11500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-11500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-11500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-12500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-12500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-12500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-13500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-13500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-13500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-14500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-14500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-14500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-15000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-15000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-15000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-15500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-15500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-15500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-16000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-16000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-16000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-16500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-16500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-16500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-17000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-17000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-17000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-17500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-17500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-17500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-18000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-18000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-18000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-18500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-18500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-18500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-19000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-19000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-19000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-19500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-19500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-19500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-20000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-20000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-20000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-20500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-20500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-20500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-21000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-21000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-21000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-21500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-21500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-21500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-22000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-22000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-22000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-22500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-22500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-22500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-23000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-23000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-23000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-23500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-23500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-23500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-24000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-24000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-24000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-24500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-24500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-24500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-25000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-25000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-25000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-25500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-25500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-25500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-26000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-26000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-26000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-26500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-26500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-26500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-27000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-27000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-27000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-27500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-27500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-27500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-28000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-28000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-28000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-28500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-28500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-28500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-29000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-29000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-29000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-29500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-29500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-29500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-30000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-30000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-30000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 8\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-30500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-30500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-30500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-31000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-31000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-31000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-31500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-31500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-31500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-32000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-32000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-32000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-32500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-32500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-32500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-33000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-33000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-33000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-33500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-33500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-33500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-34000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-34000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-34000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-34500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-34500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-34500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-35000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-35000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-35000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-35500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-35500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-35500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-36000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-36000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-36000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-36500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-36500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-36500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-37000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-37000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-37000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-37500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-37500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-37500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-38000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-38000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-38000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-38500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-38500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-38500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-39000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-39000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-39000/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-39500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-39500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-39500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-40000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-40000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-40000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-40500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-40500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-40500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-41000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-41000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-41000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-41500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-41500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-41500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-42000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-42000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-42000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-42500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-42500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-42500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-43000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-43000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-43000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-43500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-43500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-43500/pytorch_model.bin\n",
      "/opt/conda/lib/python3.9/site-packages/transformers/trainer.py:1355: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-44000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-44000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-44000/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-44500\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-44500/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-44500/pytorch_model.bin\n",
      "Saving model checkpoint to /aiffel/aiffel/transformers/checkpoint-45000\n",
      "Configuration saved in /aiffel/aiffel/transformers/checkpoint-45000/config.json\n",
      "Model weights saved in /aiffel/aiffel/transformers/checkpoint-45000/pytorch_model.bin\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 8\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45000, training_loss=0.23620720638699003, metrics={'train_runtime': 5365.2577, 'train_samples_per_second': 67.098, 'train_steps_per_second': 8.387, 'total_flos': 4285149980900160.0, 'train_loss': 0.23620720638699003, 'epoch': 3.0})"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dynamic Padding\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=huggingface_tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=huggingface_model,           # 학습시킬 model\n",
    "    args=training_arguments,           # TrainingArguments을 통해 설정한 arguments\n",
    "    train_dataset=nsme_train_dataset,    # training dataset\n",
    "    eval_dataset=nsme_val_dataset,       # evaluation dataset\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=data_collator,       # Dynamic Padding 추가\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0a061273",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: document, id.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 15000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 00:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.475166916847229,\n",
       " 'eval_accuracy': 0.9034,\n",
       " 'eval_f1': 0.9035093560631285,\n",
       " 'eval_runtime': 54.0362,\n",
       " 'eval_samples_per_second': 277.591,\n",
       " 'eval_steps_per_second': 34.699,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(nsme_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c86ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Tue Mar 18 03:17:27 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   65C    P0              29W /  70W |  14923MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A        31      C   /opt/conda/bin/python                         0MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19503d3",
   "metadata": {},
   "source": [
    "1. 데이터별 정확도\n",
    "- 데이터 1만개 : 0.87\n",
    "- 데이터 5만개 : 0.89\n",
    "- 데이터 15만개 : 0.91"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd2cca",
   "metadata": {},
   "source": [
    "2. Dynamic Padding, Bucketing 사용 전후 학습시간\n",
    "- 사용전 : 3시간 18분, 약 200분\n",
    "- 사용후 : 1시간 32분, 약 90분\n",
    "- 사용 후 학습 시간 2배 이상 단축\n",
    "- 정확도를 유의미한 차이가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93deb532",
   "metadata": {},
   "source": [
    "3. 문제발생\n",
    "- 적절한 패딩이 적용안되었음\n",
    "- trainsfrom 함수에서 이미 padding이 되어있어서 동적 패딩이 적용한됨\n",
    "- padding 부분을 삭제\n",
    "- 삭제하고 비교해보니 3시간 걸리는 부분이 1시간 미만으로 시간이 3배 단축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ed20e4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1041892719.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_31/1041892719.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    - 15만개에서 5만개 1만개 썻을땐 동등한 효과가 있는가\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# - 15만개에서 5만개 1만개 썻을땐 동등한 효과가 있는가\n",
    "# - sequence lenth 통계치 확인해보자\n",
    "# - metrics 너무 러프하다. metrics 수정해야한다.\n",
    "# - 데이터 추출에 기준 정해보기(문장 길이 등등)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709fbbc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
