{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ffddad4",
   "metadata": {},
   "source": [
    "# 네이버 영화 리뷰 감성 분석 코퍼스(nsmc)를 활용한 토크나이저 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6708cb",
   "metadata": {},
   "source": [
    "- 네이버 영화리뷰 감정 분석 코퍼스에 SentencePiece를 적용시킨 모델 학습하기\n",
    "- 학습된 모델로 sp_tokenize() 메소드 구현하기\n",
    "- 구현된 토크나이저를 적용하여 네이버 영화리뷰 감정 분석 모델을 재학습하기\n",
    "- KoNLPy 형태소 분석기를 사용한 모델과 성능 비교하기\n",
    "- SentencePiece 모델의 model_type, vocab_size 등을 변경해 가면서 성능 개선 여부 확인하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9637cfb",
   "metadata": {},
   "source": [
    "## 실험목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc602f01",
   "metadata": {},
   "source": [
    "1. SentencePiece를 이용하여 모델을 만들기까지의 과정이 정상적으로 진행되었는가?\t\n",
    "    - 코퍼스 분석, 전처리, SentencePiece 적용, 토크나이저 구현 및 동작이 빠짐없이 진행되었는가?\n",
    "2. SentencePiece를 통해 만든 Tokenizer가 자연어처리 모델과 결합하여 동작하는가?\t\n",
    "    - SentencePiece 토크나이저가 적용된 Text Classifier 모델이 정상적으로 수렴하여 80% 이상의 test accuracy가 확인되었다.\n",
    "3. SentencePiece의 성능을 다각도로 비교분석하였는가?\t\n",
    "    - SentencePiece 토크나이저를 활용했을 때의 성능을 다른 토크나이저 혹은 SentencePiece의 다른 옵션의 경우와 비교하여 분석을 체계적으로 진행하였다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f987f96",
   "metadata": {},
   "source": [
    "## 코드 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c868169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import konlpy\n",
    "import re\n",
    "import sentencepiece as spm\n",
    "import os\n",
    "from konlpy.tag import Mecab, Komoran\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38255ebc",
   "metadata": {},
   "source": [
    "### sp_tokenizer 학습용 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d65b893d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 94123\n",
      "Example:\n",
      ">> 개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\n",
      ">> 북한의 핵무기 계획을 포기하도록 하려는 압력이 거세지고 있는 가운데, 일본과 북한의 외교관들이 외교 관계를 정상화하려는 회담을 재개했다.\n",
      ">> \"경호 로보트가 침입자나 화재를 탐지하기 위해서 개인적으로, 그리고 전문적으로 사용되고 있습니다.\"\n",
      ">> 수자원부 당국은 논란이 되고 있고, 막대한 비용이 드는 이 사업에 대해 내년에 건설을 시작할 계획이다.\n",
      ">> 또한 근력 운동은 활발하게 걷는 것이나 최소한 20분 동안 뛰는 것과 같은 유산소 활동에서 얻는 운동 효과를 심장과 폐에 주지 않기 때문에, 연구학자들은 근력 운동이 심장에 큰 영향을 미치는지 여부에 대해 논쟁을 해왔다.\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인\n",
    "path_to_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko'\n",
    "\n",
    "with open(path_to_file, \"r\") as f:\n",
    "    raw = f.read().splitlines()\n",
    "\n",
    "print(\"Data Size:\", len(raw))\n",
    "\n",
    "print(\"Example:\")\n",
    "for sen in raw[0:100][::20]: print(\">>\", sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a73ab2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 77591\n"
     ]
    }
   ],
   "source": [
    "# 중복 제거\n",
    "raw = list(set(raw))\n",
    "\n",
    "print(\"Data Size:\", len(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d830ca62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장의 최단 길이: 1\n",
      "문장의 최장 길이: 377\n",
      "문장의 평균 길이: 64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ9klEQVR4nO3df5RcZZ3n8ffHBAKCk/CjNwNJ1g5jBhc5DmILcWQdjnEgIWJYD7JxWY2YOVlmYRZHGQiyR9D1R3AcGZlhYKOJBGX5MSgSJ3EkA8xxHZdIRyEkRKSFQDoE0kACCIoEvvvHfSpeiv5d1VXV9Xxe59TpW8996rnfvt39qXufe7tbEYGZmeXhdc0uwMzMGsehb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+WZ1J6pQUkibWccwzJd1Wx/E2SzoxLV8q6Vt1HPtTkr5er/Gsvhz6bU7SCZJ+LOkZSU9L+jdJ76jDuB+V9KN61FhPkrZKeu942qakayT9VtJz6bFJ0hclTa70iYjrIuKkYY71uaH6RcRbIuJfR1tzaXsnSuqtGvsLEfFntY5tY8Oh38Yk/R7wT8DfAQcD04DPAC82sy7r15ci4g1AB3AWMBv4N0kH1HMj9Tz7sPHJod/e/hAgIq6PiJcj4tcRcVtEbKx0kPQxSVsk7ZL0A0lvLK0LSWdLelDSbklXqvAfgKuBd0r6laTdqf8kSV+W9KikJyRdLWn/tO5ESb2SPilpp6Qdks4qbWt/SX8j6ZF0VvKj0mtnp7OV3ZLurUxLjISk10laKumXkp6SdJOkg9O6ynTMolT7k5IurqptVdpHWyRdUDm6lfRN4N8D30v74oLSZs/sb7zBRMRvIuJu4P3AIRRvAK86s0pfg8vTfnxW0n2Sjpa0BDgTuCDV8r3Uf6ukCyVtBJ6XNLGfs5P9JN2YzjR+KumPSp9/SHpT6fk1kj6X3pC+DxyetvcrSYerarpI0vtVTCftlvSv6funsm6rpPMlbUxf9xsl7TecfWWj49Bvb78AXk6BNU/SQeWVkhYAnwI+QHGE+X+B66vGeB/wDuCtwBnAyRGxBTgb+H8RcWBETEl9l1G80RwDvInizOLTpbF+H5ic2hcDV5Zq+jLwduCPKc5KLgBekTQNWAN8LrWfD3xbUscI98VfAKcBfwIcDuwCrqzqcwJwJDAH+HQpnC4BOoEjgD8F/mvlBRHxYeBR4NS0L740jPGGFBHPAeuA/9jP6pOAd1Ps68kUX5enImI5cB3FWcOBEXFq6TUfAuYDUyJiTz9jLgD+kWIf/x/gu5L2GaLG54F5wGNpewdGxGPlPpL+kOJ76uMU32NrKd4g9y11OwOYC8yk+D776GDbtdo49NtYRDxLETwBfA3ok7Ra0tTU5WzgixGxJQXBF4Bjykf7wLKI2B0RjwJ3UgT6a0gSsAT4y4h4OoXWF4CFpW4vAZ+NiJciYi3wK+BISa8DPgacFxHb01nJjyPiRYqAXRsRayPilYhYB3QDp4xwd5wNXBwRvWncS4HT9erpjs+ks6F7gXuBytHuGcAXImJXRPQCVwxzmwONN1yPUYRwtZeANwBvBpS+fjuGGOuKiNgWEb8eYP2GiLg5Il4CvgLsRzHFVKv/DKyJiHVp7C8D+1O8uZdreywinga+xwDfY1YfDv02lwLhoxExHTia4ij3b9PqNwJfTafdu4GnAVEciVc8Xlp+AThwgE11AK8HNpTG++fUXvFU1VFmZbxDKULml/2M+0bgg5Ux07gnAIcN9nkPMM4tpTG2AC8DU0t9BvpcDwe2ldaVlwcz3H03kGkUX5NXiYg7gL+nOFPZKWm5ius3gxmq5r3rI+IVoJfi867V4cAjVWNvY3TfY1YHDv2MRMTPgWsowh+KH77/FhFTSo/9I+LHwxmu6vmTwK+Bt5TGmhwRw/kBfhL4DfAH/azbBnyzqsYDImLZMMatHmde1Tj7RcT2Ybx2BzC99HxG1fq6/6laSQcC76WYcnuNiLgiIt4OHEUxzfNXQ9QyVI17P6d05jWd4kwDiiB+fanv749g3Mco3nArYyttazj73caAQ7+NSXpzunA6PT2fQTG3e1fqcjVwkaS3pPWTJX1wmMM/AUyvzM2mI7ivAZdL+ndpvGmSTh5qoPTalcBX0oXACZLeKWkS8C3gVEknp/b9VFwUnj7IkPukfpXHxPS5fr4ydSWpI13TGI6bKPbTQekaw7n97IsjhjnWoFRcDH878F2K6w7f6KfPOyQdn+bcn6d4w3ylxlreLukDaV99nOIOr8r3yT3Af0n7fy7FdZGKJ4BDVLq9tMpNwHxJc1K9n0xjD+fAwsaAQ7+9PQccD6yX9DzFD/Emih88IuIW4DLgBknPpnXzhjn2HcBm4HFJT6a2C4Ee4K403r9QXMgcjvOB+4C7KaY0LgNeFxHbKC4yfgroozhi/ysG/95dS3HWUXlcCnwVWA3cJuk5in1x/DBr+yzFdMfD6XO6mVff9vpF4H+mqaPzhzlmtQtSXU8B1wIbgD9OF0ur/R7FG+wuiqmTp4C/TutWAEelWr47gu3fSjH/vgv4MPCBNAcPcB5wKrCb4u6gveOms8frgYfSNl81JRQRD1Bcl/k7ijO6Uykuev92BLVZHcn/RMVsZCT9ObAwIv5kyM5mLcZH+mZDkHSYpHepuNf/SIozpVuaXZfZaPi388yGti/wvynuI98N3AD8QzMLMhstT++YmWXE0ztmZhlp6emdQw89NDo7O5tdhpnZuLJhw4YnI6LfP1XS0qHf2dlJd3d3s8swMxtXJD0y0DpP75iZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcSh32I6l65pdglm1sYc+mZmGRky9CWtlLRT0qZS219L+rmkjZJukTSltO4iST2SHij/f1RJc1Nbj6Sldf9MzMxsSMM50r8GmFvVtg44OiLeCvwCuAhA0lHAQuAt6TX/kP6Z8gTgSor/v3oU8KHU14bg6R4zq6chQz8ifkjxj6rLbbdFxJ709C5gelpeANwQES9GxMMU/yT7uPToiYiH0j9EviH1NTOzBqrHnP7HgO+n5WnAttK63tQ2UPtrSFoiqVtSd19fXx3Ka30+mjezRqkp9CVdDOwBrqtPORARyyOiKyK6Ojr6/R8A2ai8GfhNwczqZdShL+mjwPuAM+N3/2h3OzCj1G16ahuo3ZLqYHfQm9lYGFXoS5oLXAC8PyJeKK1aDSyUNEnSTGAW8BPgbmCWpJmS9qW42Lu6ttLbg8PdzBppyH+XKOl64ETgUEm9wCUUd+tMAtZJArgrIs6OiM2SbgLup5j2OSciXk7jnAv8AJgArIyIzWPw+ZiZ2SCGDP2I+FA/zSsG6f954PP9tK8F1o6oOutX59I1bF02v9llmNk45N/INTPLiEPfzCwjDn0zs4w49FuQ7+gxs7Hi0B8n/EZgZvXg0Dczy4hD38wsIw79FuIpHDMbaw59M7OMOPTNzDLi0G8iT+eYWaM59M3MMuLQH0d8ZmBmtXLoN8loA9zBb2a1cOibmWXEod8E9Tha9xG/mY2GQ9/MLCMO/Qby0bmZNZtD38wsIw79cc5nD2Y2Eg59M7OMOPTNzDLi0G8wT8eYWTM59Mcxv4GY2Ug59BvA4WxmrWLI0Je0UtJOSZtKbQdLWifpwfTxoNQuSVdI6pG0UdKxpdcsSv0flLRobD4dMzMbzHCO9K8B5la1LQVuj4hZwO3pOcA8YFZ6LAGuguJNArgEOB44Drik8kZhZmaNM2ToR8QPgaermhcAq9LyKuC0Uvu1UbgLmCLpMOBkYF1EPB0Ru4B1vPaNpK15isfMWsFo5/SnRsSOtPw4MDUtTwO2lfr1praB2l9D0hJJ3ZK6+/r6RlmemZn1p+YLuRERQNShlsp4yyOiKyK6Ojo66jWsmZkx+tB/Ik3bkD7uTO3bgRmlftNT20DtZmbWQKMN/dVA5Q6cRcCtpfaPpLt4ZgPPpGmgHwAnSTooXcA9KbW1Pc/lm1krmThUB0nXAycCh0rqpbgLZxlwk6TFwCPAGan7WuAUoAd4ATgLICKelvS/gLtTv89GRPXF4bbTyMCvbGvrsvkN26aZjT9Dhn5EfGiAVXP66RvAOQOMsxJYOaLqbFh8NmFmw+XfyDUzy4hD38wsIw59M7OMOPTNzDLi0B8jvrhqZq3IoW9mlhGHvplZRhz6ZmYZceibmWXEoW9mlhGHfhvynUNmNhCHvplZRhz6ZmYZceibmWXEoW9mlhGHfp35IqqZtTKHfpvxm46ZDcahb2aWEYe+mVlGHPpmZhlx6JuZZcSh38Z8UdfMqjn025QD38z649AfAw5cM2tVNYW+pL+UtFnSJknXS9pP0kxJ6yX1SLpR0r6p76T0vCet76zLZ2CD8huQmZWNOvQlTQP+B9AVEUcDE4CFwGXA5RHxJmAXsDi9ZDGwK7VfnvqZmVkD1Tq9MxHYX9JE4PXADuA9wM1p/SrgtLS8ID0nrZ8jSTVu38zMRmDUoR8R24EvA49ShP0zwAZgd0TsSd16gWlpeRqwLb12T+p/SPW4kpZI6pbU3dfXN9ryzMysH7VM7xxEcfQ+EzgcOACYW2tBEbE8Iroioqujo6PW4czMrKSW6Z33Ag9HRF9EvAR8B3gXMCVN9wBMB7an5e3ADIC0fjLwVA3bNzOzEaol9B8FZkt6fZqbnwPcD9wJnJ76LAJuTcur03PS+jsiImrYfsvxnTJm1upqmdNfT3FB9qfAfWms5cCFwCck9VDM2a9IL1kBHJLaPwEsraFuMzMbhYlDdxlYRFwCXFLV/BBwXD99fwN8sJbtmZlZbfwbuWZmGXHom5llxKGfgc6la3yR2cwAh76ZWVYc+nXiI2kzGw8c+mZmGXHom5llxKFvZpYRh76ZWUYc+jXwxVszG28c+mZmGXHom5llxKFvZpYRh34deG7fzMYLh76ZWUYc+mZmGXHom5llxKGfEf+JZTNz6GfIwW+WL4d+jRygZjaeOPTNzDLi0Dczy4hDP3OenjLLi0M/Uw57szw59M3MMlJT6EuaIulmST+XtEXSOyUdLGmdpAfTx4NSX0m6QlKPpI2Sjq3Pp9Ac7XCk3A6fg5mNTK1H+l8F/jki3gz8EbAFWArcHhGzgNvTc4B5wKz0WAJcVeO2zcxshEYd+pImA+8GVgBExG8jYjewAFiVuq0CTkvLC4Bro3AXMEXSYaPdvpmZjVwtR/ozgT7gG5J+Junrkg4ApkbEjtTncWBqWp4GbCu9vje1vYqkJZK6JXX39fXVUJ6ZmVWrJfQnAscCV0XE24Dn+d1UDgAREUCMZNCIWB4RXRHR1dHRUUN5Y8dz4WY2XtUS+r1Ab0SsT89vpngTeKIybZM+7kzrtwMzSq+fntrMzKxBRh36EfE4sE3SkalpDnA/sBpYlNoWAbem5dXAR9JdPLOBZ0rTQGZm1gATa3z9XwDXSdoXeAg4i+KN5CZJi4FHgDNS37XAKUAP8ELqay2gc+kati6b3+wyzKwBagr9iLgH6Opn1Zx++gZwTi3bMzOz2vg3cs3MMuLQNzPLiEPfzCwjDn0zs4w49EfIv5hlZuOZQ98Av5mZ5cKhb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlGHPpmZhlx6Ntr+J59s/bl0Le9HPZm7c+hb2aWEYe+mVlGHPoj4OkPMxvvav0fuVlw2JtZu/CRvplZRhz69io+qzFrbw59M7OMOPTNzDLi0B+CpzvMrJ3UHPqSJkj6maR/Ss9nSlovqUfSjZL2Te2T0vOetL6z1m3b2PGbnVl7qseR/nnAltLzy4DLI+JNwC5gcWpfDOxK7ZenfmZm1kA1hb6k6cB84OvpuYD3ADenLquA09LygvSctH5O6m9mZg1S65H+3wIXAK+k54cAuyNiT3reC0xLy9OAbQBp/TOpv5mZNcioQ1/S+4CdEbGhjvUgaYmkbkndfX199RzaRsjz+mbtp5Yj/XcB75e0FbiBYlrnq8AUSZU/7zAd2J6WtwMzANL6ycBT1YNGxPKI6IqIro6OjhrKMzOzaqMO/Yi4KCKmR0QnsBC4IyLOBO4ETk/dFgG3puXV6Tlp/R0REaPdvpmZjdxY3Kd/IfAJST0Uc/YrUvsK4JDU/glg6Rhs28zMBqFWPtju6uqK7u7upm3fc9qFrcvmN7sEMxsBSRsioqu/df6NXDOzjDj0zcwy4tC3IXmay6x9OPTNzDLi0Ldh8dG+WXtw6JuZZcShb2aWEYe+mVlGHPpmZhlx6JuZZcShb8PmO3jMxr+JQ3fJj8PNzNqVj/TNzDLi0Dczy4hD38wsIw59M7OMOPTNzDLi0LcR8Z1NZuObQ9/MLCMOfRsVH/GbjU8OfTOzjDj0q/gI1szamUPfRsxvjGbjl0PfzCwjDn0zs4yMOvQlzZB0p6T7JW2WdF5qP1jSOkkPpo8HpXZJukJSj6SNko6t1ydhzdG5dI2neszGmVqO9PcAn4yIo4DZwDmSjgKWArdHxCzg9vQcYB4wKz2WAFfVsG0zMxuFUYd+ROyIiJ+m5eeALcA0YAGwKnVbBZyWlhcA10bhLmCKpMNGu/2x4KPW0ansN+8/s9ZXlzl9SZ3A24D1wNSI2JFWPQ5MTcvTgG2ll/WmtuqxlkjqltTd19dXj/LMzCypOfQlHQh8G/h4RDxbXhcRAcRIxouI5RHRFRFdHR0dtZZnDeajfbPWVlPoS9qHIvCvi4jvpOYnKtM26ePO1L4dmFF6+fTUZm3AYW82PtRy946AFcCWiPhKadVqYFFaXgTcWmr/SLqLZzbwTGkayMzMGqCWf4z+LuDDwH2S7kltnwKWATdJWgw8ApyR1q0FTgF6gBeAs2rYtpmZjcKoQz8ifgRogNVz+ukfwDmj3Z6ZmdXOv5FrZpYRh37iC5FmlgOHvtWdf1nLrHU59G1MOPDNWpND38aUw9+stTj0cTA1gvexWWtw6JuZZcShb2aWkexD39MOZpaT7EPfxp7fWM1ah0PfGsb375s1n0PfzCwjDn0zs4w49K2hPMVj1lwOfTOzjDj0zcwykm3oe3qhNXi6x6yxsg19cNA0mwPfrPGyDn1rTX4TMBs7tfxjdLO6ctibjT2HvrWk6jeArcvmN6kSs/aS5fSOjyjHH3/NzOojy9C38cnBb1a7LELfd4m0l86la17ztfTX1mx4spvTdziMb+WvX2W5Mt9fXudrAGb9a/iRvqS5kh6Q1CNp6Vhvz0f57a+/r23lbGCgdQO9zqzdNfRIX9IE4ErgT4Fe4G5JqyPi/rHYnn+oDWr7PuhcusZnDdZWGj29cxzQExEPAUi6AVgAjEnomw1kJEf7g/XZumz+gLeXlt8wKsvVH6v7Dbf2wfrXezxrL4qIxm1MOh2YGxF/lp5/GDg+Is4t9VkCLElPjwQeqGGThwJP1vD6sdbq9UHr19jq9UHr1+j6atdqNb4xIjr6W9FyF3IjYjmwvB5jSeqOiK56jDUWWr0+aP0aW70+aP0aXV/txkONFY2+kLsdmFF6Pj21mZlZAzQ69O8GZkmaKWlfYCGwusE1mJllq6HTOxGxR9K5wA+ACcDKiNg8hpusyzTRGGr1+qD1a2z1+qD1a3R9tRsPNQINvpBrZmbNlcWfYTAzs4JD38wsI20b+o3+cw/DIWmrpPsk3SOpO7UdLGmdpAfTx4MaWM9KSTslbSq19VuPClek/blR0rFNrPFSSdvTfrxH0imldRelGh+QdHID6psh6U5J90vaLOm81N4S+3GQ+lppH+4n6SeS7k01fia1z5S0PtVyY7r5A0mT0vOetL6zSfVdI+nh0j48JrU35Wdl2CKi7R4UF4l/CRwB7AvcCxzVAnVtBQ6tavsSsDQtLwUua2A97waOBTYNVQ9wCvB9QMBsYH0Ta7wUOL+fvkelr/UkYGb6HpgwxvUdBhyblt8A/CLV0RL7cZD6WmkfCjgwLe8DrE/75iZgYWq/GvjztPzfgavT8kLgxibVdw1wej/9m/KzMtxHux7p7/1zDxHxW6Dy5x5a0QJgVVpeBZzWqA1HxA+Bp4dZzwLg2ijcBUyRdFiTahzIAuCGiHgxIh4Geii+F8ZMROyIiJ+m5eeALcA0WmQ/DlLfQJqxDyMifpWe7pMeAbwHuDm1V+/Dyr69GZgjSU2obyBN+VkZrnYN/WnAttLzXgb/Rm+UAG6TtCH9uQmAqRGxIy0/DkxtTml7DVRPq+3Tc9Op88rSlFhTa0zTDG+jOBJsuf1YVR+00D6UNEHSPcBOYB3FGcbuiNjTTx17a0zrnwEOaWR9EVHZh59P+/BySZOq6+un9qZr19BvVSdExLHAPOAcSe8ur4zi3LBl7qFttXpKrgL+ADgG2AH8TVOrASQdCHwb+HhEPFte1wr7sZ/6WmofRsTLEXEMxW/pHwe8uZn1VKuuT9LRwEUUdb4DOBi4sHkVDl+7hn5L/rmHiNiePu4EbqH45n6icuqXPu5sXoUwSD0ts08j4on0Q/gK8DV+N/3QlBol7UMRqNdFxHdSc8vsx/7qa7V9WBERu4E7gXdSTItUfoG0XMfeGtP6ycBTDa5vbpo6i4h4EfgGLbIPh9Kuod9yf+5B0gGS3lBZBk4CNqW6FqVui4Bbm1PhXgPVsxr4SLozYTbwTGn6oqGq5kf/E8V+hKLGhenujpnALOAnY1yLgBXAloj4SmlVS+zHgeprsX3YIWlKWt6f4v9tbKEI19NTt+p9WNm3pwN3pLOpRtb389KbuiiuN5T3YUv8rPSr2VeSx+pBcQX9FxRzgxe3QD1HUNwVcS+wuVITxVzk7cCDwL8ABzewpuspTu1foph3XDxQPRR3IlyZ9ud9QFcTa/xmqmEjxQ/YYaX+F6caHwDmNaC+EyimbjYC96THKa2yHwepr5X24VuBn6VaNgGfTu1HULzh9AD/CExK7ful5z1p/RFNqu+OtA83Ad/id3f4NOVnZbgP/xkGM7OMtOv0jpmZ9cOhb2aWEYe+mVlGHPpmZhlx6JuZZcShb2aWEYe+mVlG/j+E+F/k4g/h4QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 데이터 길이 분포 확인\n",
    "min_len = 999\n",
    "max_len = 0\n",
    "sum_len = 0\n",
    "\n",
    "for sen in raw:\n",
    "    length = len(sen)\n",
    "    if min_len > length: min_len = length\n",
    "    if max_len < length: max_len = length\n",
    "    sum_len += length\n",
    "\n",
    "print(\"문장의 최단 길이:\", min_len)\n",
    "print(\"문장의 최장 길이:\", max_len)\n",
    "print(\"문장의 평균 길이:\", sum_len // len(raw))\n",
    "\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in raw:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fb8bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "’\n"
     ]
    }
   ],
   "source": [
    "# 이상데이터 확인 \n",
    "# ex) 길이가 1인 데이터\n",
    "def check_sentence_with_length(raw, length):\n",
    "    count = 0\n",
    "    \n",
    "    for sen in raw:\n",
    "        if len(sen) == length:\n",
    "            print(sen)\n",
    "            count += 1\n",
    "            if count > 100: return\n",
    "\n",
    "check_sentence_with_length(raw, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2beb5ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Size: 77590\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1인 문장 제거\n",
    "raw = [sen for sen in raw if len(sen) > 1]\n",
    "\n",
    "print(\"Data Size:\", len(raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb97c5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q:\n",
      "4.\n",
      "3.\n",
      "6.\n",
      "..\n",
      "음식\n",
      "양파\n",
      "1.\n",
      "과일\n",
      "햇빛\n",
      "2.\n",
      "면접\n"
     ]
    }
   ],
   "source": [
    "# 노이즈가 섞인 데이터 존재 확인\n",
    "check_sentence_with_length(raw, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d9e226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결심, 결의, 해답\n",
      "다칠 위험이 적다.\n",
      "식별, 판별, 분간\n",
      "2. 레스토랑 점원\n",
      "* 여행자의 질병:\n",
      "1. 장난감 만들기\n",
      "6. 이벤트 기획자\n",
      "“아이를 만드세요…\n",
      "무료 인터넷 사용:\n",
      "메일 한통이 만드는\n",
      "햄의 지금 심정은?\n",
      "웃는 얼굴을 한다:\n",
      "생후 24개월까지:\n",
      "tad 조금, 약간\n",
      "이 건물은 추하다.\n",
      "이것이 사실일까요?\n",
      "6살의 아이처럼…”\n",
      "관여, 참가, 분배\n",
      "*홍역, 이하선염:\n",
      "무료 무선 인터넷:\n",
      "틸트로터, 헬기니?\n",
      "mortar 박격포\n",
      "당신 삶을 바꿔라.\n",
      "모스크바의 밤 풍경\n",
      "3월’’ ''안경:\n",
      "환영한다 진행자들！\n",
      "왜 인간은 이럴까?\n",
      "요가호텔 숙박비 :\n",
      "마지막 곤돌라였다.\n",
      "개야”라고 외쳤다.\n",
      "그의 저서 ‘행복;\n",
      "또한 ‘트랜스포머:\n",
      "왕의 귀환”입니다.\n",
      "선례가 하나 있다.\n",
      "스타벅스쿠폰 무료!\n",
      "5.어미니의 실수:\n",
      "한국정보통신산업협회\n",
      "당연시 될 것이다.\n",
      "hostage 인질\n",
      "selloff 주가\n",
      "적대, 대립, 항쟁\n",
      "돈과 모든 것을…”\n",
      "위태로운 일자리 수\n",
      "전체가 아닙니다.”\n",
      "투기(매매), 추론\n",
      "(시위자들의 외침)\n",
      "오차범위는 없었다.\n",
      "epic 서사시적인\n",
      "숙박비 카드 결재:\n",
      "휘드비섬, 워싱턴주\n",
      "월가의 종가입니다.\n",
      "-순간순간을 즐기기\n",
      "그는 \"올슨은 매우\n",
      "대학탐방은 이렇다.\n",
      "그래서 다시 쐈다.\n",
      "고프섬은 무인도다.\n",
      "고맙습니다, 리타.\n",
      "* 등을 두드려라.\n",
      "비난, 고소, 고발\n",
      "日, 경기침체 심화\n",
      "“ 폴 하비입니다.\n",
      "운에 맡기고 해보다\n",
      "존 벨몬트였습니다.\n",
      "친절하게 대화한다.\n",
      "▶관련 동영상 보기\n",
      "친숙하게 들리나요?\n",
      "5. 운동 트레이너\n",
      "그의 근육질 어깨.\n",
      "헌신, 봉헌, 헌정\n",
      "숙청이 시작되었다.\n",
      "여러분은 이 동물이\n",
      "-반으로 줄인 무게\n"
     ]
    }
   ],
   "source": [
    "check_sentence_with_length(raw, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8951c0a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 연산 효율화와 노이즈 최소화를 위해 10~150 길이의 데이터만 추출\n",
    "max_len = 150\n",
    "min_len = 10\n",
    "\n",
    "# 길이 조건에 맞는 문장만 선택합니다.\n",
    "filtered_corpus = [s for s in raw if (len(s) < max_len) & (len(s) >= min_len)]\n",
    "\n",
    "# 분포도를 다시 그려봅니다.\n",
    "sentence_length = np.zeros((max_len), dtype=int)\n",
    "\n",
    "for sen in filtered_corpus:\n",
    "    sentence_length[len(sen)-1] += 1\n",
    "\n",
    "plt.bar(range(max_len), sentence_length, width=1.0)\n",
    "plt.title(\"Sentence Length Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7fc316",
   "metadata": {},
   "source": [
    "### SentencePiece를 이용해 토크나이저 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f39370e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp --model_prefix=korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "  input_format: \n",
      "  model_prefix: korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 76908 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=4996369\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.95% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1317\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.9995\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 76908 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 174340 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 76908\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 237965\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 237965 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=92555 obj=14.853 num_tokens=523272 num_tokens/piece=5.65363\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=82083 obj=13.516 num_tokens=525776 num_tokens/piece=6.40542\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=61555 obj=13.5533 num_tokens=546907 num_tokens/piece=8.88485\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=61506 obj=13.5101 num_tokens=547350 num_tokens/piece=8.89913\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=46126 obj=13.6926 num_tokens=575369 num_tokens/piece=12.4739\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=46126 obj=13.6493 num_tokens=575466 num_tokens/piece=12.476\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34594 obj=13.8894 num_tokens=606014 num_tokens/piece=17.5179\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=34594 obj=13.8387 num_tokens=606012 num_tokens/piece=17.5178\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25945 obj=14.1301 num_tokens=637532 num_tokens/piece=24.5724\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25945 obj=14.0747 num_tokens=637568 num_tokens/piece=24.5738\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=19458 obj=14.4091 num_tokens=670960 num_tokens/piece=34.4825\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=19458 obj=14.3468 num_tokens=670999 num_tokens/piece=34.4845\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14593 obj=14.7196 num_tokens=705636 num_tokens/piece=48.3544\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14593 obj=14.648 num_tokens=705645 num_tokens/piece=48.355\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10944 obj=15.0875 num_tokens=741620 num_tokens/piece=67.765\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10944 obj=15.007 num_tokens=741624 num_tokens/piece=67.7654\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=15.3757 num_tokens=769363 num_tokens/piece=87.4276\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=15.307 num_tokens=769367 num_tokens/piece=87.4281\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1124\r\n",
      "drwxr-xr-x 2 root root   4096 Feb 25 03:37 data\r\n",
      "-rw-r--r-- 1 root root 376816 Feb 25 07:36 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 Feb 25 07:36 korean_spm.vocab\r\n",
      "drwxr-xr-x 5 root root   4096 Feb 25 02:45 nsmc\r\n",
      "-rw-r--r-- 1 root root 376209 Feb 25 06:54 nsmc_korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 145641 Feb 25 06:54 nsmc_korean_spm.vocab\r\n",
      "-rw-r--r-- 1 root root  91657 Feb 25 07:32 nsmc_tokenizer.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/korean-english-park.train.ko.temp'\n",
    "\n",
    "vocab_size = 8000\n",
    "model_name = 'korean_spm'\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in filtered_corpus:   # 이전에 나왔던 정제했던 corpus를 활용해서 진행해야 합니다.\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --vocab_size={}'.format(temp_file,model_name, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l\n",
    "\n",
    "# korean_spm.model (sp_tokenizer 모델 생성)\n",
    "# korean_spm.vocab (단어 사전 생성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48bf0acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1243, 11, 302, 7, 3608, 11, 287, 38, 3]\n",
      "['▁아버지', '가', '방', '에', '들어', '가', '신', '다', '.']\n",
      "아버지가방에들어가신다.\n"
     ]
    }
   ],
   "source": [
    "# 생성된 토크나이저 동작 확인\n",
    "s = spm.SentencePieceProcessor()\n",
    "s.Load('korean_spm.model')\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoding\n",
    "tokensIDs = s.EncodeAsIds('아버지가방에들어가신다.')\n",
    "print(tokensIDs)\n",
    "\n",
    "# SentencePiece를 활용한 sentence -> encoded pieces\n",
    "print(s.SampleEncodeAsPieces('아버지가방에들어가신다.',1, 0.0))\n",
    "\n",
    "# SentencePiece를 활용한 encoding -> sentence 복원\n",
    "print(s.DecodeIds(tokensIDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d38fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24880142",
   "metadata": {},
   "source": [
    "### nsmc 데이터를 활용한 성능비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70780ddf",
   "metadata": {},
   "source": [
    "1. sp_tokenize를 nsmc 데이터로 학습 후 성능 비교\n",
    "2. 토크나이저 비교\n",
    "    - SentencePiece, mecab, KOMORAN 비교\n",
    "    - 텍스트 정제 수준을 다르게 비교\n",
    "        - 한글, 영어, , . ! ? 남겨두고 텍스트 정제\n",
    "        - 한글, 영어만 남김('ㅋㅋㅋ' 같은 모음이나 자음이 분리된 경우도 제거)\n",
    "3. sp_tokenize를 vocab_size를 4000, 16000으로 변경해서 성능비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c49c4f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nsmc 데이터 불러오기\n",
    "nsmc_train = os.getenv('HOME')+'/aiffel/sp_tokenizer/nsmc/ratings_train.txt'\n",
    "nsmc_test = os.getenv('HOME')+'/aiffel/sp_tokenizer/nsmc/ratings_test.txt'\n",
    "\n",
    "train_df = pd.read_csv(nsmc_train, sep='\\t')\n",
    "test_df = pd.read_csv(nsmc_test, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0c8957f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 50000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "534c10a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    5\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결측치 확인 및 제거\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8bbf8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id          0\n",
       "document    3\n",
       "label       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71a22db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n",
      "id          0\n",
      "document    0\n",
      "label       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.dropna(subset=['document'])\n",
    "test_df = test_df.dropna(subset=['document'])\n",
    "\n",
    "print(train_df.isnull().sum())\n",
    "print(test_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e737c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3813\n",
      "840\n"
     ]
    }
   ],
   "source": [
    "# 중복데이터 확인 및 제거\n",
    "print(train_df.duplicated(subset=['document']).sum())\n",
    "print(test_df.duplicated(subset=['document']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6349b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "train_df = train_df.drop_duplicates(subset=['document'])\n",
    "test_df = test_df.drop_duplicates(subset=['document'])\n",
    "\n",
    "print(train_df.duplicated(subset=['document']).sum())\n",
    "print(test_df.duplicated(subset=['document']).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0389a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(146182, 49157)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_df), len(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da2565d",
   "metadata": {},
   "source": [
    "### 실험1. sp_tokenize를 nsmc 데이터로 학습 후 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c2983e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 텍스트 정제\n",
    "# 토크나이저 성능 비교를 위해 최소한의 정제(특수문자만 제거)\n",
    "\n",
    "def clean_text_ver1(text):\n",
    "    # 정규 표현식을 사용하여 원하는 특수문자 제외하고 제거 (한글, 영어, 숫자, 공백, ,.!? 유지)\n",
    "    text = re.sub(r\"[^가-힣a-zA-Z0-9\\s,.!?]\", \"\", text)\n",
    "    \n",
    "    # 연속된 공백을 하나의 공백으로 변환\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802af601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "45265f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9976970</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "      <td>0</td>\n",
       "      <td>아 더빙.. 진짜 짜증나네요 목소리</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3819312</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "      <td>1</td>\n",
       "      <td>흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10265843</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "      <td>0</td>\n",
       "      <td>너무재밓었다그래서보는것을추천한다</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9045019</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "      <td>0</td>\n",
       "      <td>교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6483659</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "      <td>1</td>\n",
       "      <td>사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label  \\\n",
       "0   9976970                                아 더빙.. 진짜 짜증나네요 목소리      0   \n",
       "1   3819312                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나      1   \n",
       "2  10265843                                  너무재밓었다그래서보는것을추천한다      0   \n",
       "3   9045019                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정      0   \n",
       "4   6483659  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...      1   \n",
       "\n",
       "                                    cleaned_document  \n",
       "0                                아 더빙.. 진짜 짜증나네요 목소리  \n",
       "1                  흠...포스터보고 초딩영화줄....오버연기조차 가볍지 않구나  \n",
       "2                                  너무재밓었다그래서보는것을추천한다  \n",
       "3                      교도소 이야기구먼 ..솔직히 재미는 없다..평점 조정  \n",
       "4  사이몬페그의 익살스런 연기가 돋보였던 영화!스파이더맨에서 늙어보이기만 했던 커스틴 ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NSMC 데이터 정제\n",
    "train_df['cleaned_document'] = train_df['document'].astype(str).apply(clean_text_ver1)\n",
    "test_df['cleaned_document'] = test_df['document'].astype(str).apply(clean_text_ver1)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44e319c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sp-토크나이저 함수 선언\n",
    "def sp_tokenize(s, corpus, vocab_name, max_len): \n",
    "    vocab_dir = f\"./{vocab_name}.vocab\" \n",
    "    print(max_len)\n",
    "    print(vocab_dir)\n",
    "        \n",
    "    tensor = []\n",
    "\n",
    "    for sen in corpus:\n",
    "        tensor.append(s.EncodeAsIds(sen))\n",
    "\n",
    "    with open(vocab_dir, 'r') as f:\n",
    "        vocab = f.readlines()\n",
    "\n",
    "    word_index = {}\n",
    "    index_word = {}\n",
    "\n",
    "    for idx, line in enumerate(vocab):\n",
    "        word = line.split(\"\\t\")[0]\n",
    "\n",
    "        word_index.update({word:idx})\n",
    "        index_word.update({idx:word})\n",
    "\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen=max_len)\n",
    "\n",
    "    return tensor, word_index, index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "12479310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "./korean_spm.vocab\n",
      "68\n",
      "./korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"korean_spm.model\")\n",
    "model_name = 'korean_spm'\n",
    "max_len  = int(np.percentile([len(sp.EncodeAsIds(text)) for text in train_df['cleaned_document']], 95))  # 상위 5% 제거\n",
    "\n",
    "# 토큰화 적용\n",
    "X_train, word_index, index_word = sp_tokenize(sp, train_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "X_test, _, _ = sp_tokenize(sp, test_df['cleaned_document'].tolist(), model_name, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "17cef172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    }
   ],
   "source": [
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c9402fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f104608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train 크기: (146182, 68)\n",
      "X_train 샘플 데이터: [ 141  106 2611    3    3  912 4856    4 4856  752   69  554  514 2648\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "y_train 크기: (146182,)\n",
      "y_train 샘플 값: [0 1 0 0 1 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train 크기:\", X_train.shape)\n",
    "print(\"X_train 샘플 데이터:\", X_train[0])\n",
    "print(\"y_train 크기:\", y_train.shape)\n",
    "print(\"y_train 샘플 값:\", y_train[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1a6f0e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터 크기: (116945, 68)\n",
      "검증 데이터 크기: (29237, 68)\n",
      "테스트 데이터 크기: (49157, 68)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"훈련 데이터 크기:\", X_train.shape)\n",
    "print(\"검증 데이터 크기:\", X_val.shape)\n",
    "print(\"테스트 데이터 크기:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "2a5743c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1828/1828 [==============================] - 14s 7ms/step - loss: 0.6712 - accuracy: 0.5844 - val_loss: 0.6704 - val_accuracy: 0.6049\n",
      "Epoch 2/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.6487 - accuracy: 0.6159 - val_loss: 0.5738 - val_accuracy: 0.6920\n",
      "Epoch 3/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.4490 - accuracy: 0.7936 - val_loss: 0.4181 - val_accuracy: 0.8110\n",
      "Epoch 4/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.3779 - accuracy: 0.8358 - val_loss: 0.3781 - val_accuracy: 0.8302\n",
      "Epoch 5/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.3430 - accuracy: 0.8512 - val_loss: 0.3748 - val_accuracy: 0.8354\n"
     ]
    }
   ],
   "source": [
    "# 모델 하이퍼파라미터 설정\n",
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "max_length = X_train.shape[1]  # 패딩 후 문장 최대 길이\n",
    "\n",
    "# LSTM 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), # vocab_size = 8000\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3aab14",
   "metadata": {},
   "source": [
    "#### sp_tokenizer 기존(korean-english-park)버전 정확도 :   0.8327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1c175b8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 4s 3ms/step - loss: 0.3784 - accuracy: 0.8328\n",
      "테스트 정확도: 0.832780659198761\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c46f51ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt --model_prefix=nsmc_korean_spm --vocab_size=8000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_korean_spm\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 8000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 146027 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5275553\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1563\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 146027 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 300807 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 146027\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 342134\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 342134 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=153066 obj=15.1189 num_tokens=788617 num_tokens/piece=5.15214\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=141563 obj=14.0883 num_tokens=793502 num_tokens/piece=5.60529\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=106117 obj=14.1875 num_tokens=827971 num_tokens/piece=7.80244\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=105947 obj=14.131 num_tokens=828603 num_tokens/piece=7.82092\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=79457 obj=14.3626 num_tokens=871105 num_tokens/piece=10.9632\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=79447 obj=14.3008 num_tokens=871281 num_tokens/piece=10.9668\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=59585 obj=14.5656 num_tokens=912284 num_tokens/piece=15.3106\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=59585 obj=14.5032 num_tokens=912265 num_tokens/piece=15.3103\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=44688 obj=14.7919 num_tokens=955443 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=44688 obj=14.7304 num_tokens=955444 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33516 obj=15.0492 num_tokens=999539 num_tokens/piece=29.8227\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33516 obj=14.983 num_tokens=999568 num_tokens/piece=29.8236\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25137 obj=15.3358 num_tokens=1045632 num_tokens/piece=41.5973\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25137 obj=15.2641 num_tokens=1045655 num_tokens/piece=41.5982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=18852 obj=15.6474 num_tokens=1093971 num_tokens/piece=58.0294\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=18852 obj=15.567 num_tokens=1094001 num_tokens/piece=58.031\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14139 obj=15.9877 num_tokens=1144704 num_tokens/piece=80.9607\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14139 obj=15.8967 num_tokens=1144717 num_tokens/piece=80.9617\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10604 obj=16.3573 num_tokens=1199448 num_tokens/piece=113.113\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10604 obj=16.247 num_tokens=1199456 num_tokens/piece=113.114\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=8800 obj=16.5664 num_tokens=1236653 num_tokens/piece=140.529\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=8800 obj=16.497 num_tokens=1236649 num_tokens/piece=140.528\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: nsmc_korean_spm.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: nsmc_korean_spm.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1104\r\n",
      "drwxr-xr-x 2 root root   4096 Feb 25 03:37 data\r\n",
      "-rw-r--r-- 1 root root 376816 Feb 25 06:32 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 Feb 25 06:32 korean_spm.vocab\r\n",
      "drwxr-xr-x 5 root root   4096 Feb 25 02:45 nsmc\r\n",
      "-rw-r--r-- 1 root root 376209 Feb 25 06:54 nsmc_korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 145641 Feb 25 06:54 nsmc_korean_spm.vocab\r\n",
      "-rw-r--r-- 1 root root  73243 Feb 25 06:53 nsmc_tokenizer.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc_clean_train.txt'\n",
    "\n",
    "vocab_size = 8000\n",
    "model_name = \"nsmc_korean_spm\"\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in train_df['cleaned_document']:   # 정제된 텍스트\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --vocab_size={}'.format(temp_file, model_name, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1679feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "./nsmc_korean_spm.vocab\n",
      "51\n",
      "./nsmc_korean_spm.vocab\n",
      "Epoch 1/5\n",
      "1828/1828 [==============================] - 13s 6ms/step - loss: 0.6632 - accuracy: 0.5894 - val_loss: 0.6713 - val_accuracy: 0.5541\n",
      "Epoch 2/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.6617 - accuracy: 0.5765 - val_loss: 0.6493 - val_accuracy: 0.5958\n",
      "Epoch 3/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.4907 - accuracy: 0.7457 - val_loss: 0.3610 - val_accuracy: 0.8426\n",
      "Epoch 4/5\n",
      "1828/1828 [==============================] - 10s 6ms/step - loss: 0.3197 - accuracy: 0.8654 - val_loss: 0.3406 - val_accuracy: 0.8532\n",
      "Epoch 5/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.2759 - accuracy: 0.8856 - val_loss: 0.3458 - val_accuracy: 0.8515\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"nsmc_korean_spm.model\")\n",
    "model_name = 'nsmc_korean_spm'\n",
    "max_len  = int(np.percentile([len(sp.EncodeAsIds(text)) for text in train_df['cleaned_document']], 95))  # 상위 5% 제거\n",
    "\n",
    "# 토큰화 적용\n",
    "X_train, word_index, index_word = sp_tokenize(sp, train_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "X_test, _, _ = sp_tokenize(sp, test_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "max_length = X_train.shape[1]  # 패딩 후 문장 최대 길이\n",
    "\n",
    "# LSTM 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), # vocab_size = 8000\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc42928",
   "metadata": {},
   "source": [
    "#### sp_tokenizer nsmc버전 정확도 :  0.8369"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c3329f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 4s 3ms/step - loss: 0.6319 - accuracy: 0.8369\n",
      "테스트 정확도: 0.8369103074073792\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf67a5",
   "metadata": {},
   "source": [
    "#### 결론\n",
    "- 1차 학습 결과 korean-english-park데이터로 학습시킨 토크나이저와  nsmc로 학습시킨 토크나이저에 성능차이가 없음(0.497로 동일).\n",
    "- 정상적인 학습이 진행이 안되는걸 확인(val_loss, val_acc 가 동일)\n",
    "- 왜 학습이 안될까? 대해서 모델 설계부터 데이터 토큰화까지 확인\n",
    "   - 토큰화 비교 결과 두 토크나이저 결과가 다름.-> 그런데 왜 성능이 똑같을까? 이상함.\n",
    "     -> 확인결과 학습이 정상적으로 진행되지않고 있음 -> Dropout 낮게 조절, learning_rate 조절 등 여러 시도\n",
    "     -> but, 안됨 ㅠ -> 원인. max_length가 너무 길어서 학습속도 저하 -> 정상적인 학습이 되지않음 -> max_length 지정(기존 140 -> 107)\n",
    "     -> max_length 바꾸니 학습됨 -> 또 문제, nsmc로 토크나이저 하면 학습이 안됨. 왜그럴까? 아직 ing\n",
    "- 문제이유 알아냄\n",
    "    - 토큰이 아닌 문자기준으로 max_len을 측정해서 토큰나이저가 바뀌니깐 학습이 정상적으로 되지않았더거임\n",
    "    - 즉, 107이랑 길이가 korean-english-park 토크나이저면 학습이 될 정도였지만 nsmc 토크나이저한테 너무 커서 정상적인 연산이 되지않음\n",
    "    - 토큰기준으로 문장 길이를 측정하도록 코드 수정, 기존 토크나이저는 68, nsmc는 51\n",
    "- 최종 학습 결과 \n",
    "    - korean-english-park 토크나이저 : 0.8327\n",
    "    - nsmc 토크나이저 : 0.8369\n",
    "    - 큰 차이가 없다. 즉, korean-english-park 토크나이저가 충분히 다양한 한국어 데이터를 학습했거나 혹은 SentencePiece 특성상 내부단어를 분리하기때문에 토큰에 큰 성능향상은 없다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a943caa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁재', '밌', '네', '요']\n",
      "['▁재밌네요']\n",
      "['▁사랑', '과', '▁영', '혼', '사', '랑', '과', '▁영', '혼', '사', '랑', '과', '▁영', '혼']\n",
      "['▁사랑과', '▁영혼', '사랑', '과', '▁영혼', '사랑', '과', '▁영혼']\n",
      "['▁여성', '의', '▁입장', '에서', '▁바라', '보', '는', '▁잔', '잔', '하면서', '도', '▁기분', '이', '▁', '묘', '해', '지는', '▁느낌']\n",
      "['▁여성', '의', '▁입장에서', '▁바라', '보는', '▁잔잔하면서', '도', '▁기분이', '▁', '묘', '해지는', '▁느낌']\n",
      "['▁진정', '한', '▁사랑', '을', '▁알아', '가', '는', '▁성장', '영화', '.', '▁다', '소', '▁충격', '적', '이나', '▁두', '▁배우', '의', '▁진', '지', '한', '▁연기', '가', '▁인상', '적', '.']\n",
      "['▁진정한', '▁사랑을', '▁알아', '가', '는', '▁성장', '영화', '.', '▁다소', '▁충격적', '이나', '▁두', '▁배우의', '▁진지한', '▁연기가', '▁인상적', '.']\n",
      "['▁여자', '주', '인', '공', '▁너무', '▁', '멋', '있다']\n",
      "['▁여자주인공', '▁너무', '▁멋있다']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>korean_spm Tokens</th>\n",
       "      <th>nsmc_korean_spm Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>재밌네요</td>\n",
       "      <td>▁재 밌 네 요</td>\n",
       "      <td>▁재밌네요</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>사랑과 영혼사랑과 영혼사랑과 영혼</td>\n",
       "      <td>▁사랑 과 ▁영 혼 사 랑 과 ▁영 혼 사 랑 과 ▁영 혼</td>\n",
       "      <td>▁사랑과 ▁영혼 사랑 과 ▁영혼 사랑 과 ▁영혼</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>여성의 입장에서 바라보는 잔잔하면서도 기분이 묘해지는 느낌</td>\n",
       "      <td>▁여성 의 ▁입장 에서 ▁바라 보 는 ▁잔 잔 하면서 도 ▁기분 이 ▁ 묘 해 지는...</td>\n",
       "      <td>▁여성 의 ▁입장에서 ▁바라 보는 ▁잔잔하면서 도 ▁기분이 ▁ 묘 해지는 ▁느낌</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>진정한 사랑을 알아가는 성장영화. 다소 충격적이나 두 배우의 진지한 연기가 인상적.</td>\n",
       "      <td>▁진정 한 ▁사랑 을 ▁알아 가 는 ▁성장 영화 . ▁다 소 ▁충격 적 이나 ▁두 ...</td>\n",
       "      <td>▁진정한 ▁사랑을 ▁알아 가 는 ▁성장 영화 . ▁다소 ▁충격적 이나 ▁두 ▁배우의...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>여자주인공 너무 멋있다</td>\n",
       "      <td>▁여자 주 인 공 ▁너무 ▁ 멋 있다</td>\n",
       "      <td>▁여자주인공 ▁너무 ▁멋있다</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Original Text  \\\n",
       "0                                            재밌네요   \n",
       "1                              사랑과 영혼사랑과 영혼사랑과 영혼   \n",
       "2                여성의 입장에서 바라보는 잔잔하면서도 기분이 묘해지는 느낌   \n",
       "3  진정한 사랑을 알아가는 성장영화. 다소 충격적이나 두 배우의 진지한 연기가 인상적.   \n",
       "4                                    여자주인공 너무 멋있다   \n",
       "\n",
       "                                   korean_spm Tokens  \\\n",
       "0                                           ▁재 밌 네 요   \n",
       "1                   ▁사랑 과 ▁영 혼 사 랑 과 ▁영 혼 사 랑 과 ▁영 혼   \n",
       "2  ▁여성 의 ▁입장 에서 ▁바라 보 는 ▁잔 잔 하면서 도 ▁기분 이 ▁ 묘 해 지는...   \n",
       "3  ▁진정 한 ▁사랑 을 ▁알아 가 는 ▁성장 영화 . ▁다 소 ▁충격 적 이나 ▁두 ...   \n",
       "4                               ▁여자 주 인 공 ▁너무 ▁ 멋 있다   \n",
       "\n",
       "                              nsmc_korean_spm Tokens  \n",
       "0                                              ▁재밌네요  \n",
       "1                         ▁사랑과 ▁영혼 사랑 과 ▁영혼 사랑 과 ▁영혼  \n",
       "2       ▁여성 의 ▁입장에서 ▁바라 보는 ▁잔잔하면서 도 ▁기분이 ▁ 묘 해지는 ▁느낌  \n",
       "3  ▁진정한 ▁사랑을 ▁알아 가 는 ▁성장 영화 . ▁다소 ▁충격적 이나 ▁두 ▁배우의...  \n",
       "4                                    ▁여자주인공 ▁너무 ▁멋있다  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SentencePiece 모델 로드\n",
    "sp_korean = spm.SentencePieceProcessor()\n",
    "sp_korean.load(\"korean_spm.model\")  # 기존 모델\n",
    "\n",
    "sp_nsmc = spm.SentencePieceProcessor()\n",
    "sp_nsmc.load(\"nsmc_korean_spm.model\")  # NSMC 학습 모델\n",
    "\n",
    "# 비교할 문장 샘플 (train_df에서 5개 선택)\n",
    "sample_texts = train_df['cleaned_document'].sample(5, random_state=42).tolist()\n",
    "\n",
    "# 토큰화 비교 함수\n",
    "def compare_tokenization(texts, sp1, sp2, model_name1=\"korean_spm\", model_name2=\"nsmc_korean_spm\"):\n",
    "    results = []\n",
    "    for text in texts:\n",
    "        tokens1 = sp1.EncodeAsPieces(text)\n",
    "        tokens2 = sp2.EncodeAsPieces(text)\n",
    "        print(tokens1)\n",
    "        print(tokens2)\n",
    "\n",
    "        results.append({\n",
    "            \"Original Text\": text,\n",
    "            f\"{model_name1} Tokens\": \" \".join(tokens1),\n",
    "            f\"{model_name2} Tokens\": \" \".join(tokens2),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# 비교 실행\n",
    "comparison_df = compare_tokenization(sample_texts, sp_korean, sp_nsmc)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3034a025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean-English-Park vocab 크기: 8000\n",
      "NSMC vocab 크기: 8000\n"
     ]
    }
   ],
   "source": [
    "print(\"Korean-English-Park vocab 크기:\", sp_korean.GetPieceSize())\n",
    "print(\"NSMC vocab 크기:\", sp_nsmc.GetPieceSize())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7877b2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean-English-Park 토큰화: ['▁이', '▁영화', '▁정말', '▁재미있', '어', '요', '!']\n",
      "NSMC 토큰화: ['▁이', '▁영화', '▁정말', '▁재미있어요', '!']\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"이 영화 정말 재미있어요!\"\n",
    "\n",
    "tokens_park = sp_korean.EncodeAsPieces(sample_text)\n",
    "tokens_nsmc = sp_nsmc.EncodeAsPieces(sample_text)\n",
    "\n",
    "print(\"Korean-English-Park 토큰화:\", tokens_park)\n",
    "print(\"NSMC 토큰화:\", tokens_nsmc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce712ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c4b3682",
   "metadata": {},
   "source": [
    "### 실험 2. 토크나이저 비교\n",
    "    - SentencePiece, mecab, KOMORAN 비교\n",
    "    - 텍스트 정제 수준을 다르게 비교\n",
    "        - 한글, 영어, , . ! ? 남겨두고 텍스트 정제\n",
    "        - 한글, 영어만 남김('ㅋㅋㅋ' 같은 모음이나 자음이 분리된 경우도 제거)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f01bda5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = Mecab()\n",
    "komoran = Komoran()\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"nsmc_korean_spm.model\")\n",
    "model_name = \"nsmc_korean_spm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80b34688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_mecab(text):\n",
    "    return mecab.morphs(text)\n",
    "\n",
    "def tokenize_komoran(text):\n",
    "    return komoran.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97e53285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4624056a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece max_len: 51\n",
      "Mecab max_len: 55\n",
      "KOMORAN max_len: 58\n"
     ]
    }
   ],
   "source": [
    "corpus = train_df['cleaned_document'].tolist()  # 정제된 문장 데이터\n",
    "\n",
    "# ======= 1️⃣ SentencePiece `max_len` 계산 =======\n",
    "max_len_sp = int(np.percentile([len(sp.EncodeAsIds(text)) for text in corpus], 95))\n",
    "print(f\"SentencePiece max_len: {max_len_sp}\")\n",
    "\n",
    "# ======= 2️⃣ Mecab `max_len` 계산 =======\n",
    "max_len_mecab = int(np.percentile([len(mecab.morphs(text)) for text in corpus], 95))\n",
    "print(f\"Mecab max_len: {max_len_mecab}\")\n",
    "\n",
    "# ======= 3️⃣ KOMORAN `max_len` 계산 =======\n",
    "max_len_komoran = int(np.percentile([len(komoran.morphs(text)) for text in corpus], 95))\n",
    "print(f\"KOMORAN max_len: {max_len_komoran}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a544acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len_sp = 51\n",
    "max_len_mecab = 55\n",
    "max_len_komoran = 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b77c0c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 SentencePiece 토큰화 중...\n",
      "51\n",
      "./nsmc_korean_spm.vocab\n",
      "51\n",
      "./nsmc_korean_spm.vocab\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 SentencePiece 토큰화 중...\")\n",
    "X_train_sp, _, _ = sp_tokenize(sp, train_df['cleaned_document'].tolist(),model_name, max_len_sp)\n",
    "X_test_sp, _, _ = sp_tokenize(sp, test_df['cleaned_document'].tolist(),model_name, max_len_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f94796e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 KOMORAN 토큰화 중...\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "print(\"🔹 KOMORAN 토큰화 중...\")\n",
    "# 🔹 Komoran 단어 사전 생성\n",
    "word_freq_komoran = Counter(word for sentence in X_train_komoran for word in sentence)\n",
    "word_index_komoran = {word: i+1 for i, (word, _) in enumerate(word_freq_komoran.most_common())}\n",
    "\n",
    "# 🔹 정수 인코딩 적용 (딕셔너리 `get()` 사용하여 빠르게 변환)\n",
    "X_train_komoran_int = [[word_index_komoran.get(word, 0) for word in sentence] for sentence in X_train_komoran]\n",
    "X_test_komoran_int = [[word_index_komoran.get(word, 0) for word in sentence] for sentence in X_test_komoran]\n",
    "\n",
    "# 🔹 문장 길이 맞추기 (패딩 적용)\n",
    "max_len_komoran = int(np.percentile([len(seq) for seq in X_train_komoran_int], 95))  # 상위 95% 길이\n",
    "X_train_komoran_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_komoran_int, padding='post', maxlen=max_len_komoran)\n",
    "X_test_komoran_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_komoran_int, padding='post', maxlen=max_len_komoran)\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "57de4c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Mecab 토큰화 중...\n",
      "완료\n"
     ]
    }
   ],
   "source": [
    "print(\"🔹 Mecab 토큰화 중...\")\n",
    "# 🔹 Mecab 단어 사전 구축\n",
    "word_freq_mecab = Counter(word for sentence in X_train_mecab for word in sentence)\n",
    "word_index_mecab = {word: i+1 for i, (word, _) in enumerate(word_freq_mecab.most_common())}\n",
    "\n",
    "# 🔹 정수 인코딩 적용\n",
    "X_train_mecab_int = [[word_index_mecab.get(word, 0) for word in sentence] for sentence in X_train_mecab]\n",
    "X_test_mecab_int = [[word_index_mecab.get(word, 0) for word in sentence] for sentence in X_test_mecab]\n",
    "\n",
    "# 🔹 문장 길이 맞추기 (패딩 적용)\n",
    "max_len_mecab = int(np.percentile([len(seq) for seq in X_train_mecab_int], 95))  # 상위 95% 길이\n",
    "X_train_mecab_pad = tf.keras.preprocessing.sequence.pad_sequences(X_train_mecab_int, padding='post', maxlen=max_len_mecab)\n",
    "X_test_mecab_pad = tf.keras.preprocessing.sequence.pad_sequences(X_test_mecab_int, padding='post', maxlen=max_len_mecab)\n",
    "\n",
    "print(\"완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b19e968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 설정\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e04c546",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_sp, X_val_sp, y_train_sp, y_val_sp = train_test_split(X_train_sp, y_train, test_size=0.2, random_state=42)\n",
    "X_train_mecab, X_val_mecab, y_train_mecab, y_val_mecab = train_test_split(X_train_mecab_pad, y_train, test_size=0.2, random_state=42)\n",
    "X_train_komoran, X_val_komoran, y_train_komoran, y_val_komoran = train_test_split(X_train_komoran_pad, y_train, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39fb475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델학습 함수\n",
    "def training_model(X_train, X_val, y_tain, y_val):\n",
    "    # 모델 하이퍼파라미터 설정\n",
    "    embedding_dim = 128\n",
    "    lstm_units = 64\n",
    "    max_length = X_train.shape[1]  # 패딩 후 문장 최대 길이\n",
    "\n",
    "    # LSTM 모델 구성\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), # vocab_size = 8000\n",
    "        tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.2),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    learning_rate = 0.001  \n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    # 모델 학습\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=5,\n",
    "        batch_size=64,\n",
    "        validation_data=(X_val, y_val)\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c1f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sp = training_model(X_train_sp, X_val_sp, y_train_sp, y_val_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef0bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_mecab = training_model(X_train_mecab, X_val_mecab, y_train_mecab, y_val_mecab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677cc95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_komoran = training_model(X_train_komoran, X_val_komoran, y_train_komoran, y_val_komoran)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da80af15",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_test_loss, sp_test_acc = model_sp.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", sp_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779a72f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab_test_loss, mecab_test_acc = model_mecab.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", mecab_test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c4586f",
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran_test_loss, komoran_test_acc = model_komoran.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", komoran_test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a967c6",
   "metadata": {},
   "source": [
    "### 실험 3. vocab_size 바꿔보며 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9c030755",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt --model_prefix=nsmc_korean_spm_4000 --vocab_size=4000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_korean_spm_4000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 4000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 146027 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5275553\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1563\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 146027 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 300807 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 146027\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 342134\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 342134 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=153066 obj=15.1189 num_tokens=788617 num_tokens/piece=5.15214\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=141563 obj=14.0883 num_tokens=793502 num_tokens/piece=5.60529\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=106117 obj=14.1875 num_tokens=827971 num_tokens/piece=7.80244\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=105947 obj=14.131 num_tokens=828603 num_tokens/piece=7.82092\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=79457 obj=14.3626 num_tokens=871105 num_tokens/piece=10.9632\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=79447 obj=14.3008 num_tokens=871281 num_tokens/piece=10.9668\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=59585 obj=14.5656 num_tokens=912284 num_tokens/piece=15.3106\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=59585 obj=14.5032 num_tokens=912265 num_tokens/piece=15.3103\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=44688 obj=14.7919 num_tokens=955443 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=44688 obj=14.7304 num_tokens=955444 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33516 obj=15.0492 num_tokens=999539 num_tokens/piece=29.8227\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33516 obj=14.983 num_tokens=999568 num_tokens/piece=29.8236\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25137 obj=15.3358 num_tokens=1045632 num_tokens/piece=41.5973\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25137 obj=15.2641 num_tokens=1045655 num_tokens/piece=41.5982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=18852 obj=15.6474 num_tokens=1093971 num_tokens/piece=58.0294\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=18852 obj=15.567 num_tokens=1094001 num_tokens/piece=58.031\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=14139 obj=15.9877 num_tokens=1144704 num_tokens/piece=80.9607\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=14139 obj=15.8967 num_tokens=1144717 num_tokens/piece=80.9617\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=10604 obj=16.3573 num_tokens=1199448 num_tokens/piece=113.113\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=10604 obj=16.247 num_tokens=1199456 num_tokens/piece=113.114\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=7953 obj=16.7642 num_tokens=1258290 num_tokens/piece=158.216\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=7953 obj=16.6504 num_tokens=1258357 num_tokens/piece=158.224\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=5964 obj=17.2293 num_tokens=1321846 num_tokens/piece=221.637\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=5964 obj=17.0983 num_tokens=1322264 num_tokens/piece=221.708\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=4473 obj=17.7497 num_tokens=1392938 num_tokens/piece=311.41\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4473 obj=17.5875 num_tokens=1393423 num_tokens/piece=311.519\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=4400 obj=17.6317 num_tokens=1398447 num_tokens/piece=317.829\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=4400 obj=17.6104 num_tokens=1398472 num_tokens/piece=317.835\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: nsmc_korean_spm_4000.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: nsmc_korean_spm_4000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1492\r\n",
      "drwxr-xr-x 2 root root   4096 Feb 25 03:37 data\r\n",
      "-rw-r--r-- 1 root root 376816 Feb 25 07:36 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 Feb 25 07:36 korean_spm.vocab\r\n",
      "drwxr-xr-x 5 root root   4096 Feb 25 02:45 nsmc\r\n",
      "-rw-r--r-- 1 root root 299624 Feb 25 07:45 nsmc_korean_spm_4000.model\r\n",
      "-rw-r--r-- 1 root root  65503 Feb 25 07:45 nsmc_korean_spm_4000.vocab\r\n",
      "-rw-r--r-- 1 root root 376209 Feb 25 06:54 nsmc_korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 145641 Feb 25 06:54 nsmc_korean_spm.vocab\r\n",
      "-rw-r--r-- 1 root root  99505 Feb 25 07:45 nsmc_tokenizer.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 4000\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc_clean_train.txt'\n",
    "\n",
    "vocab_size = 4000\n",
    "model_name = \"nsmc_korean_spm_4000\"\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in train_df['cleaned_document']:   # 정제된 텍스트\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --vocab_size={}'.format(temp_file, model_name, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fde8054c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n",
      "./nsmc_korean_spm_4000.vocab\n",
      "59\n",
      "./nsmc_korean_spm_4000.vocab\n",
      "Epoch 1/5\n",
      "1828/1828 [==============================] - 21s 7ms/step - loss: 0.6568 - accuracy: 0.6021 - val_loss: 0.6425 - val_accuracy: 0.6201\n",
      "Epoch 2/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.6637 - accuracy: 0.5753 - val_loss: 0.6883 - val_accuracy: 0.5138\n",
      "Epoch 3/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.6748 - accuracy: 0.5468 - val_loss: 0.6532 - val_accuracy: 0.7056\n",
      "Epoch 4/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.5738 - accuracy: 0.7351 - val_loss: 0.5577 - val_accuracy: 0.7508\n",
      "Epoch 5/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.5041 - accuracy: 0.7752 - val_loss: 0.5283 - val_accuracy: 0.7438\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"nsmc_korean_spm_4000.model\")\n",
    "model_name = 'nsmc_korean_spm_4000'\n",
    "max_len  = int(np.percentile([len(sp.EncodeAsIds(text)) for text in train_df['cleaned_document']], 95))  # 상위 5% 제거\n",
    "\n",
    "# 토큰화 적용\n",
    "X_train, word_index, index_word = sp_tokenize(sp, train_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "X_test, _, _ = sp_tokenize(sp, test_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "max_length = X_train.shape[1]  # 패딩 후 문장 최대 길이\n",
    "\n",
    "# LSTM 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), # vocab_size = 4000\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8d5bc71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 4s 3ms/step - loss: 0.5337 - accuracy: 0.7395\n",
      "테스트 정확도: 0.7395080924034119\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa55f8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(177) LOG(INFO) Running command: --input=/aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt --model_prefix=nsmc_korean_spm_16000 --vocab_size=16000\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "  input_format: \n",
      "  model_prefix: nsmc_korean_spm_16000\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 16000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: /aiffel/aiffel/sp_tokenizer/data/nsmc_clean_train.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 146027 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5275553\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1563\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 146027 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 300807 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 146027\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 342134\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 342134 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=153066 obj=15.1189 num_tokens=788617 num_tokens/piece=5.15214\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=141563 obj=14.0883 num_tokens=793502 num_tokens/piece=5.60529\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=106117 obj=14.1875 num_tokens=827971 num_tokens/piece=7.80244\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=105947 obj=14.131 num_tokens=828603 num_tokens/piece=7.82092\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=79457 obj=14.3626 num_tokens=871105 num_tokens/piece=10.9632\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=79447 obj=14.3008 num_tokens=871281 num_tokens/piece=10.9668\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=59585 obj=14.5656 num_tokens=912284 num_tokens/piece=15.3106\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=59585 obj=14.5032 num_tokens=912265 num_tokens/piece=15.3103\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=44688 obj=14.7919 num_tokens=955443 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=44688 obj=14.7304 num_tokens=955444 num_tokens/piece=21.3803\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=33516 obj=15.0492 num_tokens=999539 num_tokens/piece=29.8227\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=33516 obj=14.983 num_tokens=999568 num_tokens/piece=29.8236\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=25137 obj=15.3358 num_tokens=1045632 num_tokens/piece=41.5973\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25137 obj=15.2641 num_tokens=1045655 num_tokens/piece=41.5982\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=18852 obj=15.6474 num_tokens=1093971 num_tokens/piece=58.0294\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=18852 obj=15.567 num_tokens=1094001 num_tokens/piece=58.031\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=17600 obj=15.6597 num_tokens=1106069 num_tokens/piece=62.8448\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=17600 obj=15.64 num_tokens=1106079 num_tokens/piece=62.8454\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: nsmc_korean_spm_16000.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: nsmc_korean_spm_16000.vocab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2336\r\n",
      "drwxr-xr-x 2 root root   4096 Feb 25 03:37 data\r\n",
      "-rw-r--r-- 1 root root 376816 Feb 25 07:36 korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 146213 Feb 25 07:36 korean_spm.vocab\r\n",
      "drwxr-xr-x 5 root root   4096 Feb 25 02:45 nsmc\r\n",
      "-rw-r--r-- 1 root root 536250 Feb 25 07:47 nsmc_korean_spm_16000.model\r\n",
      "-rw-r--r-- 1 root root 312871 Feb 25 07:47 nsmc_korean_spm_16000.vocab\r\n",
      "-rw-r--r-- 1 root root 299624 Feb 25 07:45 nsmc_korean_spm_4000.model\r\n",
      "-rw-r--r-- 1 root root  65503 Feb 25 07:45 nsmc_korean_spm_4000.vocab\r\n",
      "-rw-r--r-- 1 root root 376209 Feb 25 06:54 nsmc_korean_spm.model\r\n",
      "-rw-r--r-- 1 root root 145641 Feb 25 06:54 nsmc_korean_spm.vocab\r\n",
      "-rw-r--r-- 1 root root 111302 Feb 25 07:47 nsmc_tokenizer.ipynb\r\n"
     ]
    }
   ],
   "source": [
    "# vocab_size = 16000\n",
    "temp_file = os.getenv('HOME')+'/aiffel/sp_tokenizer/data/nsmc_clean_train.txt'\n",
    "\n",
    "vocab_size = 16000\n",
    "model_name = \"nsmc_korean_spm_16000\"\n",
    "\n",
    "with open(temp_file, 'w') as f:\n",
    "    for row in train_df['cleaned_document']:   # 정제된 텍스트\n",
    "        f.write(str(row) + '\\n')\n",
    "\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input={} --model_prefix={} --vocab_size={}'.format(temp_file, model_name, vocab_size)    \n",
    ")\n",
    "#위 Train에서  --model_type = unigram이 디폴트 적용되어 있습니다. --model_type = bpe로 옵션을 주어 변경할 수 있습니다.\n",
    "\n",
    "!ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8ca82c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "./nsmc_korean_spm_16000.vocab\n",
      "45\n",
      "./nsmc_korean_spm_16000.vocab\n",
      "Epoch 1/5\n",
      "1828/1828 [==============================] - 14s 6ms/step - loss: 0.4623 - accuracy: 0.7638 - val_loss: 0.3377 - val_accuracy: 0.8521\n",
      "Epoch 2/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.2924 - accuracy: 0.8771 - val_loss: 0.3374 - val_accuracy: 0.8572\n",
      "Epoch 3/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.2342 - accuracy: 0.9031 - val_loss: 0.3589 - val_accuracy: 0.8502\n",
      "Epoch 4/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.1818 - accuracy: 0.9286 - val_loss: 0.4347 - val_accuracy: 0.8501\n",
      "Epoch 5/5\n",
      "1828/1828 [==============================] - 11s 6ms/step - loss: 0.1372 - accuracy: 0.9484 - val_loss: 0.4944 - val_accuracy: 0.8412\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(\"nsmc_korean_spm_16000.model\")\n",
    "model_name = 'nsmc_korean_spm_16000'\n",
    "max_len  = int(np.percentile([len(sp.EncodeAsIds(text)) for text in train_df['cleaned_document']], 95))  # 상위 5% 제거\n",
    "\n",
    "# 토큰화 적용\n",
    "X_train, word_index, index_word = sp_tokenize(sp, train_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "X_test, _, _ = sp_tokenize(sp, test_df['cleaned_document'].tolist(), model_name, max_len)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# 모델 하이퍼파라미터 설정\n",
    "embedding_dim = 128\n",
    "lstm_units = 64\n",
    "max_length = X_train.shape[1]  # 패딩 후 문장 최대 길이\n",
    "\n",
    "# LSTM 모델 구성\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length), # vocab_size = 16000\n",
    "    tf.keras.layers.LSTM(lstm_units, return_sequences=False),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "learning_rate = 0.001  \n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# 모델 학습\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4544322e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1537/1537 [==============================] - 4s 3ms/step - loss: 0.5067 - accuracy: 0.8356\n",
      "테스트 정확도: 0.8356286883354187\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 평가\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"테스트 정확도:\", test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be751ab",
   "metadata": {},
   "source": [
    "#### 결론\n",
    "- vocab_size = 4000 일때 test acc : 0.7395\n",
    "- vocab_size = 8000 일때 test acc : 0.8369\n",
    "- vocab_size = 16000 일때 test acc : 0.8356\n",
    "- vocab_size 8000, 16000 일떄 성능은 큰 차이가 없었다. 다만 vocab_size가 4000일 경우 성능 저하 확인\n",
    "- vocab size가 충분하지 않으면 성능이 저하됨. 그러나, vocab_size를 너무 키워도 불필요한 연산이 진행되기때문에 적절한 size 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c38b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
