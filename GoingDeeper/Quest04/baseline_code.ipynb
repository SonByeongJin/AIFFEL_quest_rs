{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab4dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b69ecc77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b49372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90129deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_train_data = 'korean-english-park.train.ko'\n",
    "# dec_train_data = 'korean-english-park.train.en'\n",
    "# enc_val_data = 'korean-english-park.dev.ko'\n",
    "# dec_val_data = 'korean-english-park.dev.en'\n",
    "# enc_test_data = 'korean-english-park.test.ko'\n",
    "# dec_test_data = 'korean-english-park.test.en'\n",
    "\n",
    "# # 데이터 읽기\n",
    "# def load_data(file_path):\n",
    "#     with open(file_path, 'r', encoding='utf-8') as file:\n",
    "#         data = file.readlines()\n",
    "#     return data\n",
    "\n",
    "# # 파일 읽기\n",
    "# enc_train_data = load_data(enc_train_data)\n",
    "# dec_train_data = load_data(dec_train_data)\n",
    "# enc_val_data = load_data(enc_val_data)\n",
    "# dec_val_data = load_data(dec_val_data)\n",
    "# enc_test_data = load_data(enc_test_data)\n",
    "# dec_test_data = load_data(dec_test_data)\n",
    "\n",
    "# # 데이터 미리 보기\n",
    "# print(enc_train_data[:3])  # 첫 3개 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e202040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_data = enc_train_data + enc_val_data\n",
    "# dec_data = dec_train_data + dec_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13e96881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(enc_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bd0b888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(enc_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70bd99ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 특수기호 확인하는 함수\n",
    "# def find_special_char(data):\n",
    "#     # 한글과 숫자를 제외한 특수문자만 찾는 정규표현식\n",
    "#     pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "    \n",
    "#     special_chars = []\n",
    "    \n",
    "#     # 리스트의 각 항목에 대해 특수기호를 찾음\n",
    "#     for text in data:\n",
    "#         if isinstance(text, str):  # 문자열인 경우에만 처리\n",
    "#             # 정규표현식을 통해 특수문자 추출\n",
    "#             special_chars.extend(re.findall(pattern, text))\n",
    "    \n",
    "#     return special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "776d7826",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '}',\n",
       " '~',\n",
       " '±',\n",
       " '²',\n",
       " '´',\n",
       " '·',\n",
       " '×',\n",
       " 'é',\n",
       " '˙',\n",
       " '˝',\n",
       " '–',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '‥',\n",
       " '…',\n",
       " '℃',\n",
       " 'ℓ',\n",
       " '▲',\n",
       " '▶',\n",
       " '〈',\n",
       " '〉',\n",
       " '〔',\n",
       " '〕',\n",
       " 'い',\n",
       " 'か',\n",
       " 'き',\n",
       " 'く',\n",
       " 'さ',\n",
       " 'ざ',\n",
       " 'し',\n",
       " 'す',\n",
       " 'そ',\n",
       " 'て',\n",
       " 'と',\n",
       " 'ば',\n",
       " 'ぶ',\n",
       " 'ぷ',\n",
       " 'ま',\n",
       " 'や',\n",
       " 'よ',\n",
       " 'ら',\n",
       " 'り',\n",
       " 'ん',\n",
       " 'ㄴ',\n",
       " 'ㅇ',\n",
       " 'ㅋ',\n",
       " 'ㆍ',\n",
       " '㈜',\n",
       " '㎝',\n",
       " '㎞',\n",
       " '㎠',\n",
       " '㎡',\n",
       " '㎢',\n",
       " '㎾',\n",
       " '一',\n",
       " '万',\n",
       " '三',\n",
       " '上',\n",
       " '不',\n",
       " '丙',\n",
       " '中',\n",
       " '主',\n",
       " '之',\n",
       " '九',\n",
       " '亞',\n",
       " '交',\n",
       " '京',\n",
       " '仁',\n",
       " '代',\n",
       " '佛',\n",
       " '促',\n",
       " '信',\n",
       " '修',\n",
       " '假',\n",
       " '備',\n",
       " '傷',\n",
       " '像',\n",
       " '僞',\n",
       " '價',\n",
       " '億',\n",
       " '兆',\n",
       " '光',\n",
       " '克',\n",
       " '兒',\n",
       " '內',\n",
       " '全',\n",
       " '公',\n",
       " '兵',\n",
       " '再',\n",
       " '切',\n",
       " '前',\n",
       " '劉',\n",
       " '力',\n",
       " '加',\n",
       " '動',\n",
       " '勝',\n",
       " '北',\n",
       " '區',\n",
       " '卍',\n",
       " '占',\n",
       " '印',\n",
       " '卿',\n",
       " '反',\n",
       " '受',\n",
       " '古',\n",
       " '司',\n",
       " '吉',\n",
       " '同',\n",
       " '名',\n",
       " '吳',\n",
       " '吾',\n",
       " '命',\n",
       " '和',\n",
       " '品',\n",
       " '商',\n",
       " '善',\n",
       " '喩',\n",
       " '器',\n",
       " '四',\n",
       " '因',\n",
       " '國',\n",
       " '園',\n",
       " '圓',\n",
       " '團',\n",
       " '在',\n",
       " '地',\n",
       " '坤',\n",
       " '型',\n",
       " '城',\n",
       " '基',\n",
       " '堀',\n",
       " '報',\n",
       " '場',\n",
       " '塔',\n",
       " '墓',\n",
       " '壽',\n",
       " '外',\n",
       " '多',\n",
       " '大',\n",
       " '天',\n",
       " '夫',\n",
       " '奔',\n",
       " '奴',\n",
       " '姓',\n",
       " '婦',\n",
       " '子',\n",
       " '字',\n",
       " '孩',\n",
       " '學',\n",
       " '宇',\n",
       " '安',\n",
       " '官',\n",
       " '室',\n",
       " '家',\n",
       " '富',\n",
       " '察',\n",
       " '寧',\n",
       " '寶',\n",
       " '寺',\n",
       " '對',\n",
       " '導',\n",
       " '小',\n",
       " '局',\n",
       " '居',\n",
       " '屋',\n",
       " '山',\n",
       " '岩',\n",
       " '島',\n",
       " '峽',\n",
       " '川',\n",
       " '州',\n",
       " '市',\n",
       " '席',\n",
       " '常',\n",
       " '平',\n",
       " '幹',\n",
       " '店',\n",
       " '度',\n",
       " '座',\n",
       " '庫',\n",
       " '康',\n",
       " '廣',\n",
       " '建',\n",
       " '式',\n",
       " '弗',\n",
       " '强',\n",
       " '影',\n",
       " '後',\n",
       " '心',\n",
       " '性',\n",
       " '愁',\n",
       " '愚',\n",
       " '愛',\n",
       " '慶',\n",
       " '懷',\n",
       " '戰',\n",
       " '所',\n",
       " '手',\n",
       " '掃',\n",
       " '掌',\n",
       " '換',\n",
       " '播',\n",
       " '故',\n",
       " '效',\n",
       " '敎',\n",
       " '敢',\n",
       " '敵',\n",
       " '文',\n",
       " '斌',\n",
       " '新',\n",
       " '方',\n",
       " '族',\n",
       " '旗',\n",
       " '日',\n",
       " '早',\n",
       " '明',\n",
       " '星',\n",
       " '時',\n",
       " '晉',\n",
       " '晴',\n",
       " '書',\n",
       " '曾',\n",
       " '最',\n",
       " '會',\n",
       " '月',\n",
       " '本',\n",
       " '朱',\n",
       " '李',\n",
       " '東',\n",
       " '林',\n",
       " '査',\n",
       " '株',\n",
       " '案',\n",
       " '棺',\n",
       " '植',\n",
       " '楊',\n",
       " '機',\n",
       " '權',\n",
       " '次',\n",
       " '止',\n",
       " '正',\n",
       " '死',\n",
       " '殘',\n",
       " '毁',\n",
       " '母',\n",
       " '毛',\n",
       " '毬',\n",
       " '民',\n",
       " '氣',\n",
       " '水',\n",
       " '氷',\n",
       " '江',\n",
       " '沈',\n",
       " '河',\n",
       " '油',\n",
       " '法',\n",
       " '泥',\n",
       " '泰',\n",
       " '流',\n",
       " '浅',\n",
       " '海',\n",
       " '淘',\n",
       " '淺',\n",
       " '港',\n",
       " '湖',\n",
       " '湛',\n",
       " '溫',\n",
       " '滑',\n",
       " '澤',\n",
       " '濠',\n",
       " '濤',\n",
       " '濱',\n",
       " '瀋',\n",
       " '灣',\n",
       " '火',\n",
       " '無',\n",
       " '然',\n",
       " '焼',\n",
       " '煙',\n",
       " '照',\n",
       " '燭',\n",
       " '父',\n",
       " '爾',\n",
       " '物',\n",
       " '獨',\n",
       " '獸',\n",
       " '率',\n",
       " '王',\n",
       " '珀',\n",
       " '現',\n",
       " '球',\n",
       " '理',\n",
       " '琥',\n",
       " '環',\n",
       " '生',\n",
       " '産',\n",
       " '田',\n",
       " '男',\n",
       " '異',\n",
       " '畵',\n",
       " '疆',\n",
       " '疑',\n",
       " '疫',\n",
       " '病',\n",
       " '登',\n",
       " '發',\n",
       " '的',\n",
       " '省',\n",
       " '眞',\n",
       " '石',\n",
       " '社',\n",
       " '神',\n",
       " '票',\n",
       " '祭',\n",
       " '福',\n",
       " '私',\n",
       " '種',\n",
       " '空',\n",
       " '突',\n",
       " '章',\n",
       " '管',\n",
       " '節',\n",
       " '築',\n",
       " '精',\n",
       " '紅',\n",
       " '紙',\n",
       " '結',\n",
       " '維',\n",
       " '總',\n",
       " '纏',\n",
       " '置',\n",
       " '羅',\n",
       " '美',\n",
       " '群',\n",
       " '習',\n",
       " '翔',\n",
       " '老',\n",
       " '者',\n",
       " '耳',\n",
       " '聖',\n",
       " '聲',\n",
       " '職',\n",
       " '肢',\n",
       " '胡',\n",
       " '能',\n",
       " '膚',\n",
       " '臺',\n",
       " '舊',\n",
       " '航',\n",
       " '船',\n",
       " '花',\n",
       " '芽',\n",
       " '英',\n",
       " '草',\n",
       " '華',\n",
       " '董',\n",
       " '葬',\n",
       " '蓄',\n",
       " '蓋',\n",
       " '蔣',\n",
       " '藥',\n",
       " '藻',\n",
       " '號',\n",
       " '街',\n",
       " '衛',\n",
       " '要',\n",
       " '視',\n",
       " '親',\n",
       " '覺',\n",
       " '言',\n",
       " '詩',\n",
       " '誌',\n",
       " '語',\n",
       " '談',\n",
       " '論',\n",
       " '諡',\n",
       " '諸',\n",
       " '識',\n",
       " '貧',\n",
       " '買',\n",
       " '費',\n",
       " '賣',\n",
       " '質',\n",
       " '赤',\n",
       " '走',\n",
       " '超',\n",
       " '足',\n",
       " '路',\n",
       " '身',\n",
       " '軌',\n",
       " '軍',\n",
       " '輔',\n",
       " '輿',\n",
       " '辛',\n",
       " '近',\n",
       " '迷',\n",
       " '送',\n",
       " '通',\n",
       " '進',\n",
       " '道',\n",
       " '邦',\n",
       " '邸',\n",
       " '郞',\n",
       " '郡',\n",
       " '都',\n",
       " '鄕',\n",
       " '配',\n",
       " '酒',\n",
       " '酸',\n",
       " '醒',\n",
       " '醜',\n",
       " '野',\n",
       " '銀',\n",
       " '銅',\n",
       " '錦',\n",
       " '鎔',\n",
       " '長',\n",
       " '門',\n",
       " '開',\n",
       " '間',\n",
       " '陳',\n",
       " '陽',\n",
       " '雙',\n",
       " '難',\n",
       " '雨',\n",
       " '雲',\n",
       " '電',\n",
       " '霧',\n",
       " '靑',\n",
       " '靜',\n",
       " '非',\n",
       " '頁',\n",
       " '頂',\n",
       " '頭',\n",
       " '題',\n",
       " '類',\n",
       " '食',\n",
       " '香',\n",
       " '馬',\n",
       " '駐',\n",
       " '體',\n",
       " '髮',\n",
       " '魚',\n",
       " '鳥',\n",
       " '鳩',\n",
       " '麥',\n",
       " '麻',\n",
       " '鼓',\n",
       " '\\uf0d7',\n",
       " '金',\n",
       " '懶',\n",
       " '不',\n",
       " '良',\n",
       " '女',\n",
       " '年',\n",
       " '戀',\n",
       " '領',\n",
       " '料',\n",
       " '遼',\n",
       " '龍',\n",
       " '柳',\n",
       " '輪',\n",
       " '率',\n",
       " '識',\n",
       " '！',\n",
       " '（',\n",
       " '）',\n",
       " '－',\n",
       " '．',\n",
       " '１',\n",
       " '２',\n",
       " '３',\n",
       " '４',\n",
       " '５',\n",
       " '８',\n",
       " '？',\n",
       " '\\U000d51d1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(find_special_char(enc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c4b5ca1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¥',\n",
       " '¦',\n",
       " '¨',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '´',\n",
       " 'µ',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ç',\n",
       " 'Ê',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ñ',\n",
       " 'Ù',\n",
       " 'Û',\n",
       " 'â',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'ñ',\n",
       " 'ó',\n",
       " 'ö',\n",
       " 'ø',\n",
       " 'û',\n",
       " '˝',\n",
       " '–',\n",
       " '—',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '℃',\n",
       " '∼'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set(find_special_char(dec_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ea1a7317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30a03323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 토크나이저가 정상적으로 생성되었습니다!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import tensorflow as tf\n",
    "\n",
    "# ✅ JSON에서 `word_index`만 로드\n",
    "def load_word_index(json_path):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        word_index = json.load(f)  # ✅ JSON 파일 로드\n",
    "    return word_index\n",
    "\n",
    "# ✅ `word_index`를 기반으로 새로운 `Tokenizer` 생성\n",
    "def create_tokenizer_from_word_index(word_index):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')  # ✅ 필터링 없이 생성\n",
    "    tokenizer.word_index = word_index  # ✅ 기존 `word_index` 적용\n",
    "    tokenizer.index_word = {v: k for k, v in word_index.items()}  # ✅ `index_word` 생성\n",
    "    return tokenizer\n",
    "\n",
    "# 🔹 한국어 & 영어 `word_index` 로드\n",
    "enc_word_index = load_word_index(\"data/enc_tokenizer.json\")  # ✅ Mecab으로 만든 word_index\n",
    "dec_word_index = load_word_index(\"data/dec_tokenizer.json\")  # ✅ 공백 단위로 split한 word_index\n",
    "\n",
    "# 🔹 `word_index` 기반 `Tokenizer` 생성\n",
    "enc_tokenizer = create_tokenizer_from_word_index(enc_word_index)\n",
    "dec_tokenizer = create_tokenizer_from_word_index(dec_word_index)\n",
    "\n",
    "print(\"✅ 토크나이저가 정상적으로 생성되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c2c464f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ enc_train shape: (30000, 30)\n",
      "✅ dec_train shape: (30000, 40)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 토큰화된 데이터를 불러오기 (이미 토큰화된 데이터 사용)\n",
    "data = np.load(\"data/tokenized_data.npz\")\n",
    "\n",
    "# ✅ 기존의 토큰화된 데이터\n",
    "enc_train = data['enc_corpus']  # 한국어 (입력 데이터)\n",
    "dec_train = data['dec_corpus']  # 영어 (출력 데이터)\n",
    "\n",
    "# ✅ `maxlen` 설정 (enc=30, dec=40)\n",
    "ENC_MAXLEN = 30\n",
    "DEC_MAXLEN = 40\n",
    "\n",
    "# ✅ `pad_sequences()`를 이용해 패딩 적용\n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(enc_train, maxlen=ENC_MAXLEN, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(dec_train, maxlen=DEC_MAXLEN, padding='post')\n",
    "\n",
    "print(f\"✅ enc_train shape: {enc_train.shape}\")  # (샘플 수, 30)\n",
    "print(f\"✅ dec_train shape: {dec_train.shape}\")  # (샘플 수, 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269605f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "72ef302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 제 23차 연례 컴덱스 박람회의 개회사를 한 케이츠는 2년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다 .\n",
      "영어: <start> Gates , who opened the 23rd annual Comdex trade show , said there was a negative perception of high tech following the collapse of the tech bubble about two years ago . <end>\n"
     ]
    }
   ],
   "source": [
    "# enc_corpus = []\n",
    "# dec_corpus = []\n",
    "\n",
    "# num_examples = 50000\n",
    "\n",
    "# for kor in enc_data[:num_examples]:\n",
    "#     enc_corpus.append(preprocess_sentence(kor))\n",
    "    \n",
    "# for eng in dec_data[:num_examples]:\n",
    "#     dec_corpus.append(preprocess_sentence(eng, s_token=True, e_token=True))\n",
    "\n",
    "# print(\"한국어:\", enc_corpus[100])\n",
    "# print(\"영어:\", dec_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e948ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import MeCab\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eac1bf50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# nltk.download('punkt')\n",
    "# mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "906230c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize(corpus, lang='ko'):\n",
    "#     tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "#     tokenized_corpus = []\n",
    "    \n",
    "#     for sentence in corpus:\n",
    "#         if lang == 'ko':  # 한국어 텍스트일 경우 MeCab 사용\n",
    "#             parsed = mecab.parse(sentence)\n",
    "#             tokens = [line.split('\\t')[0] for line in parsed.splitlines() if line]\n",
    "#         elif lang == 'en':  # 영어 텍스트일 경우 NLTK 사용\n",
    "#             tokens = nltk.word_tokenize(sentence)\n",
    "#         else:\n",
    "#             raise ValueError(\"Language should be either 'ko' or 'en'.\")\n",
    "\n",
    "#         # 리스트에 토큰 추가\n",
    "#         tokenized_corpus.append(' '.join(tokens))\n",
    "    \n",
    "#     tokenizer.fit_on_texts(tokenized_corpus)  # 토큰을 기반으로 Tokenizer 학습\n",
    "    \n",
    "#     # 특수 토큰 추가\n",
    "#     tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "#     tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 2\n",
    "    \n",
    "#     tensor = tokenizer.texts_to_sequences(tokenized_corpus)  # 텍스트를 시퀀스로 변환\n",
    "#     tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 시퀀스 길이를 동일하게 맞추기\n",
    "\n",
    "#     return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "090e6aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_corpus, enc_tokenizer = tokenize(enc_corpus)\n",
    "# dec_corpus, dec_tokenizer = tokenize(dec_corpus, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f79ea368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enc_train, enc_val, dec_train, dec_val = train_test_split(enc_corpus, dec_corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a2b4056",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ae3b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9abdd8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "709e201e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (32, 30, 512)\n",
      "Decoder Output: (32, 30370)\n",
      "Decoder Hidden State: (32, 512)\n",
      "Attention: (32, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 32\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512\n",
    "embedding_dim = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f35dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d32365ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정 (베스트 모델 저장)\n",
    "def create_checkpoint_manager(encoder, decoder, optimizer, checkpoint_dir='./checkpoints'):\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=1)\n",
    "    return checkpoint, manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "85cc98f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1aa8a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a53bedbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "\n",
    "best_val_loss = float('inf')  # 초기 검증 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "04939175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_step 함수 정의\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e140d086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 938/938 [03:22<00:00,  4.63it/s, loss=0.0385]  \n",
      "Epoch 2: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0330]\n",
      "Epoch 3: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0328]\n",
      "Epoch 4: 100%|██████████| 938/938 [02:14<00:00,  6.97it/s, loss=0.0326]\n",
      "Epoch 5: 100%|██████████| 938/938 [02:14<00:00,  6.97it/s, loss=0.0326]\n",
      "Epoch 6: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 7: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 8: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0326]\n",
      "Epoch 9: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]\n",
      "Epoch 10: 100%|██████████| 938/938 [02:14<00:00,  6.98it/s, loss=0.0327]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 훈련 완료! 모델이 성공적으로 학습되었습니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 10  # 학습 반복 횟수\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0  # 에포크별 손실 초기화\n",
    "    \n",
    "    # 🔹 배치 단위로 섞기\n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    \n",
    "    # 🔹 tqdm을 사용한 학습 진행 표시\n",
    "    t = tqdm(idx_list, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    # 🔹 학습 진행\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],  # ✅ 입력 (한국어)\n",
    "                                dec_train[idx:idx+BATCH_SIZE],  # ✅ 출력 (영어)\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss  # 손실값 누적\n",
    "        t.set_postfix(loss=f\"{total_loss.numpy() / (batch + 1):.4f}\")  # 진행 상황 출력\n",
    "\n",
    "print(\"✅ 훈련 완료! 모델이 성공적으로 학습되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b018aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00cdfd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1956fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a94cf77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: 오바마는 대통령이다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 시민들은 도시 속에 산다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 커피는 필요 없다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 일곱 명의 사망자가 발생했다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 번역할 문장 리스트\n",
    "input_sentences = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "# ✅ 각 문장에 대해 번역 실행\n",
    "for sentence in input_sentences:\n",
    "    translated_sentence, processed_input, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    # ✅ 번역 결과 출력\n",
    "    print(f\"입력 문장: {processed_input}\")\n",
    "    print(f\"번역 결과: {translated_sentence}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9d69861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 텍스트 전처리 함수\n",
    "def kor_preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣a-zA-Z0-9?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "\n",
    "# 영어 텍스트 전처리 함수\n",
    "def eng_preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "    sentence = sentence.lower().strip()\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = kor_preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention\n",
    "\n",
    "\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "642172da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "입력 문장: 오바마는 대통령이다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 시민들은 도시 속에 산다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 커피는 필요 없다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n",
      "입력 문장: 일곱 명의 사망자가 발생했다 .\n",
      "번역 결과: . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "# ✅ 번역할 문장 리스트\n",
    "input_sentences = [\n",
    "    \"오바마는 대통령이다.\",\n",
    "    \"시민들은 도시 속에 산다.\",\n",
    "    \"커피는 필요 없다.\",\n",
    "    \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "# ✅ 각 문장에 대해 번역 실행\n",
    "for sentence in input_sentences:\n",
    "    translated_sentence, processed_input, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    # ✅ 번역 결과 출력\n",
    "    print(f\"입력 문장: {processed_input}\")\n",
    "    print(f\"번역 결과: {translated_sentence}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30328ac6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
