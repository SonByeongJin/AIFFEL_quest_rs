{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0206f121",
   "metadata": {},
   "source": [
    "# Pretrain-BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27253d70",
   "metadata": {},
   "source": [
    "## 프로젝트 목표"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d756809",
   "metadata": {},
   "source": [
    "1. 한글 코퍼스를 가공하여 BERT pretrain용 데이터셋을 잘 생성하였다.\t\n",
    "- MLM, NSP task의 특징이 잘 반영된 pretrain용 데이터셋 생성과정이 체계적으로 진행되었다.\n",
    "2. 구현한 BERT 모델의 학습이 안정적으로 진행됨을 확인하였다.\t\n",
    "- 학습진행 과정 중에 MLM, NSP loss의 안정적인 감소가 확인되었다.\n",
    "3. 1M짜리 mini BERT 모델의 제작과 학습이 정상적으로 진행되었다.\t\n",
    "- 학습된 모델 및 학습과정의 시각화 내역이 제출되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b10d7f9",
   "metadata": {},
   "source": [
    "1. Tokenizer 준비\n",
    "- SentencePiece 모델을 이용해 BERT의 MLM 학습용 데이터를 만드세요.\n",
    "\n",
    "- 이를 위해 한글 나무 위키 코퍼스로부터 8000의 vocab_size를 갖는 sentencepiece 모델을 만들어 보세요. BERT에 사용되는 주요 특수문자가 vocab에 포함되어야 합니다. (시간이 부족하다면 클라우드에 저장된 sentencepiece 모델을 사용하세요.)\n",
    "\n",
    "2. 데이터 전처리 (1) MASK 생성\n",
    "- BERT의 MLM에 필요한 빈칸(mask)을 학습 데이터 전체 토큰의 15% 정도로 만들어 주세요. 그 중 80%는 [MASK] 토큰, 10%는 랜덤한 토큰, 나머지 10%는 원래의 토큰을 그대로 사용하세요.\n",
    "\n",
    "3. 데이터 전처리 (2) NSP pair 생성\n",
    "- BERT의 pretrain task인 NSP는 두 문장이 연속하는지 확인하는 것입니다. 이를 위해 2개의 문장을 짝지어 50%의 확률로 TRUE와 FALSE를 지정해 주세요.\n",
    "\n",
    "- 두 문장 사이에 segment 처리를 해주세요. 첫 번째 문장의 segment는 0, 두 번째 문장은 1로 채워준 후 둘 사이에 구분자인 [SEP] 등을 넣어주세요.\n",
    "\n",
    "- MLM과 NSP는 동시에 학습된다는 것을 염두에 두고 학습 데이터를 구성해 보세요.\n",
    "\n",
    "4. 데이터 전처리 (3) 데이터셋 완성\n",
    "- BERT pretrain 데이터셋을 생성해, json 포맷으로 저장하세요. 데이터셋의 사이즈가 크므로np.memmap을 사용해 메모리 사용량을 최소화해 보세요.\n",
    "\n",
    "5. BERT 모델 구현\n",
    "- pad mask, ahead mask 함수, gelu activation 함수, parameter initializer 생성 함수, json을 config 형태로 사용하기 위한 유틸리티 함수를 먼저 만들어 두세요.\n",
    "\n",
    "- Embedding 레이어, Transformer encoder 레이어, BERT 레이어를 구성한 후, pretraine용 BERT 모델을 만들어 봅시다.\n",
    "\n",
    "6. pretrain 진행\n",
    "- loss, accuracy 함수를 정의하고 Learning Rate 스케쥴링을 구현한 후, 10 Epoch까지 모델 학습을 시켜보세요. 학습을 진행할 때는 배치 사이즈에 유의하세요.\n",
    "\n",
    "7. 프로젝트 결과\n",
    "- 학습된 모델과 학습과정을 시각화해 보세요. NSP와 MLM의 loss가 안정적으로 수렴하나요? 모델이 작기 때문에 loss가 잘 수렴하지 않을 수도 있어요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96402c3b",
   "metadata": {},
   "source": [
    "## 코드구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b1b0035",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "/device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import collections\n",
    "import json\n",
    "import shutil\n",
    "import zipfile\n",
    "import copy\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import sentencepiece as spm\n",
    "from tqdm import tqdm\n",
    "\n",
    "random_seed = 1234\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "tf.random.set_seed(random_seed)\n",
    "\n",
    "# tf version 및 gpu 확인\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be1a190",
   "metadata": {},
   "source": [
    "### 1. 토크나이저 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da1e50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/data'\n",
    "model_dir = os.getenv('HOME')+'/aiffel/bert_pretrain/models'\n",
    "\n",
    "# vocab loading\n",
    "# vocab_size 8000으로 학습한 vocab 모델 로드\n",
    "vocab = spm.SentencePieceProcessor()\n",
    "vocab.load(f\"{model_dir}/ko_8000.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebe67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):\n",
    "            vocab_list.append(vocab.decode(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b45aec19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "string_a = \"추적추적 비가 내리는 날이었어 그날은 왠지 손님이 많아 첫 번에 삼십 전 둘째번 오십 전 오랜만에 받아보는 십 전짜리 백통화 서푼에\"\n",
    "string_b = \"손바닥 위엔 기쁨의 눈물이 흘러 컬컬한 목에 모주 한잔을 적셔 몇 달 포 전부터 콜록거리는 아내 생각에 그토록 먹고 싶다던\"\n",
    "tokens_org = [\"[CLS]\"] + vocab.encode_as_pieces(string_a) + [\"[SEP]\"] + vocab.encode_as_pieces(string_b) + [\"[SEP]\"]\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b36e2",
   "metadata": {},
   "source": [
    "### 2. MASK 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "045a0728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokens_org)\n",
    "\n",
    "# 전체 token의 15% mask\n",
    "mask_cnt = int((len(tokens_org) - 3) * 0.15)\n",
    "mask_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60ba1048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4] ['▁추', '적', '추', '적']\n",
      "[5, 6] ['▁비', '가']\n",
      "[7, 8] ['▁내', '리는']\n",
      "[9, 10, 11] ['▁날', '이었', '어']\n",
      "[12, 13, 14] ['▁그', '날', '은']\n",
      "[15, 16, 17] ['▁', '왠', '지']\n",
      "[18, 19, 20] ['▁손', '님', '이']\n",
      "[21, 22] ['▁많', '아']\n",
      "[23] ['▁첫']\n",
      "[24, 25] ['▁번', '에']\n",
      "[26, 27] ['▁삼', '십']\n",
      "[28] ['▁전']\n",
      "[29, 30, 31] ['▁둘', '째', '번']\n",
      "[32, 33] ['▁오', '십']\n",
      "[34] ['▁전']\n",
      "[35, 36, 37] ['▁오', '랜', '만에']\n",
      "[38, 39, 40] ['▁받아', '보', '는']\n",
      "[41] ['▁십']\n",
      "[42, 43, 44] ['▁전', '짜', '리']\n",
      "[45, 46, 47] ['▁백', '통', '화']\n",
      "[48, 49, 50] ['▁서', '푼', '에']\n",
      "[52, 53, 54] ['▁손', '바', '닥']\n",
      "[55, 56] ['▁위', '엔']\n",
      "[57, 58, 59] ['▁기', '쁨', '의']\n",
      "[60, 61] ['▁눈', '물이']\n",
      "[62, 63] ['▁흘', '러']\n",
      "[64, 65, 66] ['▁컬', '컬', '한']\n",
      "[67, 68] ['▁목', '에']\n",
      "[69, 70] ['▁모', '주']\n",
      "[71, 72, 73] ['▁한', '잔', '을']\n",
      "[74, 75] ['▁적', '셔']\n",
      "[76] ['▁몇']\n",
      "[77] ['▁달']\n",
      "[78] ['▁포']\n",
      "[79, 80] ['▁전', '부터']\n",
      "[81, 82, 83, 84] ['▁콜', '록', '거', '리는']\n",
      "[85] ['▁아내']\n",
      "[86, 87] ['▁생각', '에']\n",
      "[88, 89, 90] ['▁그', '토', '록']\n",
      "[91, 92] ['▁먹', '고']\n",
      "[93, 94, 95] ['▁싶', '다', '던']\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "cand_idx = []  # word 단위의 index array\n",
    "for (i, token) in enumerate(tokens_org):\n",
    "    if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "        continue\n",
    "    if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미하는 값\n",
    "        cand_idx[-1].append(i)\n",
    "    else:\n",
    "        cand_idx.append([i])\n",
    "\n",
    "# 결과확인\n",
    "for cand in cand_idx:\n",
    "    print(cand, [tokens_org[i] for i in cand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbfb6ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24, 25],\n",
       " [57, 58, 59],\n",
       " [32, 33],\n",
       " [64, 65, 66],\n",
       " [41],\n",
       " [79, 80],\n",
       " [52, 53, 54],\n",
       " [67, 68],\n",
       " [29, 30, 31],\n",
       " [91, 92],\n",
       " [23],\n",
       " [26, 27],\n",
       " [76],\n",
       " [42, 43, 44],\n",
       " [78],\n",
       " [60, 61],\n",
       " [38, 39, 40],\n",
       " [93, 94, 95],\n",
       " [9, 10, 11],\n",
       " [81, 82, 83, 84],\n",
       " [85],\n",
       " [12, 13, 14],\n",
       " [34],\n",
       " [71, 72, 73],\n",
       " [77],\n",
       " [45, 46, 47],\n",
       " [48, 49, 50],\n",
       " [28],\n",
       " [74, 75],\n",
       " [62, 63],\n",
       " [88, 89, 90],\n",
       " [5, 6],\n",
       " [35, 36, 37],\n",
       " [55, 56],\n",
       " [18, 19, 20],\n",
       " [86, 87],\n",
       " [7, 8],\n",
       " [15, 16, 17],\n",
       " [1, 2, 3, 4],\n",
       " [21, 22],\n",
       " [69, 70]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random mask를 위해서 index 순서를 섞음\n",
    "random.shuffle(cand_idx)\n",
    "cand_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a8919dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '번', '[MASK]', '[MASK]', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '프', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[MASK]', '[MASK]', '[MASK]', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "mask_lms = []  # mask 된 값\n",
    "for index_set in cand_idx:\n",
    "    if len(mask_lms) >= mask_cnt:  # 핸재 mask된 개수가 15%를 넘으면 중지\n",
    "          break\n",
    "    if len(mask_lms) + len(index_set) > mask_cnt:  # 이번에 mask할 개수를 포함해 15%를 넘으면 skip\n",
    "          continue\n",
    "    dice = random.random()  # 0과 1 사이의 확률 값\n",
    "\n",
    "    for index in index_set:\n",
    "        masked_token = None\n",
    "        if dice < 0.8:  # 80% replace with [MASK]\n",
    "            masked_token = \"[MASK]\"\n",
    "        elif dice < 0.9: # 10% keep original\n",
    "            masked_token = tokens[index]\n",
    "        else:  # 10% random word\n",
    "            masked_token = random.choice(vocab_list)\n",
    "        mask_lms.append({\"index\": index, \"label\": tokens[index]})\n",
    "        tokens[index] = masked_token\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01288cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_idx   : [23, 24, 25, 32, 33, 41, 57, 58, 59, 64, 65, 66, 79, 80]\n",
      "mask_label : ['▁첫', '▁번', '에', '▁오', '십', '▁십', '▁기', '쁨', '의', '▁컬', '컬', '한', '▁전', '부터']\n"
     ]
    }
   ],
   "source": [
    "# 순서 정렬 및 mask_idx, mask_label 생성\n",
    "mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee26a5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_mask(tokens, mask_cnt, vocab_list):\n",
    "    # 원본 tokens를 deepcopy해서 작업 (원본 훼손 방지)\n",
    "    # tokens = copy.deepcopy(tokens)\n",
    "    \n",
    "    # 띄어쓰기 단위로 mask하기 위해서 index 분할\n",
    "    cand_idx = []  # word 단위의 index array\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token == \"[CLS]\" or token == \"[SEP]\":\n",
    "            continue\n",
    "        if 0 < len(cand_idx) and not token.startswith(u\"\\u2581\"):  # u\"\\u2581\"는 단어의 시작을 의미\n",
    "            cand_idx[-1].append(i)\n",
    "        else:\n",
    "            cand_idx.append([i])\n",
    "\n",
    "    # index 순서를 섞어줌 (random masking)\n",
    "    random.shuffle(cand_idx)\n",
    "\n",
    "    # 마스크 결과 저장할 리스트\n",
    "    mask_lms = []\n",
    "\n",
    "    # cand_idx에서 하나씩 꺼내 마스킹 수행\n",
    "    for index_set in cand_idx:\n",
    "        # 이미 mask_cnt를 넘으면 종료\n",
    "        if len(mask_lms) >= mask_cnt:\n",
    "            break\n",
    "        # 이번 index_set 추가 시 mask_cnt를 초과하면 건너뜀\n",
    "        if len(mask_lms) + len(index_set) > mask_cnt:\n",
    "            continue\n",
    "        \n",
    "        # dice 값으로 마스크 방식 결정\n",
    "        dice = random.random()\n",
    "\n",
    "        for index in index_set:\n",
    "            masked_token = None\n",
    "            if dice < 0.8:  # 80% 확률로 [MASK]\n",
    "                masked_token = \"[MASK]\"\n",
    "            elif dice < 0.9:  # 10% 확률로 원래 토큰 유지\n",
    "                masked_token = tokens[index]\n",
    "            else:  # 10% 확률로 랜덤 토큰 대체\n",
    "                masked_token = random.choice(vocab_list)\n",
    "\n",
    "            # 마스크 정보 저장\n",
    "            mask_lms.append({\n",
    "                \"index\": index,\n",
    "                \"label\": tokens[index]\n",
    "            })\n",
    "\n",
    "            # 토큰 교체\n",
    "            tokens[index] = masked_token\n",
    "\n",
    "    # index 순으로 정렬\n",
    "    mask_lms = sorted(mask_lms, key=lambda x: x[\"index\"])\n",
    "\n",
    "    # 인덱스와 라벨 생성\n",
    "    mask_idx = [p[\"index\"] for p in mask_lms]\n",
    "    mask_label = [p[\"label\"] for p in mask_lms]\n",
    "\n",
    "    return tokens, mask_idx, mask_label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f99f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens_org\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "tokens\n",
      "['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '[MASK]', '[MASK]', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '팹', '著', '內', '[SEP]', '▁손', '바', '닥', '[MASK]', '[MASK]', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '[MASK]', '[MASK]', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]'] \n",
      "\n",
      "mask_idx   : [21, 22, 23, 48, 49, 50, 55, 56, 62, 63, 69, 70, 86, 87]\n",
      "mask_label : ['▁많', '아', '▁첫', '▁서', '푼', '에', '▁위', '엔', '▁흘', '러', '▁모', '주', '▁생각', '에']\n"
     ]
    }
   ],
   "source": [
    "# tokens가 mask되므로 재 실행을 위해서 넣어줌 (테스트용)\n",
    "tokens = copy.deepcopy(tokens_org)\n",
    "\n",
    "tokens, mask_idx, mask_label = create_pretrain_mask(tokens, mask_cnt, vocab_list)\n",
    "\n",
    "print(\"tokens_org\")\n",
    "print(tokens_org, \"\\n\")\n",
    "print(\"tokens\")\n",
    "print(tokens, \"\\n\")\n",
    "\n",
    "print(\"mask_idx   :\", mask_idx)\n",
    "print(\"mask_label :\", mask_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52d25f0",
   "metadata": {},
   "source": [
    "### 3.  NSP pair 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3dd9cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"\"\"추적추적 비가 내리는 날이었어\n",
    "그날은 왠지 손님이 많아\n",
    "첫 번에 삼십 전 둘째 번 오십 전\n",
    "오랜만에 받아보는 십 전짜리 백통화 서푼에\n",
    "손바닥 위엔 기쁨의 눈물이 흘러\n",
    "컬컬한 목에 모주 한잔을 적셔\n",
    "몇 달 포 전부터 콜록거리는 아내\n",
    "생각에 그토록 먹고 싶다던\n",
    "설렁탕 한 그릇을 이제는 살 수 있어\n",
    "집으로 돌아가는 길 난 문득 떠올라\n",
    "아내의 목소리가 거칠어만 가는 희박한 숨소리가\n",
    "오늘은 왠지 나가지 말라던 내 옆에 있어 달라던\n",
    "그리도 나가고 싶으면 일찍이라도 들어와 달라던\n",
    "아내의 간절한 목소리가 들려와\n",
    "나를 원망하듯 비는 점점 거세져\n",
    "싸늘히 식어가는 아내가 떠올라 걱정은 더해져\n",
    "난 몰라 오늘은 운수 좋은 날\n",
    "난 맨날 이렇게 살 수 있으면 얼마나 좋을까\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49a3664f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어'],\n",
       " ['▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아'],\n",
       " ['▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전']]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 줄 단위로 tokenize\n",
    "doc = [vocab.encode_as_pieces(line) for line in string.split(\"\\n\")]\n",
    "doc[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "07e547eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최대 길이\n",
    "n_test_seq = 64\n",
    "# 최소 길이\n",
    "min_seq = 8\n",
    "# [CLS], tokens_a, [SEB], tokens_b, [SEP]\n",
    "max_seq = n_test_seq - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c91b575d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_tokens(tokens_a, tokens_b, max_seq):\n",
    "    \"\"\"\n",
    "    tokens_a, tokens_b의 길이를 줄임 최대 길이: max_seq\n",
    "    :param tokens_a: tokens A\n",
    "    :param tokens_b: tokens B\n",
    "    :param max_seq: 두 tokens 길이의 최대 값\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_seq:\n",
    "            break\n",
    "\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            del tokens_a[0]\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7cbf1791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list):\n",
    "    \"\"\"\n",
    "    doc별 pretrain 데이터 생성\n",
    "    \"\"\"\n",
    "    # for CLS], [SEP], [SEP]\n",
    "    max_seq = n_seq - 3\n",
    "\n",
    "    instances = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    for i in range(len(doc)):\n",
    "        current_chunk.append(doc[i])  # line 단위로 추가\n",
    "        current_length += len(doc[i])  # current_chunk의 token 수\n",
    "        if 1 < len(current_chunk) and (i == len(doc) - 1 or current_length >= max_seq):  # 마지막 줄 이거나 길이가 max_seq 이상 인 경우\n",
    "            \n",
    "\n",
    "            # token a\n",
    "            a_end = 1\n",
    "            if 1 < len(current_chunk):\n",
    "                a_end = random.randrange(1, len(current_chunk))\n",
    "            tokens_a = []\n",
    "            for j in range(a_end):\n",
    "                tokens_a.extend(current_chunk[j])\n",
    "            # token b\n",
    "            tokens_b = []\n",
    "            for j in range(a_end, len(current_chunk)):\n",
    "                tokens_b.extend(current_chunk[j])\n",
    "\n",
    "            if random.random() < 0.5:  # 50% 확률로 swap\n",
    "                is_next = 0    # False\n",
    "                tokens_t = tokens_a\n",
    "                tokens_a = tokens_b\n",
    "                tokens_b = tokens_t\n",
    "            else:\n",
    "                is_next = 1   # True\n",
    "            # max_seq 보다 큰 경우 길이 조절\n",
    "            trim_tokens(tokens_a, tokens_b, max_seq)\n",
    "            assert 0 < len(tokens_a)\n",
    "            assert 0 < len(tokens_b)\n",
    "\n",
    "\n",
    "            # tokens & segment 생성\n",
    "            tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"] + tokens_b + [\"[SEP]\"]\n",
    "            segment = [0] * (len(tokens_a) + 2) + [1] * (len(tokens_b) + 1)\n",
    "            \n",
    "            # mask\n",
    "            tokens, mask_idx, mask_label = create_pretrain_mask(tokens, int((len(tokens) - 3) * 0.15), vocab_list)\n",
    "            \n",
    "\n",
    "            instance = {\n",
    "                \"tokens\": tokens,\n",
    "                \"segment\": segment,\n",
    "                \"is_next\": is_next,\n",
    "                \"mask_idx\": mask_idx,\n",
    "                \"mask_label\": mask_label\n",
    "            }\n",
    "            instances.append(instance)\n",
    "            \n",
    "    return instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f4af23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '▁비', '가', '[MASK]', '[MASK]', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '[MASK]', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '[MASK]', '[MASK]', '[MASK]', '▁눈', '물이', '▁흘', '러', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 6, 7, 27, 56, 57, 58], 'mask_label': ['적', '추', '적', '▁내', '리는', '▁전', '▁기', '쁨', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '[SEP]', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 2, 3, 4, 49, 50, 51, 55, 56], 'mask_label': ['▁추', '적', '추', '적', '▁서', '푼', '에', '▁위', '엔']}\n",
      "{'tokens': ['[CLS]', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '[MASK]', '▁오', '랜', '만에', '▁받아', '보', '는', '[MASK]', '▁전', '짜', '리', '▁백', '통', '화', '▁서', '푼', '에', '[SEP]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [12, 15, 22, 26, 27, 28, 45, 46, 47], 'mask_label': ['▁번', '▁전', '▁십', '▁백', '통', '화', '▁컬', '컬', '한']}\n",
      "{'tokens': ['[CLS]', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '[MASK]', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '[MASK]', '[MASK]', '▁삼', '십', '▁전', '▁둘', '째', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [12, 21, 33, 34, 35, 36, 56, 57, 60], 'mask_label': ['▁몇', '▁아내', '▁추', '적', '추', '적', '▁번', '에', '▁전']}\n",
      "{'tokens': ['[CLS]', '리', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '[SEP]', '[MASK]', '▁달', '▁포', '▁전', '부터', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁아내', '▁생각', '에', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁그', '릇', '을', '▁이', '제는', '▁살', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [33, 38, 39, 40, 41, 53, 54, 55, 56], 'mask_label': ['▁몇', '▁콜', '록', '거', '리는', '▁설', '렁', '탕', '▁한']}\n",
      "{'tokens': ['[CLS]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '[MASK]', '▁이', '제는', '▁살', '▁수', '기술', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '이들은', '계는', '휘', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 13, 14, 15, 20, 44, 45, 46], 'mask_label': ['▁먹', '고', '▁그', '릇', '을', '▁있어', '▁그', '날', '은']}\n",
      "{'tokens': ['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[SEP]', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '[MASK]', '[MASK]', '▁번', '▁오', '십', '▁전', '[MASK]', '[MASK]', '[MASK]', '▁받아', '보', '는', '▁십', '[MASK]', '[MASK]', '[MASK]', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '▁흘', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [30, 31, 36, 37, 38, 43, 44, 45, 62], 'mask_label': ['▁둘', '째', '▁오', '랜', '만에', '▁전', '짜', '리', '▁흘']}\n",
      "{'tokens': ['[CLS]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '[MASK]', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '[MASK]', '▁오', '십', '▁전', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '▁전', '짜', '리', '▁백', '통', '화', '[MASK]', '[MASK]', '[MASK]', '▁손', '바', '닥', '▁위', '엔', '▁기', '쁨', '의', '▁눈', '물이', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [9, 10, 11, 24, 32, 49, 50, 51, 62], 'mask_label': ['▁날', '이었', '어', '▁첫', '▁번', '▁서', '푼', '에', '▁흘']}\n",
      "{'tokens': ['[CLS]', '▁그', '토', '록', '▁먹', '고', '[MASK]', '[MASK]', '[MASK]', '▁설', '렁', '탕', '▁한', '▁그', '릇', '을', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '[SEP]', '▁아내', '의', '[MASK]', '[MASK]', '[MASK]', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '[MASK]', '[MASK]', '▁말', '라', '던', '▁내', '▁옆', '에', '卷', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 35, 36, 37, 54, 55, 62], 'mask_label': ['▁싶', '다', '던', '▁목', '소', '리가', '▁나', '가지', '▁있어']}\n",
      "{'tokens': ['[CLS]', '올', '라', '[MASK]', '[MASK]', '▁목', '소', '리가', '▁거', '칠', '어', '만', '[MASK]', '▁희', '박', '한', '▁숨', '소', '리가', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '[MASK]', '▁옆', '에', '▁있어', '▁달', '라', '던', '[SEP]', '▁그리', '도', '▁나가', '고', '▁싶', '으면', '▁일', '찍', '이라', '도', '▁들어', '와', '[MASK]', '[MASK]', '[MASK]', '▁아내', '의', '▁간', '절', '한', '▁목', '소', '리가', '▁들', '려', '와', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [3, 4, 12, 29, 49, 50, 51, 52, 53], 'mask_label': ['▁아내', '의', '▁가는', '▁내', '▁달', '라', '던', '▁아내', '의']}\n",
      "{'tokens': ['[CLS]', '적', '▁비', '가', '▁내', '리는', '[MASK]', '[MASK]', '[MASK]', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '▁첫', '▁번', '에', '▁삼', '십', '▁전', '▁둘', '째', '▁번', '▁오', '십', '▁전', '[SEP]', '▁오', '랜', '만에', '▁받아', '보', '는', '▁십', '[MASK]', '[MASK]', '[MASK]', '▁백', '통', '화', '▁서', '푼', '에', '▁손', '바', '닥', '▁위', '엔', '위의', '들의', '소련', '▁눈', '물이', '▁흘', '러', '▁컬', '컬', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 40, 41, 42, 54, 55, 56], 'mask_label': ['▁날', '이었', '어', '▁전', '짜', '리', '▁기', '쁨', '의']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁나', '를', '▁원', '망', '하', '듯', '▁비', '는', '▁점', '점', '[MASK]', '[MASK]', '[MASK]', '▁싸', '늘', '히', '▁식', '어', '가는', '▁아내', '가', '▁떠', '올', '라', '▁', '걱', '정은', '▁더', '해', '져', '[SEP]', '▁추', '적', '추', '적', '▁비', '가', '▁내', '리는', '▁날', '이었', '어', '▁그', '날', '은', '▁', '왠', '지', '▁손', '님', '이', '▁많', '아', '르게', '▁번', '에', '[MASK]', '[MASK]', '▁전', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 12, 13, 14, 55, 58, 59, 61, 62], 'mask_label': ['와', '▁거', '세', '져', '▁첫', '▁삼', '십', '▁둘', '째']}\n",
      "{'tokens': ['[CLS]', '제는', '[MASK]', '[MASK]', '▁있어', '[MASK]', '[MASK]', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '어', '만', '▁가는', '▁희', '박', '한', '▁숨', '소', '리가', '[SEP]', '▁오늘', '은', '▁', '왠', '지', '▁나', '가지', '▁말', '라', '던', '[MASK]', '▁옆', '에', '▁있어', '▁달', '라', '던', '[MASK]', '[MASK]', '▁나가', '고', '[MASK]', '[MASK]', '▁일', '찍', '이라', '도', '▁들어', '와', '▁달', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [2, 3, 5, 6, 43, 50, 51, 54, 55], 'mask_label': ['▁살', '▁수', '▁집', '으로', '▁내', '▁그리', '도', '▁싶', '으면']}\n",
      "{'tokens': ['[CLS]', '컬', '한', '▁목', '에', '▁모', '주', '▁한', '잔', '을', '▁적', '셔', '▁몇', '▁달', '▁포', '▁전', '부터', '▁콜', '록', '거', '리는', '▁아내', '[MASK]', '[MASK]', '▁그', '토', '록', '▁먹', '고', '▁싶', '다', '던', '[SEP]', '▁설', '렁', '탕', '▁한', '[MASK]', '[MASK]', '[MASK]', '▁이', '제는', '▁살', '▁수', '▁있어', '▁집', '으로', '▁돌아', '가는', '▁길', '▁난', '▁문', '득', '▁떠', '올', '라', '▁아내', '의', '▁목', '소', '리가', '▁거', '칠', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 8, 9, 22, 23, 37, 38, 39, 42], 'mask_label': ['▁한', '잔', '을', '▁생각', '에', '▁그', '릇', '을', '▁살']}\n"
     ]
    }
   ],
   "source": [
    "instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "\n",
    "# 최종 데이터셋 결과 확인\n",
    "for instance in instances:\n",
    "    print(instance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79255d68",
   "metadata": {},
   "source": [
    "### 4. 데이터생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958361c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3957761"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_file = os.getenv('HOME')+'/aiffel/bert_pretrain/data/kowiki.txt'\n",
    "\n",
    "# line count 확인\n",
    "total = 0\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    for line in in_f:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23712dbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<09:42, 6798.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 lines : ['▁지', '미', '▁카', '터']\n",
      "['▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.']\n",
      "['▁그는', '▁2002', '년', '▁말', '▁인', '권', '과', '▁중', '재', '▁역할', '에', '▁대한', '▁공', '로를', '▁인정', '받아', '▁노', '벨', '▁평화', '상을', '▁받', '게', '▁되었다', '.']\n",
      "\n",
      "14 lines : ['▁수학']\n",
      "['▁수학', '(', '數', '學', ',', '▁)', '은', '▁양', ',', '▁구조', ',', '▁공간', ',', '▁변화', ',', '▁미', '적', '분', '▁등의', '▁개념', '을', '▁다루', '는', '▁학', '문', '이다', '.', '▁현대', '▁수학', '은', '▁형식', '▁논', '리를', '▁이용', '해서', '▁공', '리로', '▁구성된', '▁추', '상', '적', '▁구조를', '▁연구', '하는', '▁학', '문', '으로', '▁여겨', '지', '기도', '▁한다', '.', '▁수학', '은', '▁그', '▁구조', '와', '▁발전', '▁과정', '에서는', '▁자연', '과학', '에', '▁속하는', '▁물리', '학을', '▁비롯한', '▁다른', '▁학', '문', '들과', '▁깊', '은', '▁연', '관을', '▁맺', '고', '▁있다', '.', '▁하지만', ',', '▁어느', '▁과학', '의', '▁분야', '들과', '는', '▁달리', ',', '▁자연', '계에서', '▁관측', '되지', '▁않는', '▁개념', '들에', '▁대해서', '까지', '▁이론', '을', '▁일반', '화', '▁및', '▁추', '상', '화', '시', '킬', '▁수', '▁있다는', '▁차', '이가', '▁있다고', '▁한다', '.', '▁수', '학자', '들은', '▁그러', '한', '▁개념', '들에', '▁대해서', '▁추', '측', '을', '▁하고', ',', '▁적', '절', '하게', '▁선택', '된', '▁정의', '와', '▁공', '리', '로부터', '의', '▁엄', '밀', '한', '▁연', '역을', '▁통해', '서', '▁추', '측', '들의', '▁진', '위를', '▁파', '악', '한다', '.']\n",
      "['▁수', '학의', '▁기초', '를', '▁확', '실', '히', '▁세', '우', '기', '▁위해', ',', '▁수', '리', '논', '리', '학과', '▁집합', '론', '이', '▁발전', '하였고', ',', '▁이와', '▁더불어', '▁범', '주', '론', '이', '▁최근', '에도', '▁발전', '되고', '▁있다', '.', '▁“', '근', '본', '▁위', '기', '”', '라는', '▁말', '은', '▁대', '략', '▁19', '00', '년', '에서', '▁1930', '년', '▁사이에', '▁일어난', ',', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁탐', '구를', '▁상징', '적으로', '▁보여', '주는', '▁말이다', '.', '▁수', '학의', '▁엄', '밀', '한', '▁기초', '에', '▁대한', '▁몇', '▁가지', '▁의견', '▁불', '일', '치는', '▁오늘날', '에도', '▁계속', '되고', '▁있다', '.', '▁수', '학의', '▁기초', '에', '▁대한', '▁위', '기는', '▁그', '▁당시', '▁수많은', '▁논', '쟁', '에', '▁의해', '▁촉', '발', '되었으며', ',', '▁그', '▁논', '쟁', '에는', '▁칸', '토', '어의', '▁집합', '론', '과', '▁브라', '우', '어', '-', '힐', '베', '르트', '▁논', '쟁', '이', '▁포함', '되었다', '.']\n",
      "\n",
      "4 lines : ['▁수학', '▁상', '수']\n",
      "['▁수학', '에서', '▁상', '수', '란', '▁그', '▁값', '이', '▁변', '하지', '▁않는', '▁불', '변', '량', '으로', ',', '▁변', '수의', '▁반대', '말', '이다', '.', '▁물리', '▁상', '수', '와는', '▁달리', ',', '▁수학', '▁상', '수는', '▁물리', '적', '▁측정', '과는', '▁상', '관', '없이', '▁정의', '된다', '.']\n",
      "['▁특정', '▁수학', '▁상', '수', ',', '▁예를', '▁들', '면', '▁골', '롬', '-', '딕', '맨', '▁상', '수', ',', '▁프랑', '세', '즈', '-', '로', '빈', '슨', '▁상', '수', ',', '▁formula', '_1', ',', '▁레', '비', '▁상', '수', '같은', '▁상', '수는', '▁다른', '▁수학', '상', '수', '▁또는', '▁함수', '와', '▁약', '한', '▁상', '관', '관', '계', '▁또는', '▁강한', '▁상', '관', '관', '계를', '▁갖', '는다', '.']\n",
      "\n",
      "10 lines : ['▁문학']\n",
      "['▁문학', '(', '文', '學', ')', '은', '▁언', '어를', '▁예술', '적', '▁표현', '의', '▁제', '재', '로', '▁삼', '아', '▁새로운', '▁의미', '를', '▁창', '출', '하여', ',', '▁인간', '과', '▁사회', '를', '▁진', '실', '되', '게', '▁묘사', '하는', '▁예술', '의', '▁하', '위', '분', '야', '이다', '.', '▁간', '단', '하게', '▁설명', '하면', ',', '▁언', '어를', '▁통해', '▁인간의', '▁삶', '을', '▁미', '적', '(', '美', '的', ')', '으로', '▁형', '상', '화', '한', '▁것이라고', '▁볼', '▁수', '▁있다', '.', '▁문학', '은', '▁원래', '▁문', '예', '(', '文', '藝', ')', '라고', '▁부', '르는', '▁것이', '▁', '옳', '으며', ',', '▁문', '학을', '▁학', '문', '의', '▁대상', '으로서', '▁탐', '구', '하는', '▁학', '문', '의', '▁명칭', '▁역시', '▁문', '예', '학', '이다', '.', '▁문', '예', '학', '은', '▁음악', '사', '학', ',', '▁미술', '사', '학', '▁등과', '▁함께', '▁예술', '학의', '▁핵', '심', '분', '야', '로서', '▁인', '문', '학의', '▁하', '위', '범', '주에', '▁포함', '된다', '.']\n",
      "['▁반', '영', '론', '적', '▁관', '점에', '▁의한', '▁감', '상은', '▁작품', '을', '▁창', '작', '된', '▁당시', '▁시대', '▁정', '황', '과', '▁연결', '시켜', '▁감', '상', '하는', '▁입', '장', '이고', ',', '▁내', '재', '적', '▁관', '점', '의', '▁감', '상은', '▁작품', '의', '▁형식', ',', '▁내용', '에', '▁국', '한', '하여', '▁감', '상', '하는', '▁것이다', '.', '▁표현', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁작가', '의', '▁전기', '적', '▁사실', '과', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것이', '고', ',', '▁수용', '론', '적', '▁관', '점', '의', '▁감', '상은', '▁독', '자와', '▁작품', '을', '▁연결', '시켜', '▁감', '상', '하는', '▁것을', '▁말한다', '.']\n",
      "\n",
      "10 lines : ['▁나라', '▁목록']\n",
      "['▁이', '▁문', '서는', '▁나라', '▁목록', '이며', ',', '▁전', '▁세계', '▁20', '6', '개', '▁나라', '의', '▁각', '▁현', '황', '과', '▁주', '권', '▁승', '인', '▁정보를', '▁개', '요', '▁형태로', '▁나', '열', '하고', '▁있다', '.']\n",
      "['▁위', '▁목록', '에', '▁포함', '되지', '▁않은', '▁다음', '▁국가', '는', '▁몬', '테', '비', '데', '오', '▁협', '약', '의', '▁모든', '▁조건', '을', '▁만족', '하지', '▁못', '하거나', ',', '▁자주', '적이고', '▁독립', '적', '임을', '▁주장', '하지', '▁않는', '▁국가', '이다', '.']\n",
      "\n",
      "['▁화학']\n",
      "['▁화학', '(', '化', '學', ',', '▁)', '은', '▁물질', '의', '▁성', '질', ',', '▁조성', ',', '▁구조', ',', '▁변화', '▁및', '▁그', '에', '▁수', '반', '하는', '▁에너', '지의', '▁변', '화를', '▁연구', '하는', '▁자연', '과', '학의', '▁한', '▁분야', '이다', '.', '▁물리', '학', '도', '▁역시', '▁물질', '을', '▁다루', '는', '▁학', '문', '이지만', ',', '▁물리', '학', '이', '▁원', '소', '와', '▁화', '합', '물을', '▁모두', '▁포함한', '▁물', '체의', '▁운동', '과', '▁에너', '지', ',', '▁열', '적', '·', '전', '기', '적', '·', '광', '학적', '·', '기', '계', '적', '▁속', '성을', '▁다루', '고', '▁이러한', '▁현', '상', '으로부터', '▁통일', '된', '▁이론', '을', '▁구축', '하려는', '▁것', '과는', '▁달리', '▁화학', '에서는', '▁물질', '▁자', '체를', '▁연구', '▁대상으로', '▁한다', '.', '▁화학', '은', '▁이미', '▁존재', '하는', '▁물질', '을', '▁이용하여', '▁특', '정한', '▁목', '적', '에', '▁맞', '는', '▁새로운', '▁물질', '을', '▁합', '성', '하는', '▁길', '을', '▁제공', '하며', ',', '▁이는', '▁농', '작', '물의', '▁증', '산', ',', '▁질', '병', '의', '▁치료', '▁및', '▁예', '방', ',', '▁에너', '지', '▁효', '율', '▁증', '대', ',', '▁환경', '오', '염', '▁감소', '▁등', '▁여러', '▁가지', '▁이', '점을', '▁제공', '한다', '.']\n",
      "['▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '해', '낸', '▁화', '합', '물을', '▁뜻', '하였으나', '▁지금', '은', '▁유', '기', '▁화', '합', '물의', '▁범', '위가', '▁크게', '▁넓', '어져', '▁탄', '소', '▁사', '슬', '▁또는', '▁탄', '소', '▁고', '리를', '▁가진', '▁모든', '▁화', '합', '물을', '▁뜻', '한다', '.', '▁유', '기', '화', '학의', '▁오', '랜', '▁관', '심', '사는', '▁유', '기', '▁화', '합', '물의', '▁합', '성', '▁메', '커', '니', '즘', '이다', '.', '▁현', '대에', '▁들어', '서', '▁핵', '자', '기', '▁공', '명', '법', '과', '▁X', '선', '▁결정', '학', '▁등이', '▁개발', '되어', '▁유', '기', '▁화', '합', '물', '▁분석', '에', '▁있어서', '▁매우', '▁중요한', '▁방법', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 위키가 주제별로 잘 나눠지는지 여부 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)  \n",
    "            if 0 < len(doc):\n",
    "                if 0 < count:\n",
    "                    count -= 1\n",
    "                    print(len(doc), \"lines :\", doc[0])\n",
    "                    print(doc[1])\n",
    "                    print(doc[-1])\n",
    "                    print()\n",
    "                else:\n",
    "                    break\n",
    "                doc = []\n",
    "        else:  # 빈 줄이 아니면 doc에 저장\n",
    "            pieces = vocab.encode_as_pieces(line)    \n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        print(doc[0])\n",
    "        print(doc[1])\n",
    "        print(doc[-1])\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19a3fbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 105/3957761 [00:00<34:32, 1909.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc: 21 instances: 20\n",
      "{'tokens': ['[CLS]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 2, 3, 4, 5, 46, 47, 48, 49], 'mask_label': ['으로', '▁자리', '잡', '았다', '.', '▁분', '과', '이다', '.']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 9, 21, 22, 44, 45, 50], 'mask_label': ['▁플', '라스', '틱', ',', '▁등', '도', '▁연구', '하는', '▁원래']}\n",
      "\n",
      "doc: 14 instances: 13\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '[MASK]', '[MASK]', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [44, 45, 50, 53, 54, 55, 56, 61, 62], 'mask_label': ['▁연구', '하는', '▁원래', '▁화', '합', '물', '은', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 38, 39, 40, 50, 51, 52, 59, 60], 'mask_label': ['으로', '▁탄', '소로', '▁이루어진', '▁원래', '▁유', '기', '▁동물', '로부터']}\n",
      "\n",
      "doc: 4 instances: 3\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '[MASK]', '[MASK]', '▁화', '합', '물', '은', '▁식물', '이나', '[MASK]', '[MASK]', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 9, 50, 51, 52, 59, 60], 'mask_label': ['▁플', '라스', '틱', ',', '▁원래', '▁유', '기', '▁동물', '로부터']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '[MASK]', '[MASK]', '[MASK]', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [23, 24, 25, 26, 27, 41, 42, 43, 50], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물을', '▁원래']}\n",
      "\n",
      "doc: 10 instances: 9\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '[MASK]', '[MASK]', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '差', 'ね', '시마', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '구성되어', '陟', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [21, 22, 38, 39, 50, 51, 52, 61, 62], 'mask_label': ['▁등', '도', '▁탄', '소로', '▁원래', '▁유', '기', '▁추', '출']}\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 21, 22, 40, 50, 59, 60, 61, 62], 'mask_label': ['으로', '▁등', '도', '▁이루어진', '▁원래', '▁동물', '로부터', '▁추', '출']}\n",
      "\n",
      "doc: 10 instances: 9\n",
      "{'tokens': ['[CLS]', '[MASK]', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁고', '분', '자', '물', '질', '[MASK]', '[MASK]', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [1, 10, 11, 12, 13, 14, 15, 21, 22], 'mask_label': ['으로', '▁합', '성', '섬', '유', '등', '의', '▁등', '도']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '▁화', '합', '물', '은', '[MASK]', '[MASK]', '▁동물', '로부터', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [6, 7, 8, 9, 40, 57, 58, 61, 62], 'mask_label': ['▁플', '라스', '틱', ',', '▁이루어진', '▁식물', '이나', '▁추', '출']}\n",
      "\n",
      "doc: 31 instances: 30\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '왁', '대와', '▁유', '기', '화', '학', '에서', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '[MASK]', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '[MASK]', '▁유', '기', '▁화', '합', '물', '은', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [1, 6, 7, 8, 9, 21, 22, 40, 50], 'mask_label': ['으로', '▁플', '라스', '틱', ',', '▁등', '도', '▁이루어진', '▁원래']}\n",
      "{'tokens': ['[CLS]', '으로', '▁자리', '잡', '았다', '.', '▁플', '라스', '틱', ',', '▁합', '성', '섬', '유', '등', '의', '▁고', '분', '자', '물', '질', '▁등', '도', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁다루', '어진', '다', '.', '[SEP]', '▁유', '기', '화', '학', '은', '▁탄', '소로', '▁이루어진', '▁화', '합', '물을', '▁연구', '하는', '▁분', '과', '이다', '.', '▁원래', '▁유', '기', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁식물', '이나', '▁동물', '로부터', '▁추', '출', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [23, 24, 25, 26, 27, 53, 54, 55, 56], 'mask_label': ['▁유', '기', '화', '학', '에서', '▁화', '합', '물', '은']}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# instance 생성 기능 확인\n",
    "count = 5\n",
    "\n",
    "with open(corpus_file, 'r') as in_f:\n",
    "    doc = []  # 단락 단위로 문서 저장\n",
    "    for line in tqdm(in_f, total=total):\n",
    "        line = line.strip()\n",
    "        if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "            if 0 < len(doc):\n",
    "                instances = create_pretrain_instances(vocab, doc, n_test_seq, 0.15, vocab_list)\n",
    "                # save\n",
    "                print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "                print(instances[0])\n",
    "                print(instances[-1])\n",
    "                print()\n",
    "                doc = []\n",
    "                if 0 < count:  # 테스트를 위해서 부분 처리함\n",
    "                    count -= 1\n",
    "                else:\n",
    "                    break\n",
    "        else:  # doc에 저장\n",
    "            if 0 < len(pieces):\n",
    "                doc.append(pieces)\n",
    "    if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "        instances = create_pretrain_instances(doc, 128)\n",
    "        # save\n",
    "        print(\"doc:\", len(doc), \"instances:\", len(instances))\n",
    "        print(instances[0])\n",
    "        print(instances[-1])\n",
    "        print()\n",
    "        doc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9f1063d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pretrain_data(vocab, in_file, out_file, n_seq, mask_prob=0.15):\n",
    "    \"\"\" pretrain 데이터 생성 \"\"\"\n",
    "    def save_pretrain_instances(out_f, doc):\n",
    "        instances = create_pretrain_instances(vocab, doc, n_seq, mask_prob, vocab_list)\n",
    "        for instance in instances:\n",
    "            out_f.write(json.dumps(instance, ensure_ascii=False))\n",
    "            out_f.write(\"\\n\")\n",
    "\n",
    "    # 특수문자 7개를 제외한 vocab_list 생성\n",
    "    vocab_list = []\n",
    "    for id in range(7, len(vocab)):\n",
    "        if not vocab.is_unknown(id):  # 생성되는 단어 목록이 unknown인 경우는 제거합니다.\n",
    "            vocab_list.append(vocab.id_to_piece(id))\n",
    "\n",
    "    # line count 확인\n",
    "    line_cnt = 0\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        for line in in_f:\n",
    "            line_cnt += 1\n",
    "\n",
    "    with open(in_file, \"r\") as in_f:\n",
    "        with open(out_file, \"w\") as out_f:\n",
    "            doc = []\n",
    "            for line in tqdm(in_f, total=line_cnt):\n",
    "                line = line.strip()\n",
    "                if line == \"\":  # line이 빈줄 일 경우 (새로운 단락)\n",
    "                    if len(doc) > 0:\n",
    "                        save_pretrain_instances(out_f, doc)\n",
    "                        doc = []  # 새로운 단락이 시작되므로 doc 초기화\n",
    "                else:  # line이 빈줄이 아닐 경우 tokenize 해서 doc에 저장\n",
    "                    tokens = vocab.encode_as_pieces(line)\n",
    "                    if tokens:\n",
    "                        doc.append(tokens)\n",
    "            if 0 < len(doc):  # 마지막에 처리되지 않은 doc가 있는 경우\n",
    "                save_pretrain_instances(out_f, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bda8b0df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3957761/3957761 [20:22<00:00, 3237.66it/s] \n"
     ]
    }
   ],
   "source": [
    "pretrain_json_path = os.getenv('HOME')+'/aiffel/bert_pretrain/data/bert_pre_train.json'\n",
    "\n",
    "make_pretrain_data(vocab, corpus_file, pretrain_json_path, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e96ddba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1630496"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 라인수\n",
    "total = 0\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        total += 1\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1177cfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] 0 0 [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 필요한 변수 초기화\n",
    "total = 1000  # 예시로 사용되는 총 인스턴스 수입니다. 실제로는 적절한 값을 설정해야 합니다.\n",
    "n_seq = 128\n",
    "file_size = total * n_seq * np.dtype(np.int32).itemsize\n",
    "\n",
    "# 파일을 생성하고 크기를 설정합니다.\n",
    "with open('enc_tokens.memmap', 'wb') as f:\n",
    "    f.write(b'\\0' * file_size)\n",
    "with open('segments.memmap', 'wb') as f:\n",
    "    f.write(b'\\0' * file_size)\n",
    "with open('labels_nsp.memmap', 'wb') as f:\n",
    "    f.write(b'\\0' * (total * np.dtype(np.int32).itemsize))\n",
    "with open('labels_mlm.memmap', 'wb') as f:\n",
    "    f.write(b'\\0' * file_size)\n",
    "\n",
    "# np.memmap을 사용하여 파일을 메모리에 맵핑합니다.\n",
    "enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='r+', dtype=np.int32, shape=(total, n_seq))\n",
    "segments = np.memmap(filename='segments.memmap', mode='r+', dtype=np.int32, shape=(total, n_seq))\n",
    "labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='r+', dtype=np.int32, shape=(total,))\n",
    "labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='r+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "# 예: 처음과 마지막 인덱스에 데이터 접근\n",
    "print(enc_tokens[0], enc_tokens[-1], segments[0], segments[-1], labels_nsp[0], labels_nsp[-1], labels_mlm[0], labels_mlm[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccc3b367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/tmp/ipykernel_31/767648317.py:16: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_31/767648317.py:17: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_31/767648317.py:18: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
      "  1%|          | 6/1000 [00:00<00:05, 195.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '▁제임스', '▁얼', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '[MASK]', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '[MASK]', '[MASK]', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '[MASK]', '[MASK]', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '[MASK]', '[MASK]', '[MASK]', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '[MASK]', '▁별', '명이', '[MASK]', '[MASK]', '[MASK]', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 8, 9, 10, 23, 24, 25, 55, 56, 65, 66, 103, 104, 105, 121, 124, 125, 126], 'mask_label': ['▁\"', '지', '미', '\"', '▁~', '▁)', '는', '▁카운', '티', '▁조지', '아', '▁예', '편', '하였고', '▁그의', '▁\"', '땅', '콩']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 3324, 1042, 6, 6, 6, 6, 207, 3714, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 6, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 203, 3008, 3625, 3616, 16, 3599, 4, 18, 3686, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 6, 6, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 6, 6, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 6, 6, 6, 165, 1697, 4290, 3873, 3703, 3683, 593, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 6, 587, 931, 6, 6, 6, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0  103 3610 3686 3718    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  203  241 3602    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0 3565\n",
      " 3835    0    0    0    0    0    0    0    0 1755 3630    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0  168 3877  414    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  307    0    0  103 4313\n",
      " 4290    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁\"', '땅', '콩', '▁농', '부', '\"', '▁(', 'P', 'e', 'an', 'ut', '▁F', 'ar', 'm', 'er', ')', '로', '▁알려', '졌다', '.', '▁196', '2', '년', '자를', '▁나누', '▁주', '[MASK]', '[MASK]', '▁의원', '▁선거', '에서', '[MASK]', '[MASK]', '[MASK]', '▁그', '▁선거', '가', '▁부정', '선거', '▁', '였', '음을', '[MASK]', '[MASK]', '[MASK]', '▁되어', '▁당선', '되고', ',', '▁196', '6', '년', '▁조지', '아', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁목표', '뀌', '[MASK]', '[MASK]', '▁전', '▁조지', '아', '주', '[MASK]', '[MASK]', '[MASK]', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '▁근무', '했다', '.', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '▁미국', '에', '[MASK]', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '[SEP]', '▁지', '미', '▁카', '터', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [24, 25, 27, 28, 32, 33, 34, 43, 44, 45, 73, 74, 75, 76, 81, 82, 83, 112], 'mask_label': ['▁조지', '아', '▁상', '원', '▁낙', '선', '하나', '▁입', '증', '하게', '▁대통령', '이', '▁되', '기', '▁상', '원의', '원을', '▁사는']}\n",
      "enc_token: [5, 103, 4313, 4290, 613, 3638, 3718, 98, 3878, 3656, 256, 2543, 309, 337, 3735, 181, 3616, 3603, 489, 376, 3599, 386, 3619, 3625, 690, 2749, 37, 6, 6, 2378, 822, 10, 6, 6, 6, 13, 822, 3608, 2386, 2163, 3596, 3671, 969, 6, 6, 6, 607, 2387, 317, 3604, 386, 3673, 3625, 1755, 3630, 37, 18, 3620, 822, 3600, 1567, 3668, 1447, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 1867, 4409, 6, 6, 25, 1755, 3630, 3646, 6, 6, 6, 157, 3821, 61, 3773, 530, 3604, 3372, 523, 3409, 673, 1755, 3630, 18, 982, 2711, 31, 3599, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 243, 3600, 6, 1733, 3628, 50, 3717, 2046, 114, 3692, 1853, 3599, 4, 18, 3686, 207, 3714, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0 1755 3630    0   76\n",
      " 3667    0    0    0 1567 3668 3294    0    0    0    0    0    0    0\n",
      "    0  213 3929  173    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  663 3597  450 3614    0    0    0    0   76  955  928\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      " 3554    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '▁제임스', '▁얼', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁~', '▁1981', '년', ')', '이다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '[MASK]', '▁후', '▁해', '군에', '▁들어가', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁승', '무', '원으로', '[MASK]', '[MASK]', '[MASK]', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '▁별', '명이', '▁\"', '땅', '콩', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [7, 8, 9, 10, 11, 12, 34, 35, 36, 37, 74, 76, 77, 93, 94, 95, 113, 114], 'mask_label': ['▁\"', '지', '미', '\"', '▁카', '터', '▁(19', '7', '7', '년', '▁그', '▁해', '군에', '▁일', '하였다', '.', '▁가', '꿔']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 3324, 1042, 6, 6, 6, 6, 6, 6, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 203, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 6, 6, 6, 6, 203, 3008, 3625, 3616, 16, 3599, 4, 18, 3686, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 6, 81, 87, 1501, 2247, 25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636, 3779, 3601, 249, 3725, 1232, 6, 6, 6, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 593, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 307, 587, 931, 103, 4313, 4290, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0  103 3610 3686 3718  207 3714    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1647 3682 3682 3625    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0   13    0   87 1501    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0   33   52 3599    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0   21 5007    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '하게', '▁되어', '▁당선', '되고', ',', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁주', '▁지', '사', '▁선거', '에', '▁낙', '선', '하지만', '▁1970', '년', '▁조지', '아', '▁주', '▁지', '사를', '▁역임', '했다', '.', '▁대통령', '이', '[MASK]', '[MASK]', '▁전', '[MASK]', '[MASK]', '[MASK]', '▁상', '원의', '원을', '▁두', '번', '▁연', '임', '했으며', ',', '▁1971', '년부터', '▁1975', '년까지', '▁조지', '아', '▁지', '사로', '[MASK]', '[MASK]', '[MASK]', '▁조지', '아', '▁주', '지', '사로', '▁지', '내', '면서', ',', '[MASK]', '[MASK]', '▁사는', '▁흑', '인', '▁등', '용', '법을', '▁내', '세', '웠다', '.', '▁1976', '년', '▁대통령', '▁선거', '에', '▁민주', '당', '▁후보', '로', '▁출', '마', '하여', '▁도', '덕', '주의', '▁정책', '으로', '▁내', '세', '워', ',', '▁포', '드를', '▁누', '르고', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁카', '터', '▁대통령', '은', '▁에너', '지', '▁개발', '을', '▁촉', '구', '했으나', '▁공', '화', '당의', '▁반', '대로', '▁무', '산', '되었다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [6, 7, 8, 9, 10, 31, 32, 34, 35, 36, 54, 55, 56, 66, 67, 103, 104, 105], 'mask_label': ['▁196', '6', '년', '▁조지', '아', '▁되', '기', '▁조지', '아', '주', '▁근무', '했다', '.', '▁미국', '에', '▁당선', '되었다', '.']}\n",
      "enc_token: [5, 173, 607, 2387, 317, 3604, 6, 6, 6, 6, 6, 37, 18, 3620, 822, 3600, 1567, 3668, 1447, 1921, 3625, 1755, 3630, 37, 18, 451, 1398, 31, 3599, 663, 3597, 6, 6, 25, 6, 6, 6, 76, 955, 928, 157, 3821, 61, 3773, 530, 3604, 3372, 523, 3409, 673, 1755, 3630, 18, 982, 6, 6, 6, 1755, 3630, 37, 3610, 982, 18, 3754, 151, 3604, 6, 6, 3554, 1733, 3628, 50, 3717, 2046, 114, 3692, 1853, 3599, 3306, 3625, 663, 822, 3600, 1114, 3724, 958, 3603, 117, 3674, 54, 75, 4089, 238, 1421, 9, 114, 3692, 3964, 3604, 119, 1486, 807, 2056, 6, 6, 6, 4, 207, 3714, 663, 3613, 1778, 3610, 570, 3607, 2270, 3653, 1003, 41, 3683, 1547, 141, 448, 107, 3726, 43, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0  386 3673 3625 1755 3630    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  450 3614    0 1755 3630 3646    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0 2711   31\n",
      " 3599    0    0    0    0    0    0    0    0    0  243 3600    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 2387   43 3599    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁지', '미', '▁카', '터', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '[MASK]', '[MASK]', '[MASK]', '▁10', '월', '▁1', '일', '▁~', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '▁~', '▁1981', '년', ')', '이다', '.', '[SEP]', '▁지', '미', '▁카', '터', '는', '▁조지', '아', '주', '▁섬', '터', '▁카운', '티', '▁플', '레', '인', '스', '▁마을', '에서', '▁태어났다', '.', '▁조지', '아', '▁공', '과', '대학교', '를', '▁졸업', '하였다', '.', '▁그', '▁후', '[MASK]', '[MASK]', '▁들어가', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '▁승', '무', '원으로', '▁일', '하였다', '.', '▁195', '3', '년', '▁미국', '▁해군', '▁대', '위로', '▁예', '편', '하였고', '▁이후', '▁땅', '콩', '·', '면', '화', '▁등을', '▁가', '꿔', '▁많은', '▁돈', '을', '▁벌', '었다', '.', '▁그의', '[MASK]', '[MASK]', '▁\"', '땅', '콩', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 1, 'mask_idx': [16, 17, 18, 76, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 122, 123], 'mask_label': ['▁192', '4', '년', '▁해', '군에', '▁전', '함', '·', '원', '자', '력', '·', '잠', '수', '함', '의', '▁별', '명이']}\n",
      "enc_token: [5, 18, 3686, 207, 3714, 3324, 1042, 103, 3610, 3686, 3718, 207, 3714, 37, 3418, 416, 6, 6, 6, 131, 3662, 7, 3629, 203, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 203, 3008, 3625, 3616, 16, 3599, 4, 18, 3686, 207, 3714, 3602, 1755, 3630, 3646, 630, 3714, 3565, 3835, 429, 3740, 3628, 3626, 1369, 10, 1605, 3599, 1755, 3630, 41, 3644, 830, 3624, 1135, 52, 3599, 13, 81, 6, 6, 2247, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 249, 3725, 1232, 33, 52, 3599, 479, 3652, 3625, 243, 2780, 14, 1509, 168, 3877, 414, 165, 1697, 4290, 3873, 3703, 3683, 593, 21, 5007, 399, 1927, 3607, 813, 17, 3599, 307, 6, 6, 103, 4313, 4290, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 1\n",
      "label_mlm: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0  810 3666 3625    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0   87 1501    0   25 3779 3873 3667 3631\n",
      " 3813 3873 4196 3636 3779 3601    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0  587  931    0    0\n",
      "    0    0]\n",
      "\n",
      "{'tokens': ['[CLS]', '▁안', '와', '르', '[MASK]', '[MASK]', '[MASK]', '▁대통령', '과', '▁메', '나', '헴', '[MASK]', '[MASK]', '▁수상', '과', '▁칭', '▁중', '동', '▁평', '화를', '▁위한', '▁캠', '프', '데', '이', '비', '드', '▁협', '정을', '▁체결', '했다', '.', '▁그러나', '[MASK]', '▁공', '화', '당', '과', '▁미국의', '▁유대', '인', '▁단', '체의', '▁반', '발', '을', '▁일으', '켰', '다', '.', '▁1979', '년', '▁백', '악', '관', '에서', '▁양', '국', '▁간의', '▁평화', '조', '약', '으로', '▁이끌', '어졌다', '.', '▁또한', '▁소련', '과', '[MASK]', '[MASK]', '[MASK]', '▁전략', '▁무', '기', '[MASK]', '▁협', '상에', '[MASK]', '[MASK]', '[MASK]', '[MASK]', '[SEP]', '▁지', '미', '▁카', '터', '▁제임스', '▁얼', '▁\"', '지', '미', '\"', '▁카', '터', '▁주', '니어', '(,', '▁192', '4', '년', '▁10', '월', '▁1', '일', '월에', '▁)', '는', '▁민주', '당', '▁출신', '▁미국', '▁3', '9', '번째', '▁대통령', '▁(19', '7', '7', '년', '[MASK]', '▁1981', '년', ')', '이다', '.', '[SEP]'], 'segment': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'is_next': 0, 'mask_idx': [4, 5, 6, 12, 13, 16, 21, 34, 70, 71, 72, 76, 79, 80, 81, 82, 106, 121], 'mask_label': ['▁사', '다', '트', '▁베', '긴', '▁함께', '▁위한', '▁이것은', '▁제', '2', '차', '▁제한', '▁조', '인', '했다', '.', '▁~', '▁~']}\n",
      "enc_token: [5, 172, 3665, 3699, 6, 6, 6, 663, 3644, 334, 3637, 5887, 6, 6, 1011, 3644, 1786, 35, 3658, 232, 934, 521, 2432, 3721, 3736, 3597, 3694, 3681, 617, 666, 2525, 31, 3599, 330, 6, 41, 3683, 3724, 3644, 679, 2670, 3628, 164, 1314, 141, 3720, 3607, 1213, 4174, 3598, 3599, 2995, 3625, 456, 3928, 3708, 10, 230, 3643, 2714, 2793, 3676, 3827, 9, 1435, 2521, 3599, 276, 1302, 3644, 6, 6, 6, 2835, 107, 3614, 6, 617, 1824, 6, 6, 6, 6, 4, 18, 3686, 207, 3714, 3324, 1042, 103, 3610, 3686, 3718, 207, 3714, 37, 3418, 416, 810, 3666, 3625, 131, 3662, 7, 3629, 649, 241, 3602, 1114, 3724, 788, 243, 49, 3632, 796, 663, 1647, 3682, 3682, 3625, 6, 3008, 3625, 3616, 16, 3599, 4]\n",
      "segment: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "label_nsp: 0\n",
      "label_mlm: [   0    0    0    0   15 3598 3677    0    0    0    0    0  271 4099\n",
      "    0    0  280    0    0    0    0  521    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0 1487    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   30 3619 3751    0    0    0 1956    0    0   53 3628   31 3599    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0  203    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0  203    0    0    0    0\n",
      "    0    0]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 라인 단위로 처리\n",
    "with open(pretrain_json_path, \"r\") as f:\n",
    "    for i, line in enumerate(tqdm(f, total=total)):\n",
    "        if 5 < i:  # 테스트를 위해서 5개만 확인\n",
    "            break\n",
    "        data = json.loads(line)\n",
    "        # encoder token\n",
    "        enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "        enc_token += [0] * (n_seq - len(enc_token))\n",
    "        # segment\n",
    "        segment = data[\"segment\"]\n",
    "        segment += [0] * (n_seq - len(segment))\n",
    "        # nsp label\n",
    "        label_nsp = data[\"is_next\"]\n",
    "        # mlm label\n",
    "        mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "        mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "        label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "        label_mlm[mask_idx] = mask_label\n",
    "\n",
    "        print(data)\n",
    "        print(\"enc_token:\", enc_token)\n",
    "        print(\"segment:\", segment)\n",
    "        print(\"label_nsp:\", label_nsp)\n",
    "        print(\"label_mlm:\", label_mlm)\n",
    "        print()\n",
    "\n",
    "        assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "        enc_tokens[i] = enc_token\n",
    "        segments[i] = segment\n",
    "        labels_nsp[i] = label_nsp\n",
    "        labels_mlm[i] = label_mlm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "523ab46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pre_train_data(vocab, filename, n_seq, count=None):\n",
    "    \"\"\"\n",
    "    학습에 필요한 데이터를 로드\n",
    "    :param vocab: vocab\n",
    "    :param filename: 전처리된 json 파일\n",
    "    :param n_seq: 시퀀스 길이 (number of sequence)\n",
    "    :param count: 데이터 수 제한 (None이면 전체)\n",
    "    :return enc_tokens: encoder inputs\n",
    "    :return segments: segment inputs\n",
    "    :return labels_nsp: nsp labels\n",
    "    :return labels_mlm: mlm labels\n",
    "    \"\"\"\n",
    "    total = 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            # 데이터 수 제한\n",
    "            if count is not None and count <= total:\n",
    "                break\n",
    "    \n",
    "    # np.memmap을 사용하면 메모리를 적은 메모리에서도 대용량 데이터 처리가 가능 함\n",
    "    enc_tokens = np.memmap(filename='enc_tokens.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    segments = np.memmap(filename='segments.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "    labels_nsp = np.memmap(filename='labels_nsp.memmap', mode='w+', dtype=np.int32, shape=(total,))\n",
    "    labels_mlm = np.memmap(filename='labels_mlm.memmap', mode='w+', dtype=np.int32, shape=(total, n_seq))\n",
    "\n",
    "    with open(filename, \"r\") as f:\n",
    "        for i, line in enumerate(tqdm(f, total=total)):\n",
    "            if total <= i:\n",
    "                print(\"data load early stop\", total, i)\n",
    "                break\n",
    "            data = json.loads(line)\n",
    "            # encoder token\n",
    "            enc_token = [vocab.piece_to_id(p) for p in data[\"tokens\"]]\n",
    "            enc_token += [0] * (n_seq - len(enc_token))\n",
    "            # segment\n",
    "            segment = data[\"segment\"]\n",
    "            segment += [0] * (n_seq - len(segment))\n",
    "            # nsp label\n",
    "            label_nsp = data[\"is_next\"]\n",
    "            # mlm label\n",
    "            mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
    "            mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
    "            label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
    "            label_mlm[mask_idx] = mask_label\n",
    "\n",
    "            assert len(enc_token) == len(segment) == len(label_mlm) == n_seq\n",
    "\n",
    "            enc_tokens[i] = enc_token\n",
    "            segments[i] = segment\n",
    "            labels_nsp[i] = label_nsp\n",
    "            labels_mlm[i] = label_mlm\n",
    "\n",
    "    return (enc_tokens, segments), (labels_nsp, labels_mlm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cc5ae396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/128000 [00:00<?, ?it/s]/tmp/ipykernel_31/2049745891.py:42: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_idx = np.array(data[\"mask_idx\"], dtype=np.int)\n",
      "/tmp/ipykernel_31/2049745891.py:43: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mask_label = np.array([vocab.piece_to_id(p) for p in data[\"mask_label\"]], dtype=np.int)\n",
      "/tmp/ipykernel_31/2049745891.py:44: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  label_mlm = np.full(n_seq, dtype=np.int, fill_value=0)\n",
      "100%|██████████| 128000/128000 [00:23<00:00, 5448.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data load early stop 128000 128000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 128000건만 메모리에 로딩\n",
    "pre_train_inputs, pre_train_labels = load_pre_train_data(vocab, pretrain_json_path, 128, count=128000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2f0a2d13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(memmap([   5,   18, 3686,  207, 3714, 3324, 1042,    6,    6,    6,    6,\n",
       "          207, 3714,   37, 3418,  416,  810, 3666, 3625,  131, 3662,    7,\n",
       "         3629,    6,  241, 3602, 1114, 3724,  788,  243,   49, 3632,  796,\n",
       "          663, 1647, 3682, 3682, 3625,  203, 3008, 3625, 3616,   16, 3599,\n",
       "            4,   18, 3686,  207, 3714, 3602, 1755, 3630, 3646,  630, 3714,\n",
       "            6,    6,  429, 3740, 3628, 3626, 1369,   10, 1605, 3599,    6,\n",
       "            6,   41, 3644,  830, 3624, 1135,   52, 3599,   13,   81,   87,\n",
       "         1501, 2247,   25, 3779, 3873, 3667, 3631, 3813, 3873, 4196, 3636,\n",
       "         3779, 3601,  249, 3725, 1232,   33,   52, 3599,  479, 3652, 3625,\n",
       "          243, 2780,   14, 1509,    6,    6,    6,  165, 1697, 4290, 3873,\n",
       "         3703, 3683,  593,   21, 5007,  399, 1927, 3607,  813,   17, 3599,\n",
       "            6,  587,  931,    6,    6,    6,    4], dtype=int32),\n",
       " memmap([   5, 3615, 1426, 3892, 3949, 3628,  587, 3613,   19, 3688,  283,\n",
       "         3824, 4083, 3644,  237, 3688,  546, 4166, 3607, 1219,  481, 3603,\n",
       "         1909, 3806, 3748, 1573, 1332, 1065, 7827, 3053, 3608,  708, 1781,\n",
       "         3599,   13,  694, 1341, 4793,    6,    6,  555, 3754, 3641, 3604,\n",
       "          848, 4435,   94, 3673, 3597,   14, 3949,    9,    6,    6,    6,\n",
       "          818,  647, 3613,  192, 3695,  684, 1220,  121, 3599,    4,    6,\n",
       "            6,    6,  192, 3695, 3650, 3617, 3739, 3761, 3890,  181, 3700,\n",
       "         3680, 3989, 3664, 3616, 3617, 6107, 4609, 4990, 3616, 3613,   90,\n",
       "         3650, 3617, 3700, 3680, 3989, 3664, 3616,  471, 1778, 1008,    6,\n",
       "          587, 3601, 3321, 3607,    6,    6,  192, 3695, 1380,    6,  367,\n",
       "          879,  683,  535,  409,   59, 3604, 3321,  123,   95, 3620, 1138,\n",
       "         1213, 3793,  329, 3604,    6,    6,    4], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " memmap([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int32),\n",
       " 1,\n",
       " 0,\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,  103, 3610, 3686, 3718,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,  203,  241, 3602,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "         3565, 3835,    0,    0,    0,    0,    0,    0,    0,    0, 1755,\n",
       "         3630,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,  168, 3877,  414,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          307,    0,    0,  103, 4313, 4290,    0], dtype=int32),\n",
       " memmap([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,  546, 4166,    0,    0,    0,    0,\n",
       "            0,    0,    0, 2923, 2283, 1330, 3607,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,   95, 3771,  317,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  192,\n",
       "         3695, 3650,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  459,\n",
       "            0,    0,    0,    0, 2012, 3599,    0,    0,    0,   13,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,  135, 3776,    0], dtype=int32))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 처음과 마지막 확인\n",
    "pre_train_inputs[0][0], pre_train_inputs[0][-1], pre_train_inputs[1][0], pre_train_inputs[1][-1], pre_train_labels[0][0], pre_train_labels[0][-1], pre_train_labels[1][0], pre_train_labels[1][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04609cee",
   "metadata": {},
   "source": [
    "### 5. BERT 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "62e8bd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def get_pad_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    pad mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: pad mask (pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    mask = tf.cast(tf.math.equal(tokens, i_pad), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=1)\n",
    "    return mask\n",
    "\n",
    "\n",
    "def get_ahead_mask(tokens, i_pad=0):\n",
    "    \"\"\"\n",
    "    ahead mask 계산하는 함수\n",
    "    :param tokens: tokens (bs, n_seq)\n",
    "    :param i_pad: id of pad\n",
    "    :return mask: ahead and pad mask (ahead or pad: 1, other: 0)\n",
    "    \"\"\"\n",
    "    n_seq = tf.shape(tokens)[1]\n",
    "    ahead_mask = 1 - tf.linalg.band_part(tf.ones((n_seq, n_seq)), -1, 0)\n",
    "    ahead_mask = tf.expand_dims(ahead_mask, axis=0)\n",
    "    pad_mask = get_pad_mask(tokens, i_pad)\n",
    "    mask = tf.maximum(ahead_mask, pad_mask)\n",
    "    return mask\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7544cc2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "@tf.function(experimental_relax_shapes=True)\n",
    "def gelu(x):\n",
    "    \"\"\"\n",
    "    gelu activation 함수\n",
    "    :param x: 입력 값\n",
    "    :return: gelu activation result\n",
    "    \"\"\"\n",
    "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0e8125e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def kernel_initializer(stddev=0.02):\n",
    "    \"\"\"\n",
    "    parameter initializer 생성\n",
    "    :param stddev: 생성할 랜덤 변수의 표준편차\n",
    "    \"\"\"\n",
    "    return tf.keras.initializers.TruncatedNormal(stddev=stddev)\n",
    "\n",
    "\n",
    "def bias_initializer():\n",
    "    \"\"\"\n",
    "    bias initializer 생성\n",
    "    \"\"\"\n",
    "    return tf.zeros_initializer\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a55f70c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Config(dict):\n",
    "    \"\"\"\n",
    "    json을 config 형태로 사용하기 위한 Class\n",
    "    :param dict: config dictionary\n",
    "    \"\"\"\n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, file):\n",
    "        \"\"\"\n",
    "        file에서 Config를 생성 함\n",
    "        :param file: filename\n",
    "        \"\"\"\n",
    "        with open(file, 'r') as f:\n",
    "            config = json.loads(f.read())\n",
    "            return Config(config)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c47ecd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class SharedEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Weighed Shaed Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"weight_shared_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.n_vocab = config.n_vocab\n",
    "        self.d_model = config.d_model\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        \"\"\"\n",
    "        shared weight 생성\n",
    "        :param input_shape: Tensor Shape (not used)\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"shared_embedding_weight\"):\n",
    "            self.shared_weights = self.add_weight(\n",
    "                \"weights\",\n",
    "                shape=[self.n_vocab, self.d_model],\n",
    "                initializer=kernel_initializer()\n",
    "            )\n",
    "\n",
    "    def call(self, inputs, mode=\"embedding\"):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :param mode: 실행 모드\n",
    "        :return: embedding or linear 실행 결과\n",
    "        \"\"\"\n",
    "        # mode가 embedding일 경우 embedding lookup 실행\n",
    "        if mode == \"embedding\":\n",
    "            return self._embedding(inputs)\n",
    "        # mode가 linear일 경우 linear 실행\n",
    "        elif mode == \"linear\":\n",
    "            return self._linear(inputs)\n",
    "        # mode가 기타일 경우 오류 발생\n",
    "        else:\n",
    "            raise ValueError(f\"mode {mode} is not valid.\")\n",
    "    \n",
    "    def _embedding(self, inputs):\n",
    "        \"\"\"\n",
    "        embedding lookup\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        embed = tf.gather(self.shared_weights, tf.cast(inputs, tf.int32))\n",
    "        return embed\n",
    "\n",
    "    def _linear(self, inputs):  # (bs, n_seq, d_model)\n",
    "        \"\"\"\n",
    "        linear 실행\n",
    "        :param inputs: 입력\n",
    "        \"\"\"\n",
    "        n_batch = tf.shape(inputs)[0]\n",
    "        n_seq = tf.shape(inputs)[1]\n",
    "        inputs = tf.reshape(inputs, [-1, self.d_model])  # (bs * n_seq, d_model)\n",
    "        outputs = tf.matmul(inputs, self.shared_weights, transpose_b=True)\n",
    "        outputs = tf.reshape(outputs, [n_batch, n_seq, self.n_vocab])  # (bs, n_seq, n_vocab)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fe5a56d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionEmbedding(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Embedding Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"position_embedding\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(config.n_seq, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: 입력\n",
    "        :return embed: position embedding lookup 결과\n",
    "        \"\"\"\n",
    "        position = tf.cast(tf.math.cumsum(tf.ones_like(inputs), axis=1, exclusive=True), tf.int32)\n",
    "        embed = self.embedding(position)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebb70a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class ScaleDotProductAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Scale Dot Product Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, name=\"scale_dot_product_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        attn_score = tf.matmul(Q, K, transpose_b=True)\n",
    "        scale = tf.math.sqrt(tf.cast(tf.shape(K)[-1], tf.float32))\n",
    "        attn_scale = tf.math.divide(attn_score, scale)\n",
    "        attn_scale -= 1.e9 * attn_mask\n",
    "        attn_prob = tf.nn.softmax(attn_scale, axis=-1)\n",
    "        attn_out = tf.matmul(attn_prob, V)\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79cb44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Q. 주석과 코드를 참조하여 아래 클래스를 완성해주세요.\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Multi Head Attention Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"multi_head_attention\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.d_model = config.d_model\n",
    "        self.n_head = config.n_head\n",
    "        self.d_head = config.d_head\n",
    "\n",
    "        # Q, K, V input dense layer\n",
    "        self.W_Q = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_K = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_V = tf.keras.layers.Dense(config.n_head * config.d_head, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        # Scale Dot Product Attention class\n",
    "        self.attention = ScaleDotProductAttention(name=\"self_attention\")\n",
    "        # output dense layer\n",
    "        self.W_O = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, Q, K, V, attn_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param Q: Q value\n",
    "        :param K: K value\n",
    "        :param V: V value\n",
    "        :param attn_mask: 실행 모드\n",
    "        :return attn_out: attention 실행 결과\n",
    "        \"\"\"\n",
    "        # reshape Q, K, V, attn_mask\n",
    "        batch_size = tf.shape(Q)[0]\n",
    "        Q_m = tf.transpose(tf.reshape(self.W_Q(Q), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, Q_len, d_head)\n",
    "        K_m = tf.transpose(tf.reshape(self.W_K(K), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        V_m = tf.transpose(tf.reshape(self.W_V(V), [batch_size, -1, self.n_head, self.d_head]), [0, 2, 1, 3])  # (bs, n_head, K_len, d_head)\n",
    "        attn_mask_m = tf.expand_dims(attn_mask, axis=1)\n",
    "        # Scale Dot Product Attention with multi head Q, K, V, attn_mask\n",
    "        attn_out = self.attention(Q_m, K_m, V_m, attn_mask_m)  # (bs, n_head, Q_len, d_head)\n",
    "        # transpose and liner\n",
    "        attn_out_m = tf.transpose(attn_out, perm=[0, 2, 1, 3])  # (bs, Q_len, n_head, d_head)\n",
    "        attn_out = tf.reshape(attn_out_m, [batch_size, -1, config.n_head * config.d_head])  # (bs, Q_len, d_model)\n",
    "        attn_out = self.W_O(attn_out) # (bs, Q_len, d_model)\n",
    "\n",
    "\n",
    "        return attn_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49c9f151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PositionWiseFeedForward(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Position Wise Feed Forward Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"feed_forward\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.W_1 = tf.keras.layers.Dense(config.d_ff, activation=gelu, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.W_2 = tf.keras.layers.Dense(config.d_model, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: inputs\n",
    "        :return ff_val: feed forward 실행 결과\n",
    "        \"\"\"\n",
    "        ff_val = self.W_2(self.W_1(inputs))\n",
    "        return ff_val\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a2a3fac5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Encoder Layer Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"encoder_layer\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(config)\n",
    "        self.norm1 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.ffn = PositionWiseFeedForward(config)\n",
    "        self.norm2 = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    " \n",
    "    def call(self, enc_embed, self_mask):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param enc_embed: enc_embed 또는 이전 EncoderLayer의 출력\n",
    "        :param self_mask: enc_tokens의 pad mask\n",
    "        :return enc_out: EncoderLayer 실행 결과\n",
    "        \"\"\"\n",
    "        self_attn_val = self.self_attention(enc_embed, enc_embed, enc_embed, self_mask)\n",
    "        norm1_val = self.norm1(enc_embed + self.dropout(self_attn_val))\n",
    "\n",
    "        ffn_val = self.ffn(norm1_val)\n",
    "        enc_out = self.norm2(norm1_val + self.dropout(ffn_val))\n",
    "\n",
    "        return enc_out\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf052b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class BERT(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    BERT Class\n",
    "    \"\"\"\n",
    "    def __init__(self, config, name=\"bert\"):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param config: Config 객체\n",
    "        :param name: layer name\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.i_pad = config.i_pad\n",
    "        self.embedding = SharedEmbedding(config)\n",
    "        self.position = PositionEmbedding(config)\n",
    "        self.segment = tf.keras.layers.Embedding(2, config.d_model, embeddings_initializer=kernel_initializer())\n",
    "        self.norm = tf.keras.layers.LayerNormalization(epsilon=config.layernorm_epsilon)\n",
    "        \n",
    "        self.encoder_layers = [EncoderLayer(config, name=f\"encoder_layer_{i}\") for i in range(config.n_layer)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        layer 실행\n",
    "        :param inputs: (enc_tokens, segments)\n",
    "        :return logits: dec_tokens에 대한 다음 토큰 예측 결과 logits\n",
    "        \"\"\"\n",
    "        enc_tokens, segments = inputs\n",
    "\n",
    "        enc_self_mask = tf.keras.layers.Lambda(get_pad_mask, output_shape=(1, None), name='enc_self_mask')(enc_tokens, self.i_pad)\n",
    "\n",
    "        enc_embed = self.get_embedding(enc_tokens, segments)\n",
    "\n",
    "        enc_out = self.dropout(enc_embed)\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            enc_out = encoder_layer(enc_out, enc_self_mask)\n",
    "\n",
    "        logits_cls = enc_out[:,0]\n",
    "        logits_lm = self.embedding(enc_out, mode=\"linear\")\n",
    "        return logits_cls, logits_lm\n",
    "    \n",
    "    def get_embedding(self, tokens, segments):\n",
    "        \"\"\"\n",
    "        token embedding, position embedding lookup\n",
    "        :param tokens: 입력 tokens\n",
    "        :param segments: 입력 segments\n",
    "        :return embed: embedding 결과\n",
    "        \"\"\"\n",
    "        embed = self.embedding(tokens) + self.position(tokens) + self.segment(segments)\n",
    "        embed = self.norm(embed)\n",
    "        return embed\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "14c575e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Encoder Layer class 정의\n",
    "class PooledOutput(tf.keras.layers.Layer):\n",
    "    def __init__(self, config, n_output, name=\"pooled_output\"):\n",
    "        super().__init__(name=name)\n",
    "\n",
    "        self.dense1 = tf.keras.layers.Dense(config.d_model, activation=tf.nn.tanh, kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    "        self.dense2 = tf.keras.layers.Dense(n_output, use_bias=False, activation=tf.nn.softmax, name=\"nsp\", kernel_initializer=kernel_initializer(), bias_initializer=bias_initializer())\n",
    " \n",
    "    def call(self, inputs):\n",
    "        outputs = self.dense1(inputs)\n",
    "        outputs = self.dense2(outputs)\n",
    "        return outputs\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22c094f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def build_model_pre_train(config):\n",
    "    enc_tokens = tf.keras.layers.Input((None,), name=\"enc_tokens\")\n",
    "    segments = tf.keras.layers.Input((None,), name=\"segments\")\n",
    "\n",
    "    bert = BERT(config)\n",
    "    logits_cls, logits_lm = bert((enc_tokens, segments))\n",
    "\n",
    "    logits_cls = PooledOutput(config, 2, name=\"pooled_nsp\")(logits_cls)\n",
    "    outputs_nsp = tf.keras.layers.Softmax(name=\"nsp\")(logits_cls)\n",
    "\n",
    "    outputs_mlm = tf.keras.layers.Softmax(name=\"mlm\")(logits_lm)\n",
    "\n",
    "    model = tf.keras.Model(inputs=(enc_tokens, segments), outputs=(outputs_nsp, outputs_mlm))\n",
    "    return model\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bd0944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'d_model': 128,\n",
       " 'n_head': 2,\n",
       " 'd_head': 32,\n",
       " 'dropout': 0.1,\n",
       " 'd_ff': 256,\n",
       " 'layernorm_epsilon': 0.001,\n",
       " 'n_layer': 3,\n",
       " 'n_seq': 256,\n",
       " 'n_vocab': 8007,\n",
       " 'i_pad': 0}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 사이즈 작게 수정(1만 파라미터)\n",
    "config = Config({\n",
    "    \"d_model\": 128, # 256\n",
    "    \"n_head\": 2,    # 4\n",
    "    \"d_head\": 32,   # 64\n",
    "    \"dropout\": 0.1, \n",
    "    \"d_ff\": 256,    # 1024\n",
    "    \"layernorm_epsilon\": 0.001, \n",
    "    \"n_layer\": 3, \n",
    "    \"n_seq\": 256, \n",
    "    \"n_vocab\": 0, \n",
    "    \"i_pad\": 0})\n",
    "config.n_vocab = len(vocab) # 32000\n",
    "config.i_pad = vocab.pad_id()\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ce526951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "2/2 [==============================] - 20s 10ms/step - loss: 9.7096 - nsp_loss: 0.7002 - mlm_loss: 9.0094 - nsp_acc: 0.5000 - mlm_acc: 0.0000e+00\n",
      "Epoch 2/2\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 9.2928 - nsp_loss: 0.6751 - mlm_loss: 8.6177 - nsp_acc: 0.8000 - mlm_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2764137ca0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_seq = 10\n",
    "\n",
    "# make test inputs\n",
    "enc_tokens = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "segments = np.random.randint(0, 2, (10, n_seq))\n",
    "labels_nsp = np.random.randint(0, 2, (10,))\n",
    "labels_mlm = np.random.randint(0, len(vocab), (10, n_seq))\n",
    "\n",
    "test_model = build_model_pre_train(config)\n",
    "test_model.compile(loss=tf.keras.losses.sparse_categorical_crossentropy, optimizer=tf.keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "\n",
    "# test model fit\n",
    "test_model.fit((enc_tokens, segments), (labels_nsp, labels_mlm), epochs=2, batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477a90d",
   "metadata": {},
   "source": [
    "### 6. Pretrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e281392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    loss 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # loss 계산\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)(y_true, y_pred)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    return loss * 20  # mlm을 더 잘 학습하도록 20배 증가 시킴\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0f7ccefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def lm_acc(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    acc 계산 함수\n",
    "    :param y_true: 정답 (bs, n_seq)\n",
    "    :param y_pred: 예측 값 (bs, n_seq, n_vocab)\n",
    "    \"\"\"\n",
    "    # 정답 여부 확인\n",
    "    y_pred_class = tf.cast(K.argmax(y_pred, axis=-1), tf.float32)\n",
    "    matches = tf.cast(K.equal(y_true, y_pred_class), tf.float32)\n",
    "    # pad(0) 인 부분 mask\n",
    "    mask = tf.cast(tf.math.not_equal(y_true, 0), dtype=matches.dtype)\n",
    "    matches *= mask\n",
    "    # 정확도 계산\n",
    "    accuracy = K.sum(matches) / K.maximum(K.sum(mask), 1)\n",
    "    return accuracy\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "53d8cfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class CosineSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"\n",
    "    CosineSchedule Class\n",
    "    \"\"\"\n",
    "    def __init__(self, train_steps=4000, warmup_steps=2000, max_lr=2.5e-4):\n",
    "        \"\"\"\n",
    "        생성자\n",
    "        :param train_steps: 학습 step 총 합\n",
    "        :param warmup_steps: warmup steps\n",
    "        :param max_lr: 최대 learning rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        assert 0 < warmup_steps < train_steps\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.train_steps = train_steps\n",
    "        self.max_lr = max_lr\n",
    "\n",
    "    def __call__(self, step_num):\n",
    "        \"\"\"\n",
    "        learning rate 계산\n",
    "        :param step_num: 현재 step number\n",
    "        :retrun: 계산된 learning rate\n",
    "        \"\"\"\n",
    "        state = tf.cast(step_num <= self.warmup_steps, tf.float32)\n",
    "        lr1 = tf.cast(step_num, tf.float32) / self.warmup_steps\n",
    "        progress = tf.cast(step_num - self.warmup_steps, tf.float32) / max(1, self.train_steps - self.warmup_steps)\n",
    "        lr2 = 0.5 * (1.0 + tf.math.cos(math.pi * progress))\n",
    "        return (state * lr1 + (1 - state) * lr2) * self.max_lr\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "53656dd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# compute lr \n",
    "test_schedule = CosineSchedule(train_steps=4000, warmup_steps=500)\n",
    "lrs = []\n",
    "for step_num in range(4000):\n",
    "    lrs.append(test_schedule(float(step_num)).numpy())\n",
    "\n",
    "# draw\n",
    "plt.plot(lrs, 'r-', label='learning_rate')\n",
    "plt.xlabel('Step')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d42b26f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "enc_tokens (InputLayer)         [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segments (InputLayer)           [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BERT)                     ((None, 128), (None, 1356736     enc_tokens[0][0]                 \n",
      "                                                                 segments[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pooled_nsp (PooledOutput)       (None, 2)            16768       bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "nsp (Softmax)                   (None, 2)            0           pooled_nsp[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "mlm (Softmax)                   (None, None, 8007)   0           bert[0][1]                       \n",
      "==================================================================================================\n",
      "Total params: 1,373,504\n",
      "Trainable params: 1,373,504\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성\n",
    "pre_train_model = build_model_pre_train(config)\n",
    "pre_train_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a8e85ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps: 20000\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "# optimizer\n",
    "train_steps = math.ceil(len(pre_train_inputs[0]) / batch_size) * epochs\n",
    "print(\"train_steps:\", train_steps)\n",
    "learning_rate = CosineSchedule(train_steps=train_steps, warmup_steps=max(100, train_steps // 10))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "# compile\n",
    "pre_train_model.compile(loss=(tf.keras.losses.sparse_categorical_crossentropy, lm_loss), optimizer=optimizer, metrics={\"nsp\": \"acc\", \"mlm\": lm_acc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7529407c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2000/2000 [==============================] - 131s 64ms/step - loss: 21.1707 - nsp_loss: 0.6019 - mlm_loss: 20.5688 - nsp_acc: 0.6409 - mlm_lm_acc: 0.0609\n",
      "\n",
      "Epoch 00001: mlm_lm_acc improved from -inf to 0.06089, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 2/10\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 18.6452 - nsp_loss: 0.3787 - mlm_loss: 18.2665 - nsp_acc: 0.9326 - mlm_lm_acc: 0.1125\n",
      "\n",
      "Epoch 00002: mlm_lm_acc improved from 0.06089 to 0.11254, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 3/10\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 17.7927 - nsp_loss: 0.3608 - mlm_loss: 17.4319 - nsp_acc: 0.9509 - mlm_lm_acc: 0.1296\n",
      "\n",
      "Epoch 00003: mlm_lm_acc improved from 0.11254 to 0.12962, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 4/10\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 17.2615 - nsp_loss: 0.3534 - mlm_loss: 16.9082 - nsp_acc: 0.9587 - mlm_lm_acc: 0.1375\n",
      "\n",
      "Epoch 00004: mlm_lm_acc improved from 0.12962 to 0.13752, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 5/10\n",
      "2000/2000 [==============================] - 128s 64ms/step - loss: 16.7996 - nsp_loss: 0.3501 - mlm_loss: 16.4495 - nsp_acc: 0.9619 - mlm_lm_acc: 0.1453\n",
      "\n",
      "Epoch 00005: mlm_lm_acc improved from 0.13752 to 0.14534, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 6/10\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 16.3005 - nsp_loss: 0.3493 - mlm_loss: 15.9512 - nsp_acc: 0.9626 - mlm_lm_acc: 0.1538\n",
      "\n",
      "Epoch 00006: mlm_lm_acc improved from 0.14534 to 0.15375, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 7/10\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 15.7790 - nsp_loss: 0.3493 - mlm_loss: 15.4298 - nsp_acc: 0.9628 - mlm_lm_acc: 0.1618\n",
      "\n",
      "Epoch 00007: mlm_lm_acc improved from 0.15375 to 0.16180, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 8/10\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 15.3061 - nsp_loss: 0.3459 - mlm_loss: 14.9602 - nsp_acc: 0.9663 - mlm_lm_acc: 0.1701\n",
      "\n",
      "Epoch 00009: mlm_lm_acc improved from 0.16723 to 0.17005, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n",
      "Epoch 10/10\n",
      "2000/2000 [==============================] - 129s 64ms/step - loss: 15.2493 - nsp_loss: 0.3455 - mlm_loss: 14.9038 - nsp_acc: 0.9668 - mlm_lm_acc: 0.1712\n",
      "\n",
      "Epoch 00010: mlm_lm_acc improved from 0.17005 to 0.17119, saving model to /aiffel/aiffel/bert_pretrain/models/bert_pre_train.hdf5\n"
     ]
    }
   ],
   "source": [
    "# save weights callback\n",
    "save_weights = tf.keras.callbacks.ModelCheckpoint(f\"{model_dir}/bert_pre_train.hdf5\", monitor=\"mlm_lm_acc\", verbose=1, save_best_only=True, mode=\"max\", save_freq=\"epoch\", save_weights_only=True)\n",
    "# train\n",
    "history = pre_train_model.fit(\n",
    "    x=(pre_train_inputs[0], pre_train_inputs[1]),  # inputs = (enc_tokens, segments)\n",
    "    y=(pre_train_labels[0], pre_train_labels[1]),  # labels = (labels_nsp, labels_mlm)\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[save_weights]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0c0656f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training result\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['nsp_loss'], 'b-', label='nsp_loss')\n",
    "plt.plot(history.history['mlm_loss'], 'r--', label='mlm_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['nsp_acc'], 'g-', label='nsp_acc')\n",
    "plt.plot(history.history['mlm_lm_acc'], 'k--', label='mlm_acc')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8718927",
   "metadata": {},
   "source": [
    "1. NSP\n",
    "- Loss가 낮고 Acc가 높음\n",
    "- NSP에 오버피팅 의심\n",
    "\n",
    "2. MLM\n",
    "- Loss가 계속 감소 중\n",
    "- Acc가 0.2 정도로 낮음\n",
    "- 모델이 작아서 MLM 학습이 어렵거나 학습이 충분하지않는거 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6403b91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181d9c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cfbb46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f785afaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa80f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
